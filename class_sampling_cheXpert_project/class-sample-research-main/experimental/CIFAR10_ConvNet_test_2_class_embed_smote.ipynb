{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "           #  \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\", \"other\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "norm=False\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[143.8888, 127.1705, 117.5357], std=[69.8313, 64.5137, 66.9933])])\n",
    "else:\n",
    "   # transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "    transform=None\n",
    "\n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=2)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=2)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=2)\n",
    "triplet_test_CIFAR10 = class_sampling.ForTripletLoss(reduced_test_CIFAR10, smote=False, transform=transform, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46c8d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[255, 255, 255,  ..., 254, 254, 254],\n",
       "          [254, 254, 254,  ..., 254, 254, 254],\n",
       "          [254, 254, 254,  ..., 254, 254, 254],\n",
       "          ...,\n",
       "          [207, 216, 175,  ..., 251, 251, 251],\n",
       "          [255, 255, 255,  ..., 255, 172, 171],\n",
       "          [187, 141, 139,  ..., 184, 198, 156]],\n",
       "\n",
       "         [[153, 175, 152,  ..., 254, 254, 253],\n",
       "          [255, 255, 255,  ..., 250, 167, 165],\n",
       "          [184, 137, 135,  ..., 147, 167, 153],\n",
       "          ...,\n",
       "          [169, 133, 133,  ..., 146, 160, 164],\n",
       "          [164, 174, 181,  ..., 255, 255, 255],\n",
       "          [255, 255, 255,  ..., 159, 134, 133]],\n",
       "\n",
       "         [[161, 145, 145,  ..., 137, 154, 135],\n",
       "          [135, 149, 134,  ..., 241, 241, 241],\n",
       "          [255, 255, 255,  ..., 157, 160, 160],\n",
       "          ...,\n",
       "          [255, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255]]],\n",
       "\n",
       "\n",
       "        [[[146, 152, 166,  ..., 150, 129, 132],\n",
       "          [148, 128, 131,  ..., 166, 168, 171],\n",
       "          [169, 170, 173,  ..., 156, 151, 137],\n",
       "          ...,\n",
       "          [106, 124, 103,  ..., 124, 119, 117],\n",
       "          [ 92, 104, 137,  ..., 120,  84,  91],\n",
       "          [118,  84,  92,  ...,  99, 118,  95]],\n",
       "\n",
       "         [[101, 121,  96,  ..., 121, 118, 116],\n",
       "          [ 89, 100, 135,  ..., 115,  81,  88],\n",
       "          [113,  80,  89,  ...,  94, 116,  91],\n",
       "          ...,\n",
       "          [151, 191, 166,  ..., 162, 121, 189],\n",
       "          [157, 118, 165,  ..., 119, 110,  99],\n",
       "          [ 69,  82, 117,  ..., 173, 218, 203]],\n",
       "\n",
       "         [[175, 204, 183,  ..., 162, 130, 192],\n",
       "          [163, 135, 174,  ..., 112, 102,  94],\n",
       "          [ 73,  83, 120,  ..., 158, 241, 224],\n",
       "          ...,\n",
       "          [199, 183, 151,  ..., 148, 194, 180],\n",
       "          [148, 195, 180,  ..., 181, 150, 198],\n",
       "          [180, 147, 199,  ..., 194, 182, 149]]],\n",
       "\n",
       "\n",
       "        [[[254, 249, 251,  ..., 243, 244, 244],\n",
       "          [244, 246, 246,  ..., 244, 248, 248],\n",
       "          [243, 245, 247,  ..., 247, 246, 246],\n",
       "          ...,\n",
       "          [151, 169, 166,  ..., 232, 231, 237],\n",
       "          [246, 246, 247,  ..., 177, 158, 149],\n",
       "          [175, 137, 133,  ..., 106, 125, 126]],\n",
       "\n",
       "         [[117, 133, 161,  ..., 232, 231, 237],\n",
       "          [247, 247, 249,  ..., 163, 155, 146],\n",
       "          [172, 152, 148,  ..., 125, 142, 126],\n",
       "          ...,\n",
       "          [168, 133, 121,  ..., 238, 233, 245],\n",
       "          [245, 238, 198,  ..., 239, 238, 240],\n",
       "          [250, 248, 246,  ..., 166, 158, 136]],\n",
       "\n",
       "         [[148, 134, 118,  ..., 239, 231, 243],\n",
       "          [241, 235, 247,  ..., 241, 241, 243],\n",
       "          [247, 250, 247,  ..., 143, 135, 115],\n",
       "          ...,\n",
       "          [255, 255, 255,  ..., 244, 243, 243],\n",
       "          [243, 243, 243,  ..., 247, 246, 245],\n",
       "          [245, 247, 248,  ..., 252, 253, 248]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[169, 202, 206,  ..., 215, 169, 204],\n",
       "          [223, 174, 209,  ..., 211, 225, 133],\n",
       "          [157, 164,  89,  ..., 124, 148, 156],\n",
       "          ...,\n",
       "          [ 86,  85,  70,  ...,  46,  70,  58],\n",
       "          [170, 205, 213,  ..., 197, 111, 129],\n",
       "          [129,  80,  99,  ..., 117, 105,  50]],\n",
       "\n",
       "         [[ 86,  77,  53,  ...,  53,  82,  67],\n",
       "          [168, 202, 207,  ..., 146,  87, 104],\n",
       "          [102,  79, 100,  ..., 147, 128, 104],\n",
       "          ...,\n",
       "          [191, 199, 216,  ...,  54,  37,  10],\n",
       "          [ 58,  39,   6,  ...,  33,  77,  57],\n",
       "          [ 48,  79,  61,  ..., 191, 192, 216]],\n",
       "\n",
       "         [[206, 193, 206,  ...,  68,  51,  29],\n",
       "          [ 74,  56,  27,  ...,  46,  84,  68],\n",
       "          [ 69,  88,  72,  ..., 106,  62, 134],\n",
       "          ...,\n",
       "          [208, 208, 196,  ..., 195, 210, 208],\n",
       "          [196, 210, 209,  ..., 207, 197, 210],\n",
       "          [207, 192, 208,  ..., 114, 162,  87]]],\n",
       "\n",
       "\n",
       "        [[[ 50,  59,  66,  ...,  61, 157, 117],\n",
       "          [ 97, 152, 138,  ...,  74,  94,  52],\n",
       "          [ 68,  90,  40,  ...,  83,  90,  85],\n",
       "          ...,\n",
       "          [ 24,  13,  34,  ..., 152, 130, 109],\n",
       "          [148,  83,  54,  ..., 226, 253, 252],\n",
       "          [253, 234, 237,  ...,  52,  47, 102]],\n",
       "\n",
       "         [[ 26,  20,  49,  ..., 205, 196, 180],\n",
       "          [152,  92,  66,  ..., 250, 248, 252],\n",
       "          [249, 247, 229,  ...,  57,  44, 134],\n",
       "          ...,\n",
       "          [ 76, 224,  72,  ...,  71,  72,  90],\n",
       "          [ 84,  85,  70,  ...,  62,  72,  71],\n",
       "          [100, 107,  95,  ...,  56, 222, 133]],\n",
       "\n",
       "         [[122, 237, 154,  ...,  80,  81,  59],\n",
       "          [ 62,  61,  53,  ...,  59,  69,  68],\n",
       "          [216, 219, 214,  ..., 127, 254, 223],\n",
       "          ...,\n",
       "          [254, 255, 254,  ...,  70,  97,  82],\n",
       "          [ 73,  99,  84,  ...,  50,  47,  33],\n",
       "          [ 44,  40,  39,  ...,  77,  83,  74]]],\n",
       "\n",
       "\n",
       "        [[[105, 116, 122,  ..., 138, 103, 111],\n",
       "          [118, 116, 134,  ..., 118, 116, 120],\n",
       "          [128, 114, 118,  ..., 111, 124, 115],\n",
       "          ...,\n",
       "          [ 66, 110,  71,  ...,  77,  76, 116],\n",
       "          [161, 168, 202,  ..., 194, 189, 129],\n",
       "          [199, 173, 123,  ...,  13,  59, 106]],\n",
       "\n",
       "         [[ 27,  89, 104,  ...,  75,  69, 104],\n",
       "          [197, 195, 223,  ..., 197, 207, 123],\n",
       "          [198, 192, 114,  ...,  12,  60, 132],\n",
       "          ...,\n",
       "          [ 78,  91,  93,  ...,   8,  26,  45],\n",
       "          [  7,  20,  39,  ...,  34,  35,  75],\n",
       "          [105, 101, 116,  ...,  49,  49,  56]],\n",
       "\n",
       "         [[ 82, 109, 113,  ...,   7,  20,  28],\n",
       "          [  7,  17,  23,  ...,  33,  35,  70],\n",
       "          [155, 158, 169,  ...,  52,  59,  67],\n",
       "          ...,\n",
       "          [184, 174, 170,  ..., 178, 161, 162],\n",
       "          [177, 163, 164,  ..., 149, 158, 147],\n",
       "          [146, 154, 151,  ..., 147, 146, 154]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_train_CIFAR10.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9a3d21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5050, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, embeds = network(ratio_train_CIFAR10.images.float())\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "undersampler_smote = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * 50 * NUM_CLASSES_REDUCED), replacement=False)\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_smote_undersampled = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler_smote)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "test_loader_tripletloss = DataLoader(triplet_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e49019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance capped smote on embeddings with triplet loss \n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4]\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "loss_fn_args = {}\n",
    "\n",
    "smote = SMOTE()\n",
    "    \n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(start_epoch):\n",
    "            _, ratio_embeds = network(ratio_train_CIFAR10.images.float())\n",
    "            embeds, labels = smote.fit_resample(ratio_embeds.detach(), ratio_train_CIFAR10.labels)\n",
    "            _, _ = train.train_sigmoid_with_smote_embeddings(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "        for epoch in range(start_epoch, n_epochs+1):\n",
    "\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote_avg\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2e6fbeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0021048287153244017, AUC: 0.4813575\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'train' has no attribute 'train_sigmoid_with_smote_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m _, ratio_embeds \u001b[38;5;241m=\u001b[39m network(ratio_train_CIFAR10\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     22\u001b[0m embeds, labels \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(ratio_embeds\u001b[38;5;241m.\u001b[39mdetach(), ratio_train_CIFAR10\u001b[38;5;241m.\u001b[39mlabels)\n\u001b[0;32m---> 24\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_with_smote_embeddings\u001b[49m(epoch, train_loader_ratio, network, optimizer, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, loss_fn\u001b[38;5;241m=\u001b[39mloss_fns\u001b[38;5;241m.\u001b[39mCappedBCELoss, loss_fn_args\u001b[38;5;241m=\u001b[39mloss_fn_args)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     26\u001b[0m     _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'train' has no attribute 'train_sigmoid_with_smote_embeddings'"
     ]
    }
   ],
   "source": [
    "# smote on embeddings \n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4]\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "loss_fn_args = {}\n",
    "\n",
    "smote = SMOTE()\n",
    "    \n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, ratio_embeds = network(ratio_train_CIFAR10.images.float())\n",
    "            embeds, labels = smote.fit_resample(ratio_embeds.detach(), ratio_train_CIFAR10.labels)\n",
    "\n",
    "            _, _ = train.train_sigmoid_with_smote_embeddings(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"cosine_distance_capped_smote_avg\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6f79075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
