{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from warnings import simplefilter\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "378c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import class_sampling\n",
    "import train\n",
    "import metric_utils\n",
    "import inference\n",
    "import loss_fns\n",
    "import torchvision.ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91dda12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "momentum = 0\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "NUM_CLASSES_REDUCED = 2\n",
    "nums = (0, 1)\n",
    "ratio = (100, 1)\n",
    "\n",
    "CLASS_LABELS = {'airplane': 0,\n",
    "                 'automobile': 1,\n",
    "                 'bird': 2,\n",
    "                 'cat': 3,\n",
    "                 'deer': 4,\n",
    "                 'dog': 5,\n",
    "                 'frog': 6,\n",
    "                 'horse': 7,\n",
    "                 'ship': 8,\n",
    "                 'truck': 9}\n",
    "\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b57e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"name\", \n",
    "            \"num_classes\", \n",
    "            \"classes_used\", \n",
    "            \"ratio\", \n",
    "            \"learning_rate\", \n",
    "            \"mean_0\", \"variance_0\",\n",
    "            \"mean_10\", \"variance_10\",\n",
    "            \"mean_20\", \"variance_20\",\n",
    "            \"mean_30\", \"variance_30\",\n",
    "          #   \"mean_40\", \"variance_40\",\n",
    "           #  \"mean_50\", \"variance_50\",\n",
    "             \"cap\", \"normalization\", \"other\"]\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c2a1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "norm=False\n",
    "\n",
    "if norm:\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[143.8888, 127.1705, 117.5357], std=[69.8313, 64.5137, 66.9933])])\n",
    "else:\n",
    "   # transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "    transform=None\n",
    "\n",
    "train_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=True, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "\n",
    "test_CIFAR10 = torchvision.datasets.CIFAR10('cifar10', train=False, download=True,\n",
    "                             transform=transform)  \n",
    "\n",
    "train_CIFAR10.data = train_CIFAR10.data.reshape(50000, 3, 32, 32)\n",
    "test_CIFAR10.data = test_CIFAR10.data.reshape(10000, 3, 32, 32)\n",
    "\n",
    "    \n",
    "reduced_train_CIFAR10 = class_sampling.Reduce(train_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "reduced_test_CIFAR10 = class_sampling.Reduce(test_CIFAR10, NUM_CLASSES_REDUCED, nums=nums, CIFAR=True, transform=transform)\n",
    "\n",
    "ratio_train_CIFAR10 = class_sampling.Ratio(train_CIFAR10, NUM_CLASSES_REDUCED, ratio, nums=nums, transform=transform)\n",
    "\n",
    "triplet_train_CIFAR10 = class_sampling.ForTripletLoss(reduced_train_CIFAR10, smote=False, transform=transform, num_classes=2)\n",
    "triplet_ratio_train_CIFAR10 = class_sampling.ForTripletLoss(ratio_train_CIFAR10, smote=False, transform=transform, num_classes=2)\n",
    "\n",
    "smote_train_CIFAR10 = class_sampling.Smote(ratio_train_CIFAR10, 5000 * NUM_CLASSES_REDUCED, transform=transform)\n",
    "triplet_smote_train_CIFAR10 = class_sampling.ForTripletLoss(smote_train_CIFAR10, smote=True, transform=transform, num_classes=2)\n",
    "triplet_test_CIFAR10 = class_sampling.ForTripletLoss(reduced_test_CIFAR10, smote=False, transform=transform, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13159c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000   50]\n"
     ]
    }
   ],
   "source": [
    "targets = ratio_train_CIFAR10.labels \n",
    "\n",
    "class_count = np.unique(targets, return_counts=True)[1]\n",
    "print(class_count)\n",
    "\n",
    "weight = 1. / class_count\n",
    "\n",
    "samples_weight = weight[targets]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "oversampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(max(class_count) * NUM_CLASSES_REDUCED), replacement=True)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "undersampler = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * NUM_CLASSES_REDUCED), replacement=False)\n",
    "undersampler_smote = torch.utils.data.WeightedRandomSampler(samples_weight, int(min(class_count) * 50 * NUM_CLASSES_REDUCED), replacement=False)\n",
    "weight *= class_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af8cd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_reduced = DataLoader(reduced_train_CIFAR10, batch_size=batch_size_train, shuffle=True)  \n",
    "\n",
    "train_loader_ratio = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True) \n",
    "\n",
    "train_loader_oversampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=oversampler)\n",
    "\n",
    "train_loader_undersampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler)\n",
    "\n",
    "train_loader_sampled = DataLoader(ratio_train_CIFAR10, batch_size=batch_size_train, sampler=sampler)\n",
    "\n",
    "train_loader_smote = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_smote_undersampled = DataLoader(smote_train_CIFAR10, batch_size=batch_size_train, sampler=undersampler_smote)\n",
    "\n",
    "train_loader_tripletloss = DataLoader(triplet_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_ratio = DataLoader(triplet_ratio_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "train_loader_tripletloss_smote = DataLoader(triplet_smote_train_CIFAR10, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader_reduced = DataLoader(reduced_test_CIFAR10, batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "test_loader_tripletloss = DataLoader(triplet_test_CIFAR10, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a9137b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used in distance capped smote - get average tensor for the entire class \n",
    "dataset = train_loader_ratio.dataset\n",
    "class0 = dataset.images[dataset.labels==0]\n",
    "class1 = dataset.images[dataset.labels==1]\n",
    "class0_avg = torch.mean(class0.float(), 0)\n",
    "class1_avg = torch.mean(class1.float(), 0)\n",
    "class_img_list = [class0, class1]\n",
    "avg_tensors_list = [class0_avg, class1_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b7ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.001968830943107605, AUC: 0.6361665000000001\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 18\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     20\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:27\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     26\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 CLASS normal\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_reduced, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"normal\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8f1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d211694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0076671247482299806, AUC: 0.6110735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020734254121780394, AUC: 0.628587\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008975799964501125, AUC: 0.6439400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019357805252075196, AUC: 0.643949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009144600006834705, AUC: 0.675112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020213021636009214, AUC: 0.67089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008470717963236983, AUC: 0.7536120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003886705994606018, AUC: 0.514122\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015828227996826172, AUC: 0.595936\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012174508654245056, AUC: 0.567244\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015572131872177125, AUC: 0.6836819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011150310660647874, AUC: 0.643232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014421252608299256, AUC: 0.712404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012044163608905113, AUC: 0.668856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01665996742248535, AUC: 0.6862579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015813767313957213, AUC: 0.6905600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010870761579216117, AUC: 0.705184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017244288921356202, AUC: 0.734825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009154729095130863, AUC: 0.7596320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018377559781074524, AUC: 0.756024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008260445851869513, AUC: 0.80658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002282716631889343, AUC: 0.587105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015921525359153747, AUC: 0.615949\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011881166224432464, AUC: 0.561692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016021928191184997, AUC: 0.6867319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010604898484036474, AUC: 0.658272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013688442707061768, AUC: 0.7098280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001267247179357132, AUC: 0.728272\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013568562865257264, AUC: 0.532369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020910239219665526, AUC: 0.550422\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009813344559752115, AUC: 0.502484\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016323612332344055, AUC: 0.619579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011425297581913448, AUC: 0.5709839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001749391257762909, AUC: 0.6334299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038080965085785, AUC: 0.591348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019294458627700805, AUC: 0.441978\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015018232464790345, AUC: 0.532296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001446212606854958, AUC: 0.590052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016287894248962402, AUC: 0.637854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010995772665384972, AUC: 0.673188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001581701934337616, AUC: 0.659426\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001093492754320107, AUC: 0.7412399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007909612655639649, AUC: 0.5652269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021779963970184326, AUC: 0.6679660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008836146163763387, AUC: 0.63694\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002288204073905945, AUC: 0.702678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008463866534725865, AUC: 0.690752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002026849448680878, AUC: 0.719262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008203459484963724, AUC: 0.747512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010193572998046876, AUC: 0.748682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010386010408401489, AUC: 0.743842\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019852848542798863, AUC: 0.7785920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014001491665840149, AUC: 0.7602965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001101876115887472, AUC: 0.796392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012921421527862548, AUC: 0.7561570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012562816885143223, AUC: 0.820144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009717195510864259, AUC: 0.687356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019463917016983032, AUC: 0.690081\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009062620215486772, AUC: 0.6662239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017195783257484437, AUC: 0.709087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009551594546525785, AUC: 0.70116\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016595609188079835, AUC: 0.7358560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009313118195917346, AUC: 0.7405039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009570589363574982, AUC: 0.5011805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016350972056388855, AUC: 0.5762929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012110045397340662, AUC: 0.553572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018443724513053895, AUC: 0.645478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009665573517432307, AUC: 0.63528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001650454878807068, AUC: 0.654357\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010650237553780622, AUC: 0.67218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010591432452201842, AUC: 0.494801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019391133189201354, AUC: 0.53718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010679098203925804, AUC: 0.46274000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018079440593719482, AUC: 0.56423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086755657550132, AUC: 0.5107520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018325347900390624, AUC: 0.605268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001027805893359208, AUC: 0.549512\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002467226266860962, AUC: 0.49211449999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018154305219650268, AUC: 0.6301359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001004764328867492, AUC: 0.5766800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019403483271598816, AUC: 0.653424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009370906521925832, AUC: 0.595504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00173450368642807, AUC: 0.6438625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001024536982842601, AUC: 0.605344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021772370338439943, AUC: 0.6291599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019137043356895448, AUC: 0.558418\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001086977752718595, AUC: 0.459476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017537219524383545, AUC: 0.66186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010254885189899124, AUC: 0.571672\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001654850125312805, AUC: 0.693519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010333627373865335, AUC: 0.6243879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012681103229522706, AUC: 0.5257050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017512226104736328, AUC: 0.396621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013878306112076976, AUC: 0.43118799999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016085657477378846, AUC: 0.41888000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014239362044499652, AUC: 0.44889999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016571618914604186, AUC: 0.48058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012518959057213057, AUC: 0.491518\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008851375579833985, AUC: 0.394701\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022655904293060304, AUC: 0.6417440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009069401588254046, AUC: 0.60942\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018319138884544373, AUC: 0.650572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009779671255019631, AUC: 0.635076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017738329172134399, AUC: 0.6864399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009667404914403906, AUC: 0.667876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001978057026863098, AUC: 0.42523900000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001972887694835663, AUC: 0.5121659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011145488486284077, AUC: 0.40484000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020207125544548033, AUC: 0.6013890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009857453996002085, AUC: 0.49250799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019874536991119386, AUC: 0.635966\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009604879815389614, AUC: 0.53166\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017852833867073059, AUC: 0.3784155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023233342170715333, AUC: 0.5972189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000935311899105511, AUC: 0.521724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00229307758808136, AUC: 0.665186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008898888480397734, AUC: 0.6355280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001983289897441864, AUC: 0.6787810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008811807359504227, AUC: 0.6924520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024164196252822877, AUC: 0.49547800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016742894053459168, AUC: 0.571703\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012209183014560453, AUC: 0.5585760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018676289916038514, AUC: 0.633934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009904458447553143, AUC: 0.608856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019339900016784668, AUC: 0.6533000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009348994578317841, AUC: 0.6239119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025133802890777586, AUC: 0.45490200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002249223232269287, AUC: 0.627273\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009032244074831504, AUC: 0.615916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002208526015281677, AUC: 0.6674635\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008650489547981484, AUC: 0.670572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021552451848983766, AUC: 0.7147754999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008405291248518642, AUC: 0.711684\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025969254970550537, AUC: 0.4249035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020958303213119505, AUC: 0.5708455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009637036760992343, AUC: 0.552364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018786900043487548, AUC: 0.6407069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009473569576840589, AUC: 0.6412439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020249083042144776, AUC: 0.679206\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008764915754741961, AUC: 0.679316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001352885603904724, AUC: 0.4485485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024831844568252563, AUC: 0.42361899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010917736091973758, AUC: 0.361972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002164663791656494, AUC: 0.455156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078405144191025, AUC: 0.398448\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021346499919891355, AUC: 0.501624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010488035323301165, AUC: 0.43668399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016203722953796386, AUC: 0.530114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003072163462638855, AUC: 0.588485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010479836682742922, AUC: 0.5180480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023032039403915404, AUC: 0.587286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009547403945338608, AUC: 0.526236\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001921819806098938, AUC: 0.605945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010153998250123299, AUC: 0.552732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008129218101501464, AUC: 0.333634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030005383491516113, AUC: 0.435077\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010937118794916586, AUC: 0.394652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021180523633956908, AUC: 0.491343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001148930329571266, AUC: 0.4620079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022330018281936643, AUC: 0.565871\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010301658605216164, AUC: 0.524832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003371488690376282, AUC: 0.589884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002845911741256714, AUC: 0.36794899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001133701276071001, AUC: 0.340316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021184813976287842, AUC: 0.41797999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011628526869681803, AUC: 0.370284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023320701122283936, AUC: 0.45413400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010569927607211147, AUC: 0.397404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003613470792770386, AUC: 0.6405080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002160325050354004, AUC: 0.468642\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001171023680432008, AUC: 0.34930800000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001940733015537262, AUC: 0.521421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011482540477472957, AUC: 0.42302800000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018642403483390808, AUC: 0.565607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001105369452645283, AUC: 0.47382399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015669259428977966, AUC: 0.70762\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027528159618377688, AUC: 0.6968620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009593207823887172, AUC: 0.643348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022390233278274537, AUC: 0.675067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008987060488008037, AUC: 0.635968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021265636682510376, AUC: 0.640035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009188651688175626, AUC: 0.603808\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005645490407943725, AUC: 0.68851\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033736543655395507, AUC: 0.671146\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010854890158095646, AUC: 0.6384360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022894320487976075, AUC: 0.6748799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008709144213301415, AUC: 0.6768319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002009647250175476, AUC: 0.6820379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008711621181873402, AUC: 0.7085199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013125876188278198, AUC: 0.63015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029850161075592042, AUC: 0.460643\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011202180633634918, AUC: 0.44290799999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023843228816986086, AUC: 0.5409745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010281204304999055, AUC: 0.48174799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00209027099609375, AUC: 0.5642759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010499044079886805, AUC: 0.5048440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037168740034103395, AUC: 0.4056835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028641793727874755, AUC: 0.375166\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010992970473437322, AUC: 0.293972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002653481364250183, AUC: 0.40545999999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010511783987173055, AUC: 0.323976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002539039492607117, AUC: 0.43797700000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010308365265626718, AUC: 0.34588399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013203486800193787, AUC: 0.378262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002571802616119385, AUC: 0.599729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009274383259955609, AUC: 0.57036\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019491751790046692, AUC: 0.554403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001017076036717632, AUC: 0.530888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017297410368919373, AUC: 0.552915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011625397810251406, AUC: 0.527944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS ratio\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "learning_rate_train_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"ratio\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f2d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f1ccb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0038651072978973387, AUC: 0.40709100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005063888132572174, AUC: 0.8398785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005734625160694122, AUC: 0.8425819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011212489008903504, AUC: 0.841191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008080781102180481, AUC: 0.55231\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006464185416698455, AUC: 0.7963709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010616356730461121, AUC: 0.7623549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011121231317520143, AUC: 0.753925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025173540115356447, AUC: 0.5626525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006488811075687408, AUC: 0.6799000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000709791898727417, AUC: 0.6942869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009997697472572326, AUC: 0.726058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008281596899032593, AUC: 0.572139\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004913568198680878, AUC: 0.8518579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006246580779552459, AUC: 0.8336830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015956854224205017, AUC: 0.814265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007366308271884918, AUC: 0.513698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005183021128177642, AUC: 0.840308\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008029429018497467, AUC: 0.818953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010578397512435913, AUC: 0.804311\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035281330347061157, AUC: 0.38053750000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007264424860477448, AUC: 0.8519850000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008754095733165741, AUC: 0.8259589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012884948253631592, AUC: 0.80179\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006137662887573242, AUC: 0.5142995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005887452065944671, AUC: 0.8550169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000608889102935791, AUC: 0.735435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006282347142696381, AUC: 0.747253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008330761790275574, AUC: 0.6295200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004755498617887497, AUC: 0.863896\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001037799596786499, AUC: 0.802286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001165145993232727, AUC: 0.7922740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012109436988830566, AUC: 0.49136199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005991648733615875, AUC: 0.8487684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005843090116977691, AUC: 0.795875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005498118996620178, AUC: 0.816298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008850356578826905, AUC: 0.647549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940476298332214, AUC: 0.4995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594555079936981, AUC: 0.705973\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006307438910007477, AUC: 0.7548905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004416076898574829, AUC: 0.323978\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014050147533416749, AUC: 0.766865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026331219673156737, AUC: 0.779182\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004152154922485352, AUC: 0.7604204999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014910327792167664, AUC: 0.49231600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007511105239391327, AUC: 0.6525365000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001356121063232422, AUC: 0.5043154999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024438369274139404, AUC: 0.580111\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001424083411693573, AUC: 0.5397235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017812739014625549, AUC: 0.684012\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014088171124458312, AUC: 0.6441939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004744303703308105, AUC: 0.673342\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022509771585464476, AUC: 0.390438\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014318109154701233, AUC: 0.776913\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037591698169708253, AUC: 0.6860169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003917380690574646, AUC: 0.704576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004887598752975464, AUC: 0.45462800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008536508679389954, AUC: 0.656873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021468346118927004, AUC: 0.637953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034337985515594483, AUC: 0.591375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017650567293167115, AUC: 0.66196\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010653643608093263, AUC: 0.667168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021494070291519164, AUC: 0.631052\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036370185613632203, AUC: 0.603567\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013272791504859924, AUC: 0.392621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016511693596839906, AUC: 0.594365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004804892063140869, AUC: 0.554214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0076607315540313725, AUC: 0.586339\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007192042171955109, AUC: 0.705309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008659768998622895, AUC: 0.6609050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00251075804233551, AUC: 0.678975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003699234962463379, AUC: 0.612506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00479100751876831, AUC: 0.544395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007851931750774383, AUC: 0.620472\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018445215821266175, AUC: 0.500974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002619206666946411, AUC: 0.47592399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025211087465286256, AUC: 0.6451229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019069305658340455, AUC: 0.745953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015072822570800781, AUC: 0.639524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002941378951072693, AUC: 0.689565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000982449561357498, AUC: 0.581275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006168580055236816, AUC: 0.777878\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000589244395494461, AUC: 0.816397\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000577460139989853, AUC: 0.83312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004177759647369385, AUC: 0.553401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006863325834274292, AUC: 0.612715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006757993996143341, AUC: 0.6529855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005748008191585541, AUC: 0.7996274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028271037340164185, AUC: 0.6727749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006341786980628968, AUC: 0.8080189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006235616207122803, AUC: 0.8241250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006174919605255127, AUC: 0.8227709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000940509021282196, AUC: 0.5437019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006323151588439941, AUC: 0.712728\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005493966341018676, AUC: 0.818747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005441974699497223, AUC: 0.832964\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006498530805110932, AUC: 0.7259580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006102970540523529, AUC: 0.7819095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005698021352291107, AUC: 0.8150240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005621075034141541, AUC: 0.819928\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006551118850708007, AUC: 0.4868045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000684427797794342, AUC: 0.623251\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006762815713882446, AUC: 0.6828545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006693606078624725, AUC: 0.7149480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014205492973327637, AUC: 0.4715385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693734735250473, AUC: 0.5195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006764842867851257, AUC: 0.7068864999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006710585653781891, AUC: 0.7434974999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027725353240966796, AUC: 0.398981\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936939358711243, AUC: 0.5009939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936058700084686, AUC: 0.500993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693530559539795, AUC: 0.5014959999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000877816766500473, AUC: 0.5271775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006711920201778412, AUC: 0.6687230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006447704434394836, AUC: 0.766165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006403065919876099, AUC: 0.7804540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002469860792160034, AUC: 0.593591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005706853270530701, AUC: 0.843007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005331571698188782, AUC: 0.8573040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005283410847187043, AUC: 0.855439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS oversampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_oversampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"oversampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52d8c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1541b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006928627490997315, AUC: 0.5610565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006705484390258789, AUC: 0.731087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006308182775974274, AUC: 0.7689\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006022096574306488, AUC: 0.792463\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950775682926178, AUC: 0.4702115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006825314462184907, AUC: 0.7292394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006524708569049835, AUC: 0.754748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006467648446559906, AUC: 0.7728849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955726444721221, AUC: 0.3766905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007012077569961548, AUC: 0.601467\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006722729504108429, AUC: 0.7478565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006584925651550293, AUC: 0.764774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918634176254272, AUC: 0.548633\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006640854477882385, AUC: 0.7368910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006435896456241608, AUC: 0.772193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132080554962159, AUC: 0.805193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965889036655426, AUC: 0.3598795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006798864006996155, AUC: 0.722772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425350904464722, AUC: 0.760243\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240421831607819, AUC: 0.7786550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957835257053375, AUC: 0.4375025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006834719181060791, AUC: 0.711951\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006513985991477967, AUC: 0.7628889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006210027635097504, AUC: 0.7961360000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007116021811962127, AUC: 0.2701275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006652469336986542, AUC: 0.749688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440185308456421, AUC: 0.780153\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006193165481090545, AUC: 0.7989299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007008937299251557, AUC: 0.3809365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006463136076927186, AUC: 0.7635879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005850833356380462, AUC: 0.7921960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005856163203716278, AUC: 0.8098865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968954503536225, AUC: 0.411403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943140625953674, AUC: 0.7122989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006632572412490845, AUC: 0.767749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006245843768119812, AUC: 0.795972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954509615898132, AUC: 0.42410250000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006669250130653382, AUC: 0.768473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006450056135654449, AUC: 0.786346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006585321128368377, AUC: 0.7965205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006984304785728454, AUC: 0.3510465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006993257403373718, AUC: 0.445579\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007002671062946319, AUC: 0.557559\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006986504197120666, AUC: 0.624322\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875935792922974, AUC: 0.6208015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006796917617321014, AUC: 0.7305775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006728697121143341, AUC: 0.732001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000661188006401062, AUC: 0.737393\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006826050281524658, AUC: 0.708147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837069392204285, AUC: 0.688508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006812298893928528, AUC: 0.6901515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006734901666641236, AUC: 0.704798\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932272017002106, AUC: 0.5695859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006879149675369263, AUC: 0.6111989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006847475469112396, AUC: 0.640729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006875899136066436, AUC: 0.6689890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902289688587189, AUC: 0.6300385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905774176120758, AUC: 0.638779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883034110069275, AUC: 0.677482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006861826181411743, AUC: 0.7036560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006971223056316376, AUC: 0.4343925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006849060654640197, AUC: 0.6515434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006698879599571228, AUC: 0.6940439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006588464975357056, AUC: 0.713968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927119195461273, AUC: 0.519376\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688279390335083, AUC: 0.6079015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000678226500749588, AUC: 0.666508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006683720946311951, AUC: 0.6944840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899041533470154, AUC: 0.6205875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006839993894100189, AUC: 0.6695689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006716804206371307, AUC: 0.7078499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006677893996238709, AUC: 0.720974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930917203426361, AUC: 0.534002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947171986103058, AUC: 0.5545225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963235139846802, AUC: 0.5801274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964735686779023, AUC: 0.6224324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932185888290405, AUC: 0.5061089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940591931343078, AUC: 0.546071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943828165531158, AUC: 0.6247775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941763460636139, AUC: 0.6806765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687966525554657, AUC: 0.7050644999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006887478828430176, AUC: 0.6989965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894939541816712, AUC: 0.690177\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006904767751693726, AUC: 0.6740535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929472386837006, AUC: 0.520876\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930617392063141, AUC: 0.510615\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929692327976226, AUC: 0.5129855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930001080036163, AUC: 0.509326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698872059583664, AUC: 0.252758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988219320774078, AUC: 0.25494649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988555192947388, AUC: 0.258632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988353133201599, AUC: 0.26737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007016754448413849, AUC: 0.3065925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007013347446918487, AUC: 0.3021595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007006541788578033, AUC: 0.3044395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006997955143451691, AUC: 0.3110185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947731077671051, AUC: 0.600099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941960752010345, AUC: 0.5978035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693750262260437, AUC: 0.5939255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934162974357605, AUC: 0.5842645000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006975911855697632, AUC: 0.3350575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968346834182739, AUC: 0.35933800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962250173091889, AUC: 0.392072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956771612167358, AUC: 0.43028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948958039283752, AUC: 0.42235900000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947318315505981, AUC: 0.42986450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946011483669281, AUC: 0.4361265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945200264453888, AUC: 0.44007999999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920962631702423, AUC: 0.5704765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006916036903858184, AUC: 0.600501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006912074089050293, AUC: 0.621478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909322440624237, AUC: 0.6336729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977652311325074, AUC: 0.3840535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973227560520172, AUC: 0.3848955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969808340072631, AUC: 0.38405150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696704238653183, AUC: 0.383508\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006994488835334778, AUC: 0.386849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006997025310993194, AUC: 0.392687\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006996612548828125, AUC: 0.40394399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006992768943309784, AUC: 0.42428699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006988563239574432, AUC: 0.30963050000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964642107486725, AUC: 0.4378635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936416327953339, AUC: 0.530629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006915313899517059, AUC: 0.585804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936706900596618, AUC: 0.5052385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943995654582978, AUC: 0.5241214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006942475736141204, AUC: 0.5752839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930316984653473, AUC: 0.618519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896259486675263, AUC: 0.604004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006859594881534576, AUC: 0.636279\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.000681488573551178, AUC: 0.661619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006757974326610565, AUC: 0.6790075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896138191223144, AUC: 0.610887\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893069446086884, AUC: 0.613855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006885486543178558, AUC: 0.633282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888088881969452, AUC: 0.6441379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698607712984085, AUC: 0.47644650000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970592141151428, AUC: 0.5706005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956733465194702, AUC: 0.6301249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951669156551361, AUC: 0.6572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006925795376300812, AUC: 0.575475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951481699943542, AUC: 0.5105139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970000267028808, AUC: 0.4651495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000699210911989212, AUC: 0.424679\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938398480415344, AUC: 0.49060099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900518834590912, AUC: 0.6081639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006852288544178009, AUC: 0.6609965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006791809201240539, AUC: 0.6885735000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006768987178802491, AUC: 0.7593004999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000679383397102356, AUC: 0.7485544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822780966758728, AUC: 0.7235449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006835326850414276, AUC: 0.703198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000697054535150528, AUC: 0.412941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958455443382264, AUC: 0.4202635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951872408390046, AUC: 0.4582435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693785697221756, AUC: 0.5277975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936173141002655, AUC: 0.49884150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692090779542923, AUC: 0.5442275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922757029533387, AUC: 0.548847\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006904305517673493, AUC: 0.5985255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974986791610717, AUC: 0.47294749999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974535882472992, AUC: 0.47196799999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974373161792755, AUC: 0.470164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974062919616699, AUC: 0.4685745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945805549621582, AUC: 0.511066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945959925651551, AUC: 0.512351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946030259132385, AUC: 0.513831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946027278900146, AUC: 0.5153995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007035338878631592, AUC: 0.3031575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703522503376007, AUC: 0.3047515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703513115644455, AUC: 0.306492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000703542560338974, AUC: 0.307971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903063356876373, AUC: 0.6405325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902830302715302, AUC: 0.6414055000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902560889720917, AUC: 0.642237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902223825454712, AUC: 0.6432424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973913013935089, AUC: 0.3781635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973600089550018, AUC: 0.37925449999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973490417003632, AUC: 0.37986600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973181366920472, AUC: 0.3809785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006964112520217896, AUC: 0.37526950000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963879466056823, AUC: 0.3755745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000696378231048584, AUC: 0.3756815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963653862476349, AUC: 0.375803\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952761113643647, AUC: 0.4577645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952200829982758, AUC: 0.45907450000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695160061120987, AUC: 0.46040699999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950892806053162, AUC: 0.462167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000690339595079422, AUC: 0.6339035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006903114020824433, AUC: 0.634817\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902790367603302, AUC: 0.6358309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902419328689575, AUC: 0.6368320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928563117980957, AUC: 0.5400750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927987933158875, AUC: 0.5429109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927626430988312, AUC: 0.544478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006927131414413453, AUC: 0.5468745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936696171760559, AUC: 0.48374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936245560646057, AUC: 0.48604149999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006935936212539673, AUC: 0.4878675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693551242351532, AUC: 0.48999699999999996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS undersampled\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 1e-3, 5e-3, 1e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_undersampled, network, optimizer, verbose=False)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"undersampled\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fda3918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdf3dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006929865777492523, AUC: 0.6035210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941011548042297, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01152815311261923, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933517456054688, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010536302033037242, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006934399306774139, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01047680665950964, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006829564273357391, AUC: 0.7606660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007110981047153472, AUC: 0.513034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008160775073684089, AUC: 0.4939\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991162598133087, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009259643141586001, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932158470153809, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011024211881184342, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900024712085724, AUC: 0.6275345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006032556593418122, AUC: 0.6915875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009716101306499822, AUC: 0.67254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006006793975830078, AUC: 0.703085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010080904476713426, AUC: 0.68551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000693379133939743, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010516728127356803, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006867653727531433, AUC: 0.6620455000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941483020782471, AUC: 0.5020020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0101606553025765, AUC: 0.5024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947364509105683, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011732829313467044, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932136416435242, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011021087063421118, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946891844272614, AUC: 0.45950349999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940916180610656, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010191532998982043, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905839145183564, AUC: 0.5271815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010548212835104159, AUC: 0.585972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954963207244873, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009828539650038918, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695802628993988, AUC: 0.5831175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958652138710022, AUC: 0.594743\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008937520402492863, AUC: 0.6741199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006913960874080657, AUC: 0.532716\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009897223864451493, AUC: 0.5484240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926067471504212, AUC: 0.5197149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009974045635450004, AUC: 0.543572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007032238245010376, AUC: 0.26280099999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963876187801361, AUC: 0.5065189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009458458411811602, AUC: 0.5388360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007120804488658906, AUC: 0.567307\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008191924195478458, AUC: 0.6288479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007221527099609375, AUC: 0.5354529999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007242459070564497, AUC: 0.6090860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007022532224655151, AUC: 0.309856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940309107303619, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01150185514204573, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006932080686092377, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011013430510417069, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006936182677745819, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011321447296897963, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007022981643676758, AUC: 0.279428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933487057685852, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010538602512661773, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937902569770813, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011403333208348491, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939776837825775, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011481297110566998, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007030169069766998, AUC: 0.26083599999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006941924095153809, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010158426478357598, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933222115039825, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011132937882206227, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947585344314575, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011739103463616701, AUC: 0.5\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947209239006043, AUC: 0.503085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004985811412334442, AUC: 0.8705590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008741397662918166, AUC: 0.910452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048722681403160097, AUC: 0.8629110000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007839573692567277, AUC: 0.911974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005121163129806519, AUC: 0.831754\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006779720865853942, AUC: 0.9525359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931937634944916, AUC: 0.6319214999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005152012705802917, AUC: 0.8624559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010805733227493739, AUC: 0.8948520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005664720535278321, AUC: 0.842025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006664018843433645, AUC: 0.9242239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005163897573947907, AUC: 0.8882985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004746445078070801, AUC: 0.95862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006993449330329895, AUC: 0.308698\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005618657171726227, AUC: 0.837395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007188617935275087, AUC: 0.8682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005276458859443665, AUC: 0.85389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0053951541858144325, AUC: 0.9311919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005647776126861573, AUC: 0.8183910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0056284768156485985, AUC: 0.942612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006987028419971466, AUC: 0.419359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000473314568400383, AUC: 0.8638839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007454072691426419, AUC: 0.884772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346084535121918, AUC: 0.8605094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007051103150490487, AUC: 0.9267599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005636484026908874, AUC: 0.8834069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004816935280762096, AUC: 0.9464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926956474781036, AUC: 0.62321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005920693576335907, AUC: 0.872232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00950404413855902, AUC: 0.898072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000488565668463707, AUC: 0.86691\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005895164248966934, AUC: 0.8858999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005560655891895294, AUC: 0.870534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004082345602535966, AUC: 0.9216799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069858717918396, AUC: 0.3564225000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005433701276779174, AUC: 0.8413849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007599443150038766, AUC: 0.8711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005662823319435119, AUC: 0.8394060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0061812017282637036, AUC: 0.9136279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344362497329712, AUC: 0.863313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004362780357351398, AUC: 0.933392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920470595359802, AUC: 0.626553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005774954557418823, AUC: 0.8625749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005773622346396493, AUC: 0.8950400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005901984572410584, AUC: 0.8413899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005424396850094937, AUC: 0.9351479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005415751934051514, AUC: 0.844387\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005204110599980496, AUC: 0.9476119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006909263730049133, AUC: 0.608957\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005072842240333557, AUC: 0.848906\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006716722314900691, AUC: 0.88254\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000501204401254654, AUC: 0.8605349999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005464349566119732, AUC: 0.916816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005297437012195587, AUC: 0.852255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006041818249343646, AUC: 0.951712\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929771304130555, AUC: 0.6099144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005701894760131835, AUC: 0.810224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007652453461495957, AUC: 0.9052879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005346051752567291, AUC: 0.865758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004242133191316434, AUC: 0.94032\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005565256476402282, AUC: 0.8667680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006119210065001308, AUC: 0.932944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006973126232624054, AUC: 0.389163\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005637988150119782, AUC: 0.828475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00714115304521995, AUC: 0.9039240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005115714669227601, AUC: 0.872054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058704357690150194, AUC: 0.9506\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005990622639656067, AUC: 0.854916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004999826568188054, AUC: 0.9478759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893006265163422, AUC: 0.592972\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005413131713867187, AUC: 0.838742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009401522638774154, AUC: 0.873108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005082505941390992, AUC: 0.8643450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007597315765843533, AUC: 0.93034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005349463224411011, AUC: 0.819852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005960197472336269, AUC: 0.952184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006917398273944855, AUC: 0.5357335000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005320677161216736, AUC: 0.872421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007882021895729669, AUC: 0.885336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047688519954681394, AUC: 0.8764339999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006409961439595364, AUC: 0.9162440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004914150685071945, AUC: 0.8588819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005879358218447997, AUC: 0.94408\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951043009757996, AUC: 0.4402385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005084710121154785, AUC: 0.8479289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007602337244713661, AUC: 0.887208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004919102936983108, AUC: 0.8564430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006338312826534309, AUC: 0.9233879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005057655572891235, AUC: 0.839117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006557884688424592, AUC: 0.94724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957819759845733, AUC: 0.554511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005380403399467468, AUC: 0.8291689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009521948453223352, AUC: 0.906452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004899816066026687, AUC: 0.8586830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00712057093582531, AUC: 0.9333940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005030784308910369, AUC: 0.850487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0057055700031837615, AUC: 0.94844\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007004860043525696, AUC: 0.3607595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00053057861328125, AUC: 0.8529720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070074458405523015, AUC: 0.886212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004892897009849548, AUC: 0.868004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005604852668129571, AUC: 0.920404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000458210751414299, AUC: 0.875929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006420788363655015, AUC: 0.946916\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940570473670959, AUC: 0.4731449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005776671469211579, AUC: 0.8586539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.012373634175498888, AUC: 0.8687280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044246050715446473, AUC: 0.889369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0070867665805438955, AUC: 0.9181079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005300732553005219, AUC: 0.8762265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005396853927338478, AUC: 0.960264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006929945051670075, AUC: 0.564638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005563854575157166, AUC: 0.8648770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009520969921999639, AUC: 0.923976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005157967507839203, AUC: 0.8525540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006059927674803403, AUC: 0.939404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000465511754155159, AUC: 0.877534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006717685177774713, AUC: 0.948132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006980034112930298, AUC: 0.392889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005355749130249024, AUC: 0.8342959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007846096653749447, AUC: 0.8555839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004474954456090927, AUC: 0.879305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00643831486159032, AUC: 0.898184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046650290489196777, AUC: 0.887073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00500908629138871, AUC: 0.929724\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906140744686127, AUC: 0.6270205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005572724044322967, AUC: 0.85573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008027610229973746, AUC: 0.898772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005564309954643249, AUC: 0.8470209999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004956791843518172, AUC: 0.911748\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004649658203125, AUC: 0.866714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006907176440305049, AUC: 0.956392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938843429088593, AUC: 0.47562899999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000519424855709076, AUC: 0.8551110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007493531125606877, AUC: 0.8831279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000504085659980774, AUC: 0.8609840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006028514882125476, AUC: 0.9416640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004914458394050599, AUC: 0.867466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006038065062891139, AUC: 0.96608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963664293289185, AUC: 0.38524\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432399749755859, AUC: 0.7625959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008959613684380408, AUC: 0.7848919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005628331005573272, AUC: 0.826678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00864995011598757, AUC: 0.84258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005056944489479065, AUC: 0.8590329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00824941654016476, AUC: 0.871856\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910701990127564, AUC: 0.599355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000616879791021347, AUC: 0.7707120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01059920307433251, AUC: 0.781044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005687816441059113, AUC: 0.799205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009738183611690408, AUC: 0.8164119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005393176674842834, AUC: 0.8172925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008101473804747704, AUC: 0.842912\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006939460039138794, AUC: 0.54028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006021830439567566, AUC: 0.7970139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010496961374093991, AUC: 0.803984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005550967454910278, AUC: 0.828073\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009545001192848282, AUC: 0.839732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005240070223808289, AUC: 0.844389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0091742208452508, AUC: 0.8625879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006914302408695221, AUC: 0.6094385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005888742804527283, AUC: 0.824057\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01029107907030842, AUC: 0.83066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443206429481506, AUC: 0.8330359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009114989762259001, AUC: 0.849788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005262280106544494, AUC: 0.853632\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008392372603463654, AUC: 0.878884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006837622821331024, AUC: 0.691576\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005895999670028686, AUC: 0.782758\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009808489296695973, AUC: 0.80072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005589506924152375, AUC: 0.806367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009164169448437077, AUC: 0.8281080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005357769429683685, AUC: 0.8292949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00911985157149853, AUC: 0.8574439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945233047008514, AUC: 0.48311099999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006134748458862304, AUC: 0.806477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01052536406139336, AUC: 0.808596\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005562895834445953, AUC: 0.822084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009412364829884898, AUC: 0.833688\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005309949219226837, AUC: 0.8396819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007839170440588848, AUC: 0.857212\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006937495470046997, AUC: 0.48171200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006372389495372772, AUC: 0.819923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0120068094163838, AUC: 0.8215159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005695891678333283, AUC: 0.822279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009782834808425149, AUC: 0.840788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005112173855304718, AUC: 0.8443940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008199451105429394, AUC: 0.868456\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006921443343162536, AUC: 0.5599354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005984731018543243, AUC: 0.8115479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010888017746481566, AUC: 0.82364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005443423092365265, AUC: 0.8276000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009130165565131914, AUC: 0.8468919999999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005055390298366547, AUC: 0.849865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008350928868397627, AUC: 0.8736079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006963136792182922, AUC: 0.39968349999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005984225273132324, AUC: 0.771389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009001542176350508, AUC: 0.78562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000558992862701416, AUC: 0.8067799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008595412568290635, AUC: 0.829488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000538530021905899, AUC: 0.82298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007287021987509019, AUC: 0.8586119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968409717082977, AUC: 0.38355749999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006436823904514313, AUC: 0.803137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01135210377154964, AUC: 0.7909520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000583594560623169, AUC: 0.8077380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01014432730060993, AUC: 0.820932\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005280269980430603, AUC: 0.833491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008627873015875864, AUC: 0.8455999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870892643928528, AUC: 0.656811\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006438588798046112, AUC: 0.785184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011279767680876325, AUC: 0.77802\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005855887830257415, AUC: 0.790768\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009658517200167816, AUC: 0.799412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005556738078594208, AUC: 0.8051299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008975181662210143, AUC: 0.823952\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007047882378101349, AUC: 0.3414305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006809703409671784, AUC: 0.7605970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010495741154887888, AUC: 0.7505620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006465067863464356, AUC: 0.7901370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010764568288727561, AUC: 0.7835139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006005278527736664, AUC: 0.7971060000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010074336410749077, AUC: 0.8006639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952670216560364, AUC: 0.435129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006432079970836639, AUC: 0.7717779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010464417521316226, AUC: 0.7932920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005900460183620453, AUC: 0.7872790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009503405046935129, AUC: 0.809028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005679526627063752, AUC: 0.8025730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008954270877460441, AUC: 0.825492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970075964927673, AUC: 0.3704055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006620379984378815, AUC: 0.8133829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010983516284734896, AUC: 0.824464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005913313031196594, AUC: 0.8132949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009973079898569843, AUC: 0.826076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005637726485729218, AUC: 0.8200820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009417588061625414, AUC: 0.837888\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897422671318054, AUC: 0.598884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006093161702156067, AUC: 0.7742269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00976294757115959, AUC: 0.785188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005706937611103058, AUC: 0.7944450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008533435092113986, AUC: 0.8078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005440973341464996, AUC: 0.820147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008820098668041796, AUC: 0.837528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000686633974313736, AUC: 0.66552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006343769133090973, AUC: 0.7708459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00998953613904443, AUC: 0.77996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000589023768901825, AUC: 0.781485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009906717373593018, AUC: 0.791872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005734374821186065, AUC: 0.7928010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009384710352019508, AUC: 0.8104199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006928159296512603, AUC: 0.5240104999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006608406603336335, AUC: 0.7565659999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010737750010915321, AUC: 0.761488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006106067299842835, AUC: 0.7730439999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010074301509573908, AUC: 0.781744\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005812602043151855, AUC: 0.7897865\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009442508126249407, AUC: 0.805176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943551301956176, AUC: 0.5027984999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006626200675964356, AUC: 0.8112789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010643097292078603, AUC: 0.82374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005914508402347564, AUC: 0.8046445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010367770584503022, AUC: 0.8259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005529929101467133, AUC: 0.8113150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009098475008907885, AUC: 0.837008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006970823407173157, AUC: 0.3750015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655546128749848, AUC: 0.788638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011142519086894422, AUC: 0.803384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005962426960468292, AUC: 0.7917019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010091654510781317, AUC: 0.800416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005699568390846253, AUC: 0.8046564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009837563828666612, AUC: 0.8201919999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922467052936554, AUC: 0.5355885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006529574692249298, AUC: 0.762027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011207672263136004, AUC: 0.763824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132130324840545, AUC: 0.778235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010650047831016012, AUC: 0.7884599999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005724292099475861, AUC: 0.790807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00923902967188618, AUC: 0.8058000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Weighted Loss \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-2, 1e-2, 5e-3, 1e-3, 5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['pos_weight'] = torch.tensor([weight[1]])\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                _, train_auc = metric_utils.auc_sigmoid(train_loader_ratio, network) \n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"weighted\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9eed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7c5a840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0016997823119163514, AUC: 0.558314\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006072645485401154, AUC: 0.842452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009716608226299285, AUC: 0.8379909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004834069758653641, AUC: 0.900148\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015771014094352722, AUC: 0.356066\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000634692519903183, AUC: 0.8579279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006025081276893616, AUC: 0.8611769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006203497052192688, AUC: 0.850038\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0052621889114379886, AUC: 0.506359\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006397747695446014, AUC: 0.8428910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00060371533036232, AUC: 0.8695795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000621343046426773, AUC: 0.8683375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004644274711608887, AUC: 0.528089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006075803637504577, AUC: 0.8708735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006134110987186432, AUC: 0.8719720000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005852752625942231, AUC: 0.8708980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0066703901290893555, AUC: 0.48590049999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006325627267360687, AUC: 0.8388990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006375732719898224, AUC: 0.8245865000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006629515290260315, AUC: 0.7979080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028038963079452515, AUC: 0.49231699999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006599623858928681, AUC: 0.7861520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006590912640094757, AUC: 0.7669104999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006864423751831055, AUC: 0.6982765000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004255101680755615, AUC: 0.318401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005136841833591462, AUC: 0.867191\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005083841681480408, AUC: 0.878155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005344894826412201, AUC: 0.877799\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008323182165622711, AUC: 0.47827000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006758357584476471, AUC: 0.7239685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006325539946556092, AUC: 0.8345245000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006521005630493164, AUC: 0.800038\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003917799830436707, AUC: 0.6462760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006850146055221558, AUC: 0.6352025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006689300239086151, AUC: 0.719969\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006886860728263855, AUC: 0.661833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001697219967842102, AUC: 0.47321450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005878326892852783, AUC: 0.865403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005133723914623261, AUC: 0.883775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007523483037948608, AUC: 0.8700829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016597579717636108, AUC: 0.5441605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008124362230300903, AUC: 0.7969895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005502504110336304, AUC: 0.87827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000766532301902771, AUC: 0.868249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012438560128211974, AUC: 0.6666050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006068281829357148, AUC: 0.8312345000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006360677778720856, AUC: 0.8215645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000933250218629837, AUC: 0.79067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014040014743804932, AUC: 0.33838399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005898403227329255, AUC: 0.853621\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005866765379905701, AUC: 0.8779990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005921134948730469, AUC: 0.883903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024984368085861205, AUC: 0.3677265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006774233877658844, AUC: 0.6941780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006496633887290954, AUC: 0.7989740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006221855878829956, AUC: 0.8456645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594098210334778, AUC: 0.7385409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000645519495010376, AUC: 0.800268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006482151448726655, AUC: 0.810327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006482721865177154, AUC: 0.8256450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000888647586107254, AUC: 0.6260999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005252465009689332, AUC: 0.8604249999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005269242525100708, AUC: 0.879075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005223398208618164, AUC: 0.8870610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032928807735443117, AUC: 0.575028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006355304718017579, AUC: 0.8228695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006211168766021728, AUC: 0.846565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006694460809230805, AUC: 0.743297\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025339356660842895, AUC: 0.7512070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344216763973236, AUC: 0.8494069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006480650007724762, AUC: 0.845044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006553604304790497, AUC: 0.832336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036312453746795655, AUC: 0.3547155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006277574598789215, AUC: 0.8433689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006066323816776276, AUC: 0.854953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006239136457443237, AUC: 0.8501425000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007523654997348785, AUC: 0.5052559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006677979230880738, AUC: 0.799845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006048056185245514, AUC: 0.8559110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006199507415294648, AUC: 0.8518269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003379288911819458, AUC: 0.42852049999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006532731056213379, AUC: 0.7748625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006462254524230957, AUC: 0.7991010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006663556396961213, AUC: 0.7829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005600713968276978, AUC: 0.420735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006327516138553619, AUC: 0.811468\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006404102146625518, AUC: 0.8157089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006381041109561921, AUC: 0.8151734999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025884140729904173, AUC: 0.39293599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006726100146770477, AUC: 0.7495830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005990945398807525, AUC: 0.8565484999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006740671694278717, AUC: 0.7771405000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0052364997863769535, AUC: 0.3043825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006256416141986847, AUC: 0.8529685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006898742318153381, AUC: 0.622421\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006569638848304748, AUC: 0.7801335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010790262818336487, AUC: 0.438575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006007805168628692, AUC: 0.8378000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005811092555522919, AUC: 0.852346\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005791400074958801, AUC: 0.853048\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035956075191497803, AUC: 0.45495900000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005923809707164765, AUC: 0.85169\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005788674056529999, AUC: 0.87055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005274659991264343, AUC: 0.874343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001993749916553497, AUC: 0.6248184999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006761313080787659, AUC: 0.7174985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006219169497489929, AUC: 0.8301805000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006869738399982452, AUC: 0.7525240000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016783910989761352, AUC: 0.511706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006054649651050567, AUC: 0.8501110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006242438852787018, AUC: 0.851482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005843751132488251, AUC: 0.865298\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026753469705581667, AUC: 0.5525380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006361958086490631, AUC: 0.8286715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006122193336486816, AUC: 0.850582\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005859781503677368, AUC: 0.8613685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006756287336349487, AUC: 0.31604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000654455304145813, AUC: 0.7669595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006590117812156677, AUC: 0.816897\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004920793920755387, AUC: 0.8686889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005638533115386963, AUC: 0.413745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006607316732406616, AUC: 0.7737269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006646182239055633, AUC: 0.7689980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006728305220603943, AUC: 0.779582\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0050609631538391115, AUC: 0.3683245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000648770272731781, AUC: 0.8016079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006473062932491302, AUC: 0.8197240000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006592483818531037, AUC: 0.810562\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004557293891906738, AUC: 0.620023\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005867063403129578, AUC: 0.8433269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000505460500717163, AUC: 0.8722650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009741262793540955, AUC: 0.845247\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008003736138343811, AUC: 0.49342699999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006204034388065338, AUC: 0.8174170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006122876703739166, AUC: 0.8243729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006075150370597839, AUC: 0.8368345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022721703052520754, AUC: 0.34670449999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069917032122612, AUC: 0.8464045000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006584508419036865, AUC: 0.8733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005554920732975007, AUC: 0.895879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007038487672805786, AUC: 0.39189550000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006222145855426789, AUC: 0.82766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006008785367012024, AUC: 0.847276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006135859489440918, AUC: 0.848204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008943282961845398, AUC: 0.5743294999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009422122240066529, AUC: 0.8102360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004575718343257904, AUC: 0.891031\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005070996582508087, AUC: 0.893384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010093324780464173, AUC: 0.47215300000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000507506936788559, AUC: 0.8685080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043721334636211395, AUC: 0.888358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000973729133605957, AUC: 0.868792\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001944989800453186, AUC: 0.436279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005219227075576782, AUC: 0.88124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008779943585395813, AUC: 0.861796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007590562403202057, AUC: 0.8784750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021482292413711546, AUC: 0.5397585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006841371059417724, AUC: 0.6463199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006793534159660339, AUC: 0.7326045000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006273750364780426, AUC: 0.8272174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008935225307941437, AUC: 0.3827345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005252303183078766, AUC: 0.864974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006314181089401245, AUC: 0.871159\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006271073818206787, AUC: 0.8783299999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017696162462234497, AUC: 0.523392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006509748697280884, AUC: 0.821617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000639784187078476, AUC: 0.844428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006444270312786103, AUC: 0.8389400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.013285571098327637, AUC: 0.46666699999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006511038541793823, AUC: 0.80594\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006584191024303437, AUC: 0.7909099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006393175721168518, AUC: 0.8424290000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008748884797096252, AUC: 0.4952105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004901093244552612, AUC: 0.8837539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580108404159546, AUC: 0.87663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005513447225093842, AUC: 0.888051\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012735681533813476, AUC: 0.42423\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005897374451160431, AUC: 0.8325450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005771711468696595, AUC: 0.865367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009813527762889862, AUC: 0.822992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008403400778770446, AUC: 0.5741970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006241676807403565, AUC: 0.829565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005987150073051453, AUC: 0.8406075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006251179575920105, AUC: 0.8255290000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004211843013763428, AUC: 0.5747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006791666746139526, AUC: 0.6460989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006304303705692291, AUC: 0.8091735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006513157784938812, AUC: 0.7712114999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006663259506225586, AUC: 0.505383\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006814873814582824, AUC: 0.6655195000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006350915729999542, AUC: 0.8154840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006730606555938721, AUC: 0.7498849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006382246255874633, AUC: 0.4289625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006577343344688415, AUC: 0.7628949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000637249231338501, AUC: 0.8203725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000628269910812378, AUC: 0.829369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003200168490409851, AUC: 0.527217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005866137444972992, AUC: 0.857011\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004554653316736221, AUC: 0.885509\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043544906377792356, AUC: 0.891394\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004877902030944824, AUC: 0.45628250000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000530308723449707, AUC: 0.87249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004760218858718872, AUC: 0.8834399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006675277650356293, AUC: 0.8799779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007933143973350525, AUC: 0.593719\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006492676436901093, AUC: 0.8089434999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006282690167427063, AUC: 0.834386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006358433961868287, AUC: 0.8300299999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009931966066360474, AUC: 0.565779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006235862672328949, AUC: 0.8255210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006714372336864471, AUC: 0.790507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006215265691280365, AUC: 0.8346879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008504252135753632, AUC: 0.54792\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006906027197837829, AUC: 0.5555875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006201402842998505, AUC: 0.8398775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006142973899841309, AUC: 0.848249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058733377456665035, AUC: 0.43861300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006265124082565308, AUC: 0.8369740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000652091771364212, AUC: 0.793458\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006410577297210694, AUC: 0.8163630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000798772543668747, AUC: 0.538294\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004248533993959427, AUC: 0.895884\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000661555677652359, AUC: 0.8911380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008369237780570984, AUC: 0.8841579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002738882064819336, AUC: 0.3500775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006619011759757996, AUC: 0.7774740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000610093504190445, AUC: 0.8507140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006536541581153869, AUC: 0.799682\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011674661636352538, AUC: 0.43867100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006630411148071289, AUC: 0.770668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006784342527389526, AUC: 0.683881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006683802902698517, AUC: 0.7748370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004528341770172119, AUC: 0.582239\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006447971761226654, AUC: 0.807736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006714428365230561, AUC: 0.8253050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005557174682617187, AUC: 0.8752039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009845744669437408, AUC: 0.5996360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006231764256954193, AUC: 0.8475834999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006054428219795227, AUC: 0.8486285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427228748798371, AUC: 0.8440275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006147117376327515, AUC: 0.25621700000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006405791938304901, AUC: 0.8278274999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000656597524881363, AUC: 0.8359080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006434761583805084, AUC: 0.8537855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009927650094032287, AUC: 0.420885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006662409603595734, AUC: 0.773325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006655406951904297, AUC: 0.7781445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006287307441234589, AUC: 0.838752\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016363895535469056, AUC: 0.555877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006290450692176819, AUC: 0.84595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006345972120761871, AUC: 0.858433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006508311033248901, AUC: 0.8388395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009225323498249054, AUC: 0.520447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281939744949341, AUC: 0.780054\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006449726521968841, AUC: 0.7630945000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006466644406318664, AUC: 0.711801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000705904483795166, AUC: 0.6997950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006631162166595459, AUC: 0.7636100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006149191260337829, AUC: 0.8038345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580527126789093, AUC: 0.794104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0042973306179046634, AUC: 0.647145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006790182590484619, AUC: 0.6969175000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006167199015617371, AUC: 0.8587880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047950632870197296, AUC: 0.887837\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007900348901748657, AUC: 0.454348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240903437137604, AUC: 0.840487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006582238376140595, AUC: 0.7974385000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891978085041046, AUC: 0.7014760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008617754280567169, AUC: 0.710345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004974657893180847, AUC: 0.884937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046567003428936004, AUC: 0.885669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005207534730434418, AUC: 0.8881100000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008142055571079255, AUC: 0.617447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006352189183235168, AUC: 0.8324230000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006203385591506958, AUC: 0.838651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006378321051597596, AUC: 0.8041900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024169543981552126, AUC: 0.32676499999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006455606520175933, AUC: 0.8051539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006819164156913757, AUC: 0.6812640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006466636657714844, AUC: 0.8066030000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033523293733596802, AUC: 0.70921\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931257545948028, AUC: 0.5019925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006879639029502869, AUC: 0.6336875000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006348356604576111, AUC: 0.8394360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004955684185028076, AUC: 0.44321599999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006445192098617553, AUC: 0.8160475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006357621252536773, AUC: 0.773208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007008750140666962, AUC: 0.8697469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010608622431755065, AUC: 0.517732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008860278129577637, AUC: 0.722947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005364231765270233, AUC: 0.872293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004756963402032852, AUC: 0.881753\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022727229595184326, AUC: 0.339943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005966231524944305, AUC: 0.8705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000650141179561615, AUC: 0.8135810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006502585411071777, AUC: 0.819791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035425173044204713, AUC: 0.401528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006517716348171234, AUC: 0.7838350000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006544008255004882, AUC: 0.7862640000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006841811835765839, AUC: 0.7165339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009855904281139375, AUC: 0.426351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006284077763557435, AUC: 0.8199545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007051601409912109, AUC: 0.8084060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008120752573013305, AUC: 0.8112110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010229341983795167, AUC: 0.4762090000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006728013753890991, AUC: 0.7261629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006216755509376526, AUC: 0.8448125000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006550813913345336, AUC: 0.8106645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018491548895835876, AUC: 0.47329750000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006430257856845856, AUC: 0.8495499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006626276969909668, AUC: 0.866771\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004818156510591507, AUC: 0.869996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0030566236972808837, AUC: 0.5565095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006444640755653381, AUC: 0.832414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006392283737659454, AUC: 0.8511894999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006568226218223572, AUC: 0.838834\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002364069700241089, AUC: 0.42441650000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006676179766654969, AUC: 0.7265670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000633499264717102, AUC: 0.8324170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006019958555698395, AUC: 0.8574825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005741714477539062, AUC: 0.44097149999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006653405427932739, AUC: 0.745385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006675766110420227, AUC: 0.7564350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006589695811271667, AUC: 0.7933804999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002404703855514526, AUC: 0.6581430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006465355157852173, AUC: 0.8222370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006802307963371277, AUC: 0.73479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005728413164615632, AUC: 0.845914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011114223003387451, AUC: 0.5440655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006270652115345001, AUC: 0.8436159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005071337819099426, AUC: 0.8874470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006567976772785187, AUC: 0.8605160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008055076003074646, AUC: 0.55534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000655803769826889, AUC: 0.7902930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366526484489441, AUC: 0.8409685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006103576719760895, AUC: 0.8534925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012645052671432495, AUC: 0.3453050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006644133627414703, AUC: 0.7635245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000680445909500122, AUC: 0.7026640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005887763798236847, AUC: 0.8253325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015225135684013367, AUC: 0.60216\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006266160011291504, AUC: 0.834107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005840407013893127, AUC: 0.8566079999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006819238066673278, AUC: 0.7037970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029198591709136963, AUC: 0.390564\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000656250923871994, AUC: 0.768436\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006813076138496399, AUC: 0.7176975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395280957221985, AUC: 0.8382890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00650628662109375, AUC: 0.620513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006311907172203064, AUC: 0.8381509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006307304799556732, AUC: 0.856053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006145641505718231, AUC: 0.866148\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022404568195343016, AUC: 0.5014259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000655907928943634, AUC: 0.7882015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006386036574840546, AUC: 0.820943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006602338552474976, AUC: 0.8014695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002111778259277344, AUC: 0.529943\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006125784516334534, AUC: 0.8420865000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429934203624725, AUC: 0.8361565000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006203412115573883, AUC: 0.866703\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003615341305732727, AUC: 0.3594355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006617630124092102, AUC: 0.7832739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006384272575378418, AUC: 0.8197100000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687994122505188, AUC: 0.711103\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004423950433731079, AUC: 0.48318799999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048771901428699494, AUC: 0.8802559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005533826053142547, AUC: 0.88811\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005025244504213333, AUC: 0.894542\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008410386443138123, AUC: 0.520683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006444062888622284, AUC: 0.8027869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006262376308441162, AUC: 0.8237585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006702935099601745, AUC: 0.7453710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020445996522903444, AUC: 0.367874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007217204868793487, AUC: 0.838235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005151991248130798, AUC: 0.875703\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000480328232049942, AUC: 0.881757\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009605608284473419, AUC: 0.42092300000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006456184685230255, AUC: 0.7995324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006095726788043976, AUC: 0.842839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005313421487808227, AUC: 0.869983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000864767462015152, AUC: 0.626714\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006261599659919739, AUC: 0.854036\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00047776447236537933, AUC: 0.879344\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006258435249328613, AUC: 0.8792580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017244099378585815, AUC: 0.5021789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006292745471000672, AUC: 0.8352849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006295864284038544, AUC: 0.8320270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006430839598178863, AUC: 0.8223150000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012820234894752502, AUC: 0.35884400000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006073509156703949, AUC: 0.847628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006892384290695191, AUC: 0.85924\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00056467205286026, AUC: 0.881151\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0073797738552093504, AUC: 0.424442\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006066675186157226, AUC: 0.850459\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006784425675868988, AUC: 0.804617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004619345813989639, AUC: 0.890327\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005941615581512451, AUC: 0.2739195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000598706990480423, AUC: 0.852279\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006464935541152954, AUC: 0.8147665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005676141381263733, AUC: 0.8579299999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 CLASS SMOTE\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-4]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['loss_cap'] = None\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(100):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, 'num_models=100']\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66655269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfecae8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.00133796089887619, AUC: 0.4763200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018923410177230835, AUC: 0.7013625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001268166184425354, AUC: 0.712176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002451915383338928, AUC: 0.7507809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.020129199981689452, AUC: 0.606409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001889579474925995, AUC: 0.736443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014351112842559814, AUC: 0.7742290000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011559209823608398, AUC: 0.767754\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025856821537017824, AUC: 0.383844\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0032390503883361816, AUC: 0.715659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020191141963005065, AUC: 0.765722\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018205525279045105, AUC: 0.781976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029204572439193725, AUC: 0.4452090000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011962282061576844, AUC: 0.6517189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015718446969985962, AUC: 0.707349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016512113213539123, AUC: 0.756853\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008279363214969635, AUC: 0.5660890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.026428075790405275, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011767725944519043, AUC: 0.793352\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016583863496780395, AUC: 0.8007084999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002598055958747864, AUC: 0.539172\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009596129655838013, AUC: 0.822715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016985405683517456, AUC: 0.7633880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013769063353538512, AUC: 0.802889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004340981483459473, AUC: 0.5781945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013716971278190612, AUC: 0.7841830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013057164549827576, AUC: 0.8327600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012205746173858643, AUC: 0.869825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004762977361679077, AUC: 0.450496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013970341086387635, AUC: 0.6853130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007356478571891784, AUC: 0.296147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013690426349639892, AUC: 0.742317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011871834993362426, AUC: 0.36593200000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002227873206138611, AUC: 0.6737609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002038293480873108, AUC: 0.700777\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010250252485275268, AUC: 0.666224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008562153875827789, AUC: 0.43289999999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015909526348114014, AUC: 0.705132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011499525904655457, AUC: 0.785276\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015663057565689087, AUC: 0.778144\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0009548438191413879, AUC: 0.354882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006295521855354309, AUC: 0.8455025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958056688308716, AUC: 0.8313905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008273731470108032, AUC: 0.8511174999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038586961030960083, AUC: 0.4031395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006003174781799317, AUC: 0.8380799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006799028217792511, AUC: 0.8508695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009013473391532898, AUC: 0.7891669999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001628433585166931, AUC: 0.40024799999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007216636538505554, AUC: 0.772903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429203152656555, AUC: 0.8759909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006566248834133148, AUC: 0.7314529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004346894979476929, AUC: 0.49152399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006129622161388397, AUC: 0.8484690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007101680934429169, AUC: 0.8697525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008846960365772248, AUC: 0.8734239999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00301114284992218, AUC: 0.37804299999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931463181972504, AUC: 0.500499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931457221508026, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931460201740265, AUC: 0.5000005000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011287293434143067, AUC: 0.411639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006596712172031403, AUC: 0.795024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009122391343116761, AUC: 0.43468300000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009565997421741486, AUC: 0.579149\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002052827715873718, AUC: 0.363737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006101572513580322, AUC: 0.8542130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006580247282981873, AUC: 0.8347730000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010149293541908263, AUC: 0.781026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011022366881370544, AUC: 0.32935000000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005675205290317536, AUC: 0.8662850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006983230412006378, AUC: 0.8702779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008173762261867524, AUC: 0.864188\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010152350664138794, AUC: 0.6110829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006902856528759003, AUC: 0.8368290000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007636710703372955, AUC: 0.837102\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007220930457115173, AUC: 0.817923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005317711353302002, AUC: 0.6832149999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006598730683326722, AUC: 0.837288\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425081193447113, AUC: 0.872864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958560645580292, AUC: 0.836257\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.001411049485206604, AUC: 0.547491\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005160257816314698, AUC: 0.8609779999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007991407513618469, AUC: 0.785937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010080675780773163, AUC: 0.788861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007130982279777527, AUC: 0.59257\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000564499944448471, AUC: 0.840656\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006310046315193176, AUC: 0.8656980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007571522891521453, AUC: 0.8691279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036430232524871826, AUC: 0.484424\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000686310887336731, AUC: 0.822626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007631292641162872, AUC: 0.8550070000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010243207216262817, AUC: 0.8318285000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0049893462657928465, AUC: 0.5075845000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006332846879959107, AUC: 0.8445644999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007571321427822113, AUC: 0.8444379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000890285313129425, AUC: 0.7868799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016513422727584839, AUC: 0.5196839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006351334154605866, AUC: 0.8394940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007287234067916871, AUC: 0.8621340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005152677297592163, AUC: 0.834006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00396085250377655, AUC: 0.39164699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006674057245254516, AUC: 0.744829\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007198351919651032, AUC: 0.860552\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007165454924106597, AUC: 0.8839714999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016052773594856262, AUC: 0.3714295000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000603365033864975, AUC: 0.8421519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006911000013351441, AUC: 0.8736339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007869353890419006, AUC: 0.862039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006058679103851318, AUC: 0.47532800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006933558285236359, AUC: 0.4980005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006938275992870331, AUC: 0.4965\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940611302852631, AUC: 0.49550299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008828040659427643, AUC: 0.490707\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006270043849945069, AUC: 0.861954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007433834969997406, AUC: 0.7043475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008199469149112701, AUC: 0.8593885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002031044840812683, AUC: 0.541605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006851530969142913, AUC: 0.7966215000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007416760623455048, AUC: 0.7635295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007826801240444183, AUC: 0.7999169999999999\n",
      "\n",
      "[['capped_smote', 2, (0, 1), (100, 1), 0.005, 0.48445655, 0.006290576529422498, 0.6976287999999999, 0.006692606123109998, 0.7131176, 0.020903857176640003, 0.7717471499999999, 0.002425201765002498, 1, False, None], ['capped_smote', 2, (0, 1), (100, 1), 0.005, 0.44268604999999994, 0.012389197877122494, 0.7995092500000001, 0.010640677727662503, 0.7778203499999999, 0.024528438431852498, 0.7623705, 0.014355111517699987, 5, False, None], ['capped_smote', 2, (0, 1), (100, 1), 0.005, 0.492247, 0.004160789570049999, 0.79518755, 0.010907137736572496, 0.7911777, 0.012427706403560002, 0.80115225, 0.011437655091612497, 10, False, None]]\n"
     ]
    }
   ],
   "source": [
    "# 2 Class Capped SMOTE \n",
    "\n",
    "import math \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "caps = [1, 5, 10]\n",
    "\n",
    "for cap in caps:\n",
    "    \n",
    "    print(cap)\n",
    "    \n",
    "    loss_fn_args = {}\n",
    "    loss_fn_args['print_loss'] = False\n",
    "    loss_fn_args['print_capped'] = False \n",
    "    loss_fn_args['loss_cap'] = cap\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(n_epochs):\n",
    "                loss_fn_args['loss_cap'] = cap\n",
    "                _, _ = train.train_sigmoid_with_smote(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "    \n",
    "    \n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)\n",
    "\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "606da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = (col_names))\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563f22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0012810109853744506, AUC: 0.4544585\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 21\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSigmoidFocalLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     23\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:23\u001b[0m, in \u001b[0;36mtrain_sigmoid\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m network(data)\n\u001b[0;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/loss_fns.py:20\u001b[0m, in \u001b[0;36mSigmoidFocalLoss.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msigmoid_focal_loss(\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, targets, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# 2 CLASS Focal Loss\n",
    "# this code might not work \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [1e-4, 1e-5]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNet(NUM_CLASSES_REDUCED)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid(epoch, train_loader_ratio, network, optimizer, verbose=False, loss_fn=loss_fns.SigmoidFocalLoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"focal_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3]]\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d438d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6eb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7924e1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006921598315238952, AUC: 0.5632135\n",
      "\n",
      "Loss before cap\n",
      "tensor([0.6831, 0.7459, 0.5364, 0.7497, 0.6382, 0.6398, 0.6786, 0.6400, 0.6401,\n",
      "        0.7816, 0.5922, 0.6889, 0.8030, 0.6779, 0.7631, 0.6872, 0.5974, 0.6980,\n",
      "        0.7272, 0.7339, 0.7323, 0.6067, 0.6251, 0.5210, 0.7205, 0.8327, 0.6379,\n",
      "        0.6990, 0.6212, 0.4908, 0.7118, 0.6685, 0.5900, 0.6023, 0.5205, 0.7086,\n",
      "        0.6193, 0.6757, 0.7100, 0.6086, 0.5149, 0.6846, 0.7233, 0.5548, 0.5982,\n",
      "        0.5765, 0.6585, 0.7136, 0.7176, 0.6636, 0.6148, 0.5052, 0.7105, 0.6931,\n",
      "        0.6034, 0.7542, 0.6064, 0.6969, 0.6078, 0.6080, 0.6383, 0.7433, 0.6470,\n",
      "        0.5520], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 2.1579,    nan, 1.2739,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.0214,    nan, 1.5438, 1.0787,    nan, 1.7746,    nan,    nan, 1.2351,\n",
      "           nan, 1.8036, 2.3029,    nan,    nan,    nan,    nan, 1.6781,    nan,\n",
      "        1.7715,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.1300,\n",
      "           nan,    nan, 1.0166,    nan,    nan,    nan, 1.2885,    nan,    nan,\n",
      "           nan,    nan, 1.3378, 2.9578,    nan, 2.1528,    nan, 1.3932,    nan,\n",
      "           nan, 1.3675,    nan, 1.1432,    nan,    nan,    nan, 1.0213,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6284, 0.6326, 0.7166, 0.8558, 0.6018, 0.8419, 0.7810, 0.7182, 0.6671,\n",
      "        0.7296, 0.5275, 0.6001, 0.9214, 0.6683, 0.7445, 0.7128, 0.7619, 0.6269,\n",
      "        0.6085, 0.6900, 0.6682, 0.7153, 0.6448, 0.7235, 0.7118, 0.8081, 0.7448,\n",
      "        0.6017, 0.6866, 0.4971, 0.6344, 0.6922, 0.7478, 0.6766, 0.5336, 0.6953,\n",
      "        0.7440, 0.7062, 0.6859, 0.7706, 0.6719, 0.5437, 0.6233, 0.6613, 0.6371,\n",
      "        0.6355, 0.5208, 0.5996, 0.7773, 0.6754, 0.7489, 0.7360, 0.5466, 0.6566,\n",
      "        0.6697, 0.7545, 0.6887, 0.6568, 0.6468, 0.6412, 0.7067, 0.6816, 0.6428,\n",
      "        0.7305], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 0.9693, 2.1278,    nan, 2.1410, 2.4992,    nan,    nan,\n",
      "        0.9181,    nan,    nan, 2.7047,    nan, 1.3065, 1.0615, 1.2417,    nan,\n",
      "           nan,    nan,    nan, 0.9531,    nan, 0.8647, 1.4704, 1.9191, 1.0183,\n",
      "           nan,    nan,    nan,    nan,    nan, 1.6488,    nan,    nan,    nan,\n",
      "        3.0393, 1.0866, 1.7479, 2.0201,    nan,    nan,    nan, 1.2628,    nan,\n",
      "           nan,    nan,    nan, 1.4437,    nan, 2.7338, 0.9063,    nan,    nan,\n",
      "           nan, 1.2688, 1.2747,    nan,    nan,    nan, 1.5132,    nan,    nan,\n",
      "        1.3508], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6343, 0.7311, 0.6376, 0.6615, 0.6250, 0.6768, 0.6595, 0.7754, 0.8138,\n",
      "        0.6732, 0.7151, 0.5773, 0.6690, 0.5779, 0.7135, 0.6996, 0.7251, 0.7093,\n",
      "        0.7659, 0.7094, 0.5918, 0.8854, 0.6445, 0.6989, 0.6842, 0.8208, 0.7184,\n",
      "        0.7486, 0.7101, 0.7312, 0.6434, 0.8590, 0.5794, 0.5201, 0.8326, 0.5809,\n",
      "        0.7507, 0.7134, 0.7164, 0.7612, 0.6610, 0.6182, 0.6704, 0.6376, 0.7128,\n",
      "        0.7419, 0.7962, 0.6574, 0.7133, 0.7158, 0.6037, 0.6952, 0.7719, 0.6914,\n",
      "        0.6367, 0.7253, 0.6657, 0.6201, 0.7191, 0.5634, 0.7114, 0.7598, 0.7495,\n",
      "        0.6572], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.1580,    nan,    nan,    nan,    nan,    nan, 1.5501, 3.1286,\n",
      "           nan, 2.0660,    nan, 1.7870,    nan, 1.6301, 1.1375, 1.2407, 1.6259,\n",
      "        1.4352, 0.9657,    nan, 2.2998,    nan,    nan, 1.4819, 1.2521, 1.3888,\n",
      "        1.6097, 2.0324, 2.7085, 2.7188, 2.7626,    nan,    nan, 2.0445,    nan,\n",
      "        1.1093, 1.3019, 1.0204, 2.5057,    nan,    nan,    nan,    nan, 0.9500,\n",
      "        1.8736, 1.3249, 1.2668, 1.2341, 1.8731,    nan, 2.3377, 1.3838, 1.4135,\n",
      "           nan, 1.2449,    nan,    nan, 1.7886,    nan, 1.0733, 1.7101, 1.1011,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7833, 0.5282, 0.5316, 0.6680, 0.6571, 0.6862, 0.7362, 0.7583, 0.5898,\n",
      "        0.8151, 0.7067, 0.5099, 0.8979, 0.6343, 0.7498, 0.6842, 0.6525, 0.8035,\n",
      "        0.7615, 0.6847, 0.6504, 0.8426, 0.7240, 0.5781, 0.7308, 0.5840, 0.7145,\n",
      "        0.7305, 0.7129, 0.7466, 0.6612, 0.7419, 0.7976, 0.6312, 0.7200, 0.5808,\n",
      "        0.5884, 0.7670, 0.6382, 0.7894, 0.7658, 0.6272, 0.7234, 0.7096, 0.6961,\n",
      "        0.8182, 0.7754, 0.7021, 0.6083, 0.7268, 0.7391, 0.7391, 0.7302, 0.7752,\n",
      "        0.7429, 0.6040, 0.7569, 0.7168, 0.5774, 0.7562, 0.6545, 0.5696, 0.7616,\n",
      "        0.6389], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2984, 2.5525, 2.9199, 4.0865, 0.0000, 2.9676, 1.3985, 3.4296, 3.3712,\n",
      "        1.6399, 1.7867, 3.7598, 4.1661, 1.6141, 1.3759, 1.2884, 1.4531, 1.6762,\n",
      "        2.0665, 0.9382, 3.3416, 1.7585, 1.9609, 3.3166, 1.1825, 2.4039, 1.4907,\n",
      "        1.0096, 1.9025, 1.5767, 2.2295, 2.3715, 1.7700, 2.6462, 0.9815, 1.9285,\n",
      "        2.7496, 1.5725, 1.5702, 2.3733, 1.5671, 2.7240, 1.2895, 0.9639, 1.5140,\n",
      "        1.5124, 3.1174, 2.8396, 2.5467, 1.0118, 2.1895, 1.3566, 1.0053, 1.0231,\n",
      "        1.2427, 2.5387, 1.3266, 1.2624, 3.1939, 1.4725, 1.4662, 2.0737, 2.0423,\n",
      "        2.8315], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6903, 0.6799, 0.7210, 0.6980, 0.6437, 0.8123, 0.7029, 0.6140, 0.7433,\n",
      "        0.5757, 0.6750, 0.7402, 0.6342, 0.6535, 0.7392, 0.7465, 0.7933, 0.7587,\n",
      "        0.6262, 0.6413, 0.6627, 0.5516, 0.6423, 0.6214, 0.5430, 0.6638, 0.6264,\n",
      "        0.7683, 0.7524, 0.6945, 0.6074, 0.7512, 0.5728, 0.6644, 0.6429, 0.5752,\n",
      "        0.7371, 0.6271, 0.7361, 0.6437, 0.6798, 0.7362, 0.8179, 0.7430, 0.5748,\n",
      "        0.5531, 0.6259, 0.6749, 0.7164, 0.6640, 0.5925, 0.6673, 0.7097, 0.6028,\n",
      "        0.7024, 0.6519, 0.6492, 0.7378, 0.6473, 0.7169, 0.6285, 0.7345, 0.5664,\n",
      "        0.5999], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.9438, 0.8807,    nan,    nan, 2.8609, 1.1071,    nan, 0.9342,\n",
      "           nan,    nan, 0.8944,    nan,    nan, 1.0858, 1.4259, 1.8712, 1.2875,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.3286, 0.9496,    nan,    nan, 0.7127,    nan, 3.8867,    nan,    nan,\n",
      "        1.0683,    nan, 0.9914,    nan,    nan, 1.0978, 1.3738, 0.6471,    nan,\n",
      "           nan,    nan,    nan, 0.9454,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.8026, 1.6167,    nan, 0.8649,    nan, 1.2705,    nan, 1.0720,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6401, 0.7080, 0.6124, 0.6722, 0.7169, 0.6338, 0.7213, 0.8479, 0.7107,\n",
      "        0.5792, 0.6343, 0.6769, 0.7181, 0.6499, 0.7218, 0.6978, 0.7961, 0.8412,\n",
      "        0.6053, 0.7098, 0.7375, 0.7903, 0.7479, 0.8070, 0.7226, 0.6277, 0.6975,\n",
      "        0.7938, 0.7039, 0.7714, 0.6350, 0.7752, 0.7214, 0.7520, 0.7148, 0.5909,\n",
      "        0.5461, 0.6440, 0.6060, 0.5523, 0.6850, 0.6297, 0.7078, 0.6569, 0.5881,\n",
      "        0.7878, 0.6574, 0.7269, 0.7168, 0.6304, 0.5501, 0.7117, 0.7449, 0.7224,\n",
      "        0.6758, 0.7017, 0.6682, 0.7491, 0.6614, 0.7293, 0.5576, 0.6335, 0.6632,\n",
      "        0.6039], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.0819,    nan,    nan, 1.4889,    nan, 2.0125, 3.9010,    nan,\n",
      "           nan,    nan,    nan, 1.2863,    nan, 1.1336, 1.0819, 1.3927, 1.5065,\n",
      "           nan, 1.1191, 1.1759, 1.8558, 1.1838, 1.5995,    nan,    nan, 1.2603,\n",
      "        2.3063, 1.4626, 2.0289,    nan, 1.7254, 1.2852, 1.2514,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 2.2760,    nan,    nan,\n",
      "        2.3632, 1.6570, 1.9608, 1.3655,    nan,    nan, 1.1177, 2.1182, 0.9105,\n",
      "           nan, 1.5148, 2.0249, 2.0739,    nan, 1.2694,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6309, 0.6555, 0.6986, 0.6892, 0.7397, 0.7505, 0.7308, 0.7991, 0.6277,\n",
      "        0.5693, 0.6334, 0.7251, 0.6714, 0.8030, 0.5440, 0.7853, 0.7093, 0.6191,\n",
      "        0.6812, 0.6781, 0.8316, 0.7828, 0.5562, 0.6079, 0.6813, 0.7128, 0.7274,\n",
      "        0.6358, 0.7251, 0.7068, 0.6122, 0.7195, 0.5217, 0.7123, 0.6978, 0.7392,\n",
      "        0.6782, 0.6773, 0.4774, 0.7218, 0.5941, 0.6179, 0.8107, 0.7727, 0.6035,\n",
      "        0.7722, 0.7668, 0.7200, 0.7331, 0.5983, 0.6187, 0.7459, 0.6992, 0.6120,\n",
      "        0.5806, 0.6079, 0.7191, 0.6411, 0.6118, 0.6342, 0.6755, 0.7191, 0.7296,\n",
      "        0.7221], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8685, 3.0334, 1.5953, 1.3514, 1.2914, 1.4896, 1.3647, 1.4351, 2.2023,\n",
      "        3.3195, 3.0573, 1.1677, 2.5663, 1.4662, 3.6810, 2.3874, 0.9721, 2.1686,\n",
      "        1.7936, 0.0000, 1.9000, 1.6000, 3.1316, 2.1356, 2.6424, 1.1890, 1.0417,\n",
      "        2.3795, 1.1984, 1.2649, 2.1710, 1.6529, 2.4886, 2.6868, 1.6396, 2.1305,\n",
      "        2.3003, 1.7984, 2.6004, 0.9452, 3.2557, 2.5009, 1.2829, 1.5665, 2.2780,\n",
      "        1.0589, 1.6801, 1.4154, 1.1624, 1.8391, 2.0968, 1.5606, 1.9800, 1.7197,\n",
      "        3.8758, 2.0191, 1.1072, 1.9422, 2.9917, 1.4188, 1.7057, 2.2211, 3.1288,\n",
      "        1.2844], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7498, 0.8218, 0.6781, 0.6413, 0.7590, 0.7319, 0.7242, 0.6876, 0.5775,\n",
      "        0.6113, 0.7598, 0.7943, 0.7378, 0.6929, 0.7043, 0.6357, 0.7991, 0.7432,\n",
      "        0.6346, 0.6055, 0.7436, 0.7022, 0.7083, 0.7228, 0.6062, 0.6448, 0.6031,\n",
      "        0.6850, 0.7072, 0.5958, 0.7247, 0.7027, 0.7254, 0.7894, 0.7480, 0.7095,\n",
      "        0.7117, 0.7895, 0.6832, 0.7287, 0.6323, 0.7454, 0.7336, 0.7289, 0.7000,\n",
      "        0.7499, 0.5874, 0.6578, 0.7350, 0.6030, 0.7048, 0.6206, 0.7041, 0.7090,\n",
      "        0.7484, 0.6991, 0.6455, 0.6532, 0.6110, 0.7372, 0.6806, 0.7731, 0.6602,\n",
      "        0.7242], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5593, 1.6384, 2.4840, 1.8793, 1.5511, 0.9378, 1.1242, 2.1623, 3.6102,\n",
      "        3.4105, 1.7284, 2.2454, 1.0398, 2.6073, 0.8655, 3.3112, 0.8629, 1.5031,\n",
      "        2.7279, 2.4052, 1.4487, 1.3464, 1.2603, 1.1359, 0.0000, 3.2797, 2.8027,\n",
      "        3.0588, 0.9502, 3.5580, 2.6219, 1.7012, 2.2424, 1.3358, 1.1268, 3.0723,\n",
      "        0.9611, 1.3725, 1.1679, 1.0232, 2.8417, 2.3091, 1.2092, 0.8878, 3.2734,\n",
      "        3.0868, 3.1355, 3.6668, 0.6762, 2.6009, 1.4559, 2.6741, 3.0501, 1.1921,\n",
      "        1.0489, 1.0225, 2.0831, 2.2262, 3.6542, 1.8517, 1.3676, 1.0913, 2.7663,\n",
      "        1.1100], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6646, 0.7216, 0.7038, 0.7404, 0.7057, 0.6167, 0.8136, 0.7865, 0.7121,\n",
      "        0.6362, 0.7792, 0.6658, 0.7411, 0.7872, 0.6128, 0.6136, 0.7216, 0.7309,\n",
      "        0.8734, 0.6258, 0.5447, 0.6479, 0.7391, 0.5802, 0.6492, 0.5697, 0.5335,\n",
      "        0.6083, 0.5742, 0.5543, 0.7023, 0.7760, 0.7751, 0.5247, 0.7793, 0.6868,\n",
      "        0.7275, 0.6229, 0.7283, 0.6707, 0.6055, 0.6852, 0.6719, 0.6033, 0.7330,\n",
      "        0.6453, 0.6787, 0.6461, 0.5736, 0.7323, 0.7309, 0.7057, 0.6168, 0.5916,\n",
      "        0.7607, 0.7903, 0.8047, 0.6523, 0.7672, 0.5968, 0.7061, 0.6723, 0.6358,\n",
      "        0.6970], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2406, 1.1753, 1.4304, 1.2293, 0.9581,    nan, 1.6736, 1.0688, 1.3384,\n",
      "           nan, 1.5681,    nan, 1.1073, 1.3144,    nan,    nan, 1.5927, 1.3367,\n",
      "        2.8070,    nan,    nan,    nan, 1.7431,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 1.8799, 2.8469,    nan, 2.8929, 1.2668,\n",
      "        0.9114,    nan, 1.3394, 2.6776,    nan,    nan, 1.5707,    nan, 1.1357,\n",
      "           nan,    nan,    nan,    nan, 0.9752, 2.3292, 1.1052,    nan,    nan,\n",
      "        2.1223, 1.3619, 2.3338,    nan, 1.4613,    nan,    nan, 1.2142,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7145, 0.6344, 0.5506, 0.7495, 0.7407, 0.6616, 0.6919, 0.7536, 0.6877,\n",
      "        0.5829, 0.6430, 0.7394, 0.6798, 0.7321, 0.6201, 0.6801, 0.5896, 0.6265,\n",
      "        0.6878, 0.6842, 0.6186, 0.8466, 0.7698, 0.7747, 0.8332, 0.6185, 0.7476,\n",
      "        0.6019, 0.8067, 0.5933, 0.6862, 0.6779, 0.6503, 0.7643, 0.7107, 0.7879,\n",
      "        0.7026, 0.6814, 0.6751, 0.7027, 0.5822, 0.7281, 0.8299, 0.7485, 0.7020,\n",
      "        0.7527, 0.6714, 0.5699, 0.5841, 0.8051, 0.6592, 0.8382, 0.6943, 0.6130,\n",
      "        0.6624, 0.6264, 0.7296, 0.6505, 0.6685, 0.7520, 0.6091, 0.6592, 0.5673,\n",
      "        0.7971], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.1853, 2.3070,    nan, 1.1451, 1.1173,    nan,    nan, 1.4066,    nan,\n",
      "           nan,    nan, 1.7932, 2.3864, 1.1510,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.8398, 1.4984, 1.5767, 1.8636,    nan, 2.3755,\n",
      "           nan, 1.8335,    nan,    nan,    nan,    nan, 1.3255,    nan, 1.3279,\n",
      "        2.0168,    nan,    nan,    nan,    nan, 1.1774, 1.7585, 1.7023, 0.9081,\n",
      "        1.1680,    nan,    nan,    nan, 1.9395,    nan, 1.8251,    nan,    nan,\n",
      "           nan,    nan, 1.3496,    nan,    nan, 1.6744,    nan,    nan,    nan,\n",
      "        1.6841], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6792, 0.7073, 0.6902, 0.6717, 0.7652, 0.7495, 0.7267, 0.6509, 0.7291,\n",
      "        0.6138, 0.7150, 0.5857, 0.7535, 0.7625, 0.6478, 0.6709, 0.6231, 0.7511,\n",
      "        0.6386, 0.5501, 0.6235, 0.7475, 0.7196, 0.6139, 0.6906, 0.7730, 0.7614,\n",
      "        0.6262, 0.6533, 0.7520, 0.7312, 0.5594, 0.6350, 0.7147, 0.6715, 0.7683,\n",
      "        0.6154, 0.6470, 0.7344, 0.6649, 0.7205, 0.7135, 0.6385, 0.7687, 0.7885,\n",
      "        0.7562, 0.5510, 0.6010, 0.7084, 0.5597, 0.6136, 0.7052, 0.7869, 0.7530,\n",
      "        0.7572, 0.7632, 0.7159, 0.7008, 0.6071, 0.6019, 0.6480, 0.7677, 0.6391,\n",
      "        0.6440], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.7062,    nan, 1.0025,    nan, 1.2798, 1.6208, 1.5968,    nan, 1.4786,\n",
      "           nan, 1.2154,    nan, 1.6043, 1.6897,    nan,    nan,    nan, 1.0188,\n",
      "           nan,    nan,    nan, 1.1073, 1.4784,    nan, 1.7947, 1.8299, 1.4535,\n",
      "           nan,    nan, 1.2128, 1.2222,    nan, 2.2596, 1.0467,    nan, 1.5012,\n",
      "           nan, 2.0595, 1.2727,    nan, 1.2893,    nan,    nan, 2.2081, 2.9516,\n",
      "        1.5252,    nan,    nan, 1.0255,    nan,    nan,    nan, 1.7788, 1.2271,\n",
      "        0.9205, 1.1973, 1.3205,    nan,    nan,    nan,    nan, 1.5575,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7559, 0.7602, 0.7451, 0.7731, 0.6783, 0.6801, 0.7625, 0.6928, 0.6994,\n",
      "        0.7492, 0.7904, 0.6431, 0.7237, 0.7046, 0.5247, 0.7122, 0.5490, 0.6597,\n",
      "        0.7751, 0.7656, 0.7296, 0.7325, 0.7573, 0.5259, 0.5814, 0.6014, 0.5842,\n",
      "        0.7912, 0.6694, 0.6431, 0.8391, 0.7345, 0.6065, 0.6905, 0.6111, 0.8057,\n",
      "        0.6987, 0.5703, 0.7576, 0.5817, 0.6972, 0.6153, 0.8173, 0.6961, 0.7374,\n",
      "        0.7008, 0.5624, 0.5669, 0.6182, 0.7272, 0.5791, 0.6376, 0.6185, 0.6201,\n",
      "        0.7072, 0.7429, 0.5280, 0.7743, 0.7293, 0.6562, 0.7285, 0.7690, 0.5796,\n",
      "        0.5824], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8148, 1.2651, 2.1549, 1.5350,    nan, 1.2009, 1.4832, 1.0793, 1.3700,\n",
      "        0.9848, 1.2869,    nan, 1.0446, 1.4806,    nan, 1.3288,    nan,    nan,\n",
      "        1.9070, 2.3173, 2.0182, 3.2920, 1.3118,    nan,    nan,    nan,    nan,\n",
      "        1.3514,    nan,    nan, 2.3413, 0.8665,    nan,    nan,    nan, 1.5679,\n",
      "           nan,    nan, 1.1941,    nan, 1.5825,    nan, 2.8870, 1.2098, 1.9116,\n",
      "        1.0448,    nan,    nan,    nan, 1.2431,    nan,    nan,    nan,    nan,\n",
      "           nan, 1.8367,    nan, 1.8183, 1.3779,    nan, 1.6478, 2.0316,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7607, 0.6527, 0.7201, 0.5839, 0.8073, 0.7004, 0.7535, 0.7373, 0.6360,\n",
      "        0.6427, 0.6716, 0.7821, 0.5553, 0.6224, 0.7080, 0.6873, 0.7507, 0.7354,\n",
      "        0.5684, 0.8164, 0.6116, 0.6222, 0.6278, 0.7956, 0.7465, 0.6275, 0.5947,\n",
      "        0.6037, 0.7450, 0.5744, 0.7278, 0.7405, 0.6114, 0.7209, 0.7040, 0.6433,\n",
      "        0.5177, 0.7199, 0.6396, 0.5174, 0.6394, 0.7808, 0.7158, 0.7620, 0.6708,\n",
      "        0.7259, 0.6265, 0.5279, 0.7265, 0.6092, 0.6618, 0.6705, 0.7531, 0.6243,\n",
      "        0.7646, 0.7547, 0.5925, 0.7801, 0.7571, 0.6398, 0.7237, 0.6033, 0.6742,\n",
      "        0.6928], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.4677, 2.5010, 1.4295, 2.0172, 1.1376, 1.0410, 1.2153, 1.8968, 1.4880,\n",
      "        3.5639, 2.0364, 1.5218, 3.7291, 3.1471, 1.0089, 1.9947, 0.9360, 1.2217,\n",
      "        1.9832, 1.8779, 2.4064, 2.2959, 1.8743, 2.2541, 1.4370, 3.5211, 2.8278,\n",
      "        2.5382, 1.1365, 3.2829, 1.5051, 1.3610, 1.9125, 1.8271, 1.9185, 1.8073,\n",
      "        2.2191, 0.8981, 2.8453, 2.8798, 2.1968, 2.9190, 0.9424, 1.6243, 1.5508,\n",
      "        1.7289, 2.2183, 3.1192, 1.0541, 3.3239, 0.0000, 3.1811, 1.1048, 2.6299,\n",
      "        1.7338, 0.9852, 3.1536, 1.7352, 1.0196, 1.0027, 1.6266, 2.1395, 1.0590,\n",
      "        1.2746], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6644, 0.6979, 0.6045, 0.6159, 0.6385, 0.5114, 0.6482, 0.8083, 0.5389,\n",
      "        0.6893, 0.5870, 0.7585, 0.6598, 0.7179, 0.6077, 0.7034, 0.7995, 0.6538,\n",
      "        0.7208, 0.7420, 0.6996, 0.6586, 0.7302, 0.7320, 0.6973, 0.7259, 0.6845,\n",
      "        0.6425, 0.7477, 0.7604, 0.6382, 0.5085, 0.7084, 0.6944, 0.7305, 0.7549,\n",
      "        0.7085, 0.6387, 0.8124, 0.7429, 0.7143, 0.7349, 0.5728, 0.6309, 0.7099,\n",
      "        0.7239, 0.5789, 0.7628, 0.6374, 0.6805, 0.7375, 0.7460, 0.6626, 0.7236,\n",
      "        0.7324, 0.6801, 0.7295, 0.7063, 0.7435, 0.6058, 0.6674, 0.6896, 0.6353,\n",
      "        0.8345], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5955, 1.2969,    nan,    nan,    nan,    nan,    nan, 1.0064,    nan,\n",
      "        0.7462,    nan,    nan,    nan, 0.9320,    nan, 2.0256, 1.9194,    nan,\n",
      "        1.4872, 2.0543,    nan,    nan, 0.9965, 1.1048, 1.0866, 1.0995, 1.6301,\n",
      "           nan, 1.3789, 1.5019,    nan,    nan, 0.9799, 1.0817, 1.5295, 2.8884,\n",
      "        0.9062,    nan, 1.9350, 0.9042, 1.1568, 2.4045,    nan,    nan, 0.9895,\n",
      "        1.2165,    nan, 1.8904,    nan,    nan, 1.0604, 1.1203,    nan, 1.0451,\n",
      "        0.9077,    nan, 0.9674, 1.8494, 2.5613,    nan, 1.2646, 2.0214,    nan,\n",
      "        3.1216], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6138, 0.5268, 0.6880, 0.7315, 0.6230, 0.6028, 0.6631, 0.6331, 0.7102,\n",
      "        0.7182, 0.7450, 0.6643, 0.5758, 0.7928, 0.8095, 0.6030, 0.6774, 0.7261,\n",
      "        0.5734, 0.7480, 0.7428, 0.8370, 0.7451, 0.7051, 0.5835, 0.6409, 0.6342,\n",
      "        0.7216, 0.6432, 0.7527, 0.7340, 0.7425, 0.5896, 0.5959, 0.6258, 0.7121,\n",
      "        0.6730, 0.8217, 0.7003, 0.6609, 0.8178, 0.7247, 0.6292, 0.5715, 0.6993,\n",
      "        0.6872, 0.7551, 0.6861, 0.7585, 0.6297, 0.5403, 0.6101, 0.6491, 0.7340,\n",
      "        0.6689, 0.5647, 0.7515, 0.7404, 0.6770, 0.6711, 0.7303, 0.5951, 0.6502,\n",
      "        0.6926], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 2.2810, 1.3653, 2.3932,    nan,    nan,    nan, 0.9554,\n",
      "        1.0469, 1.2800, 2.9432,    nan, 1.5188, 1.6367,    nan,    nan, 1.0294,\n",
      "           nan, 0.9169, 1.4546, 1.7208, 0.9902, 1.5489,    nan,    nan,    nan,\n",
      "        1.6016,    nan, 1.7766, 1.4151, 1.2334,    nan,    nan, 3.4043, 0.9907,\n",
      "           nan, 1.6066, 1.0861,    nan, 1.8909, 1.5829,    nan,    nan, 1.0421,\n",
      "           nan, 1.9849, 1.2863, 1.3548,    nan,    nan,    nan, 1.5130,    nan,\n",
      "           nan,    nan, 1.6817, 1.1986, 1.4686,    nan, 1.2289,    nan,    nan,\n",
      "        2.0834], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7379, 0.5891, 0.6497, 0.7713, 0.5822, 0.7372, 0.8172, 0.7410, 0.7073,\n",
      "        0.7780, 0.6631, 0.6812, 0.6827, 0.7214, 0.6374, 0.6085, 0.5984, 0.7434,\n",
      "        0.6469, 0.5674, 0.7018, 0.7466, 0.6633, 0.6404, 0.6012, 0.7395, 0.7413,\n",
      "        0.6750, 0.8764, 0.7077, 0.5677, 0.7319, 0.7191, 0.5795, 0.7665, 0.6866,\n",
      "        0.7142, 0.6085, 0.7775, 0.7271, 0.8073, 0.7457, 0.5271, 0.5337, 0.6205,\n",
      "        0.7482, 0.6557, 0.7919, 0.7712, 0.6873, 0.6466, 0.7500, 0.6431, 0.6479,\n",
      "        0.6208, 0.7695, 0.6160, 0.7602, 0.7131, 0.6885, 0.7432, 0.5969, 0.7043,\n",
      "        0.6200], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.1794, 2.6613, 3.2654, 1.8958, 3.0814, 1.7131, 1.8009, 1.4257, 1.2842,\n",
      "        1.7496, 1.7973, 1.1511, 2.9771, 1.7058, 2.7008, 2.9896, 2.8298, 1.2715,\n",
      "        2.0022, 3.1040, 1.0884, 1.5627, 2.7471, 2.9681, 3.2903, 1.5544, 1.3479,\n",
      "        3.1284, 2.6265, 2.3318, 2.9461, 1.0378, 1.2692, 3.2492, 1.2794, 2.5838,\n",
      "        1.8972, 3.3123, 1.2732, 1.7334, 1.5215, 1.0263, 3.5765, 3.0864, 2.5715,\n",
      "        1.8905, 0.0000, 1.6402, 1.9335, 2.1351, 3.1672, 1.3084, 2.6623, 1.7464,\n",
      "        3.1273, 1.6880, 2.8517, 1.6710, 2.2591, 2.5815, 0.9669, 2.4450, 0.9639,\n",
      "        2.6742], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6053, 0.6970, 0.5652, 0.5861, 0.7189, 0.5702, 0.6456, 0.6321, 0.6861,\n",
      "        0.6866, 0.7127, 0.6743, 0.5935, 0.6285, 0.7176, 0.7256, 0.6050, 0.7211,\n",
      "        0.6005, 0.6813, 0.7042, 0.8219, 0.5659, 0.6096, 0.6234, 0.6238, 0.8451,\n",
      "        0.7205, 0.7316, 0.5786, 0.6906, 0.7309, 0.6967, 0.6491, 0.7032, 0.7086,\n",
      "        0.5948, 0.7023, 0.7450, 0.7147, 0.5542, 0.7195, 0.6768, 0.7376, 0.6581,\n",
      "        0.6325, 0.7558, 0.5965, 0.7421, 0.5854, 0.5960, 0.6991, 0.6161, 0.7051,\n",
      "        0.7227, 0.8454, 0.8032, 0.6259, 0.6574, 0.8012, 0.7546, 0.7007, 0.7516,\n",
      "        0.7233], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan,    nan, 1.3541,    nan,    nan,    nan, 1.1479,\n",
      "        1.4126, 1.8470, 1.3376,    nan,    nan, 1.2818, 2.1900,    nan, 1.1295,\n",
      "           nan,    nan, 2.0322, 2.3706,    nan,    nan,    nan,    nan, 1.6759,\n",
      "           nan, 1.1911,    nan, 0.9727, 1.6930, 1.4981,    nan, 1.2144,    nan,\n",
      "           nan, 1.2491, 1.9026, 1.5097,    nan, 1.6985, 1.6742, 1.5415,    nan,\n",
      "           nan, 3.9797,    nan, 1.1257,    nan,    nan,    nan,    nan,    nan,\n",
      "        0.9833, 2.1167, 2.1308,    nan, 2.3350, 0.9538, 2.7815,    nan, 1.2388,\n",
      "        1.0537], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6806, 0.5686, 0.5665, 0.7227, 0.6260, 0.6983, 0.7559, 0.6937, 0.7007,\n",
      "        0.6876, 0.6300, 0.7342, 0.6941, 0.6978, 0.6505, 0.5793, 0.6980, 0.6897,\n",
      "        0.7165, 0.7450, 0.7252, 0.6938, 0.7608, 0.6670, 0.6893, 0.7120, 0.7077,\n",
      "        0.7382, 0.5941, 0.7146, 0.7196, 0.7550, 0.6398, 0.7242, 0.5616, 0.7093,\n",
      "        0.7369, 0.6550, 0.6151, 0.6944, 0.7558, 0.7678, 0.7146, 0.6323, 0.7571,\n",
      "        0.8189, 0.7419, 0.8262, 0.7477, 0.7185, 0.5328, 0.6697, 0.7455, 0.7860,\n",
      "        0.7691, 0.5966, 0.6802, 0.7472, 0.6293, 0.5090, 0.7646, 0.6997, 0.6919,\n",
      "        0.7613], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.1745,    nan,    nan, 1.1362,    nan, 0.9991, 1.0338, 1.8668, 1.0534,\n",
      "           nan,    nan, 1.5131, 0.8650, 0.8763,    nan,    nan,    nan,    nan,\n",
      "        2.4334, 1.1569, 1.0130,    nan, 1.3324,    nan, 1.2104, 1.3657, 0.9338,\n",
      "        1.2128,    nan, 1.1675, 1.4477, 1.6314,    nan, 1.4208,    nan,    nan,\n",
      "        0.7527,    nan,    nan, 1.5744, 0.9718, 2.2071,    nan,    nan, 1.2422,\n",
      "        1.8424, 2.0460, 1.9604, 1.5072,    nan,    nan,    nan, 1.1736, 1.9271,\n",
      "        0.9220,    nan,    nan, 0.8369,    nan,    nan, 1.1942, 0.8962,    nan,\n",
      "        1.7196], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7008, 0.6284, 0.7139, 0.4985, 0.6645, 0.7602, 0.5741, 0.7236, 0.7732,\n",
      "        0.6017, 0.7714, 0.6192, 0.8456, 0.7002, 0.6960, 0.8023, 0.4925, 0.6105,\n",
      "        0.6464, 0.6608, 0.5792, 0.6520, 0.5635, 0.5976, 0.7101, 0.6050, 0.6080,\n",
      "        0.7915, 0.7022, 0.7011, 0.6859, 0.6766, 0.6880, 0.5830, 0.8347, 0.7744,\n",
      "        0.8070, 0.6055, 0.5139, 0.7738, 0.5953, 0.6177, 0.7205, 0.7079, 0.6557,\n",
      "        0.6863, 0.6549, 0.5810, 0.8302, 0.6351, 0.6737, 0.6344, 0.6599, 0.5686,\n",
      "        0.7480, 0.7527, 0.6187, 0.6650, 0.8080, 0.7530, 0.7159, 0.7446, 0.7286,\n",
      "        0.6640], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.0223,    nan, 2.1173, 0.8912,    nan, 1.4641, 1.1314,\n",
      "           nan, 1.0966,    nan, 2.1426, 1.3798,    nan, 1.7764,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 1.3203,    nan,    nan,\n",
      "        1.4145,    nan, 1.6303, 1.3979,    nan, 3.2490,    nan, 1.4928, 1.2414,\n",
      "        1.1658,    nan,    nan, 1.4526,    nan,    nan, 1.3058, 1.4565,    nan,\n",
      "        3.7488,    nan,    nan, 2.2360,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.7794, 1.8682,    nan,    nan,    nan, 1.2575, 1.2428, 1.1479, 1.4163,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6547, 0.6826, 0.5298, 0.6430, 0.5477, 0.6769, 0.7792, 0.6990, 0.6361,\n",
      "        0.5893, 0.5245, 0.6931, 0.7236, 0.7550, 0.7054, 0.6321, 0.6991, 0.7688,\n",
      "        0.6232, 0.7773, 0.7176, 0.7330, 0.7456, 0.7015, 0.7224, 0.6469, 0.6459,\n",
      "        0.6448, 0.5558, 0.8279, 0.8029, 0.6464, 0.7887, 0.6738, 0.5844, 0.6588,\n",
      "        0.6274, 0.6631, 0.6877, 0.6031, 0.7052, 0.5826, 0.6412, 0.7870, 0.6335,\n",
      "        0.6554, 0.6809, 0.7911, 0.6817, 0.7529, 0.7266, 0.5933, 0.6204, 0.7444,\n",
      "        0.7123, 0.6600, 0.6454, 0.6770, 0.8086, 0.8002, 0.8117, 0.6493, 0.6787,\n",
      "        0.7717], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.8611, 0.0000, 3.2353, 1.9896, 1.5919, 1.8561, 2.4954, 1.6064, 2.2978,\n",
      "        2.4154, 3.1981, 1.5537, 1.8724, 1.3523, 1.6182, 2.0514, 1.2030, 1.3311,\n",
      "        3.2557, 2.6264, 1.0256, 1.8841, 1.6026, 1.1370, 1.0057, 1.7941, 1.3447,\n",
      "        2.9265, 2.9697, 2.2391, 2.4144, 2.2964, 2.7836, 1.5824, 2.2322, 2.6374,\n",
      "        1.3507, 2.5408, 0.8493, 2.0430, 1.8165, 3.1278, 1.5625, 1.2402, 3.3023,\n",
      "        2.2958, 1.5280, 1.8569, 2.2893, 1.0127, 1.4837, 2.9719, 3.3673, 1.0999,\n",
      "        0.9585, 1.5824, 1.9845, 3.5844, 2.7955, 0.9627, 1.9162, 3.2033, 1.5074,\n",
      "        1.1676], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6673, 0.6470, 0.6001, 0.8397, 0.5801, 0.8090, 0.6873, 0.5793, 0.6435,\n",
      "        0.6181, 0.6835, 0.7403, 0.7358, 0.6476, 0.6484, 0.6128, 0.5937, 0.6042,\n",
      "        0.7622, 0.5760, 0.7985, 0.7105, 0.7707, 0.6315, 0.5740, 0.6426, 0.7436,\n",
      "        0.7091, 0.6336, 0.6299, 0.6308, 0.8459, 0.6014, 0.6985, 0.7037, 0.7340,\n",
      "        0.7180, 0.8093, 0.7567, 0.6076, 0.6911, 0.7633, 0.8267, 0.6935, 0.7235,\n",
      "        0.6073, 0.6333, 0.6482, 0.7248, 0.6778, 0.7710, 0.6830, 0.6470, 0.7898,\n",
      "        0.6178, 0.4593, 0.6010, 0.7409, 0.7422, 0.7084, 0.7579, 0.7370, 0.5625,\n",
      "        0.7513], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.4701,    nan, 2.4479, 1.8440,    nan,    nan,\n",
      "           nan,    nan, 1.2286, 1.9088,    nan,    nan,    nan,    nan,    nan,\n",
      "        2.1177,    nan, 1.5222, 1.9217, 1.7724,    nan,    nan,    nan, 1.3465,\n",
      "        1.1542,    nan,    nan,    nan, 2.7626,    nan,    nan,    nan, 1.9048,\n",
      "        1.3107, 2.0510, 1.2736,    nan, 2.2171, 1.4738, 3.5389,    nan, 1.9730,\n",
      "           nan,    nan,    nan, 1.4730,    nan, 1.8996, 1.2475,    nan, 2.1518,\n",
      "           nan,    nan,    nan, 1.1892, 1.1302, 1.1555, 1.3590, 1.7459,    nan,\n",
      "        1.3440], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.6458, 0.6410, 0.7406, 0.7063, 0.6537, 0.5887, 0.6702, 0.5825, 0.5149,\n",
      "        0.7292, 0.5692, 0.6561, 0.5797, 0.6868, 0.6796, 0.7380, 0.7566, 0.6767,\n",
      "        0.6471, 0.4971, 0.6168, 0.6472, 0.5740, 0.7441, 0.7318, 0.5590, 0.7292,\n",
      "        0.6963, 0.6422, 0.5792, 0.6838, 0.6461, 0.7455, 0.7506, 0.6693, 0.5473,\n",
      "        0.6429, 0.6441, 0.7288, 0.7795, 0.6565, 0.6977, 0.5896, 0.7862, 0.6485,\n",
      "        0.6075, 0.7351, 0.6552, 0.7069, 0.6095, 0.6823, 0.7597, 0.8440, 0.7639,\n",
      "        0.7211, 0.5573, 0.7657, 0.6122, 0.7548, 0.6744, 0.8279, 0.7616, 0.7890,\n",
      "        0.6711], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.3206,    nan,    nan,    nan, 2.2324,    nan, 1.2642, 0.9304,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 1.5058, 0.9291,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.6456,    nan, 1.3410, 1.0051, 1.7955,    nan,\n",
      "           nan,    nan, 0.9864, 1.6353,    nan,    nan,    nan, 1.6480,    nan,\n",
      "           nan, 1.2671,    nan, 1.9368,    nan,    nan, 1.3198, 2.0426, 1.4309,\n",
      "           nan,    nan, 1.7179,    nan, 1.6261,    nan, 2.0894, 1.2110, 1.5077,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6396, 0.6709, 0.5185, 0.6402, 0.5852, 0.6642, 0.7409, 0.6836, 0.6754,\n",
      "        0.7724, 0.6520, 0.6705, 0.5844, 0.7051, 0.7776, 0.7204, 0.6024, 0.8190,\n",
      "        0.6392, 0.6592, 0.5843, 0.7409, 0.6274, 0.8473, 0.6527, 0.7269, 0.7513,\n",
      "        0.6395, 0.7619, 0.7782, 0.6530, 0.5999, 0.6992, 0.7307, 0.6979, 0.6647,\n",
      "        0.7189, 0.6444, 0.7605, 0.7311, 0.7812, 0.7145, 0.5384, 0.5693, 0.6213,\n",
      "        0.7948, 0.5017, 0.7102, 0.7836, 0.6509, 0.6048, 0.6315, 0.7557, 0.7063,\n",
      "        0.6529, 0.7469, 0.6219, 0.6161, 0.8867, 0.5991, 0.6394, 0.5805, 0.7206,\n",
      "        0.7309], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan,    nan,    nan,    nan, 1.2787, 1.6222,    nan,\n",
      "        1.1662,    nan, 1.4426,    nan, 1.2077, 1.7390, 1.7978,    nan, 1.8172,\n",
      "           nan,    nan,    nan, 0.8373,    nan, 1.5995,    nan, 1.0264, 1.2924,\n",
      "           nan, 1.1249, 1.2800,    nan,    nan, 1.3399,    nan,    nan,    nan,\n",
      "        1.3371,    nan, 0.9069, 0.9410, 1.7111, 2.4533,    nan,    nan,    nan,\n",
      "        1.4594,    nan,    nan, 1.9365,    nan,    nan,    nan, 1.0540, 2.7801,\n",
      "           nan, 1.6093,    nan,    nan, 3.1781,    nan,    nan,    nan, 1.0895,\n",
      "        1.4755], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7138, 0.7203, 0.5837, 0.7404, 0.6434, 0.7225, 0.7653, 0.7257, 0.6353,\n",
      "        0.6017, 0.6614, 0.8902, 0.7252, 0.7424, 0.5983, 0.6433, 0.5496, 0.5588,\n",
      "        0.7714, 0.8104, 0.6875, 0.6328, 0.7278, 0.6113, 0.6575, 0.7093, 0.5920,\n",
      "        0.7364, 0.7402, 0.6776, 0.6652, 0.7949, 0.5574, 0.7569, 0.7204, 0.7728,\n",
      "        0.7011, 0.7543, 0.5728, 0.6039, 0.8929, 0.7220, 0.6708, 0.6595, 0.5816,\n",
      "        0.7892, 0.7991, 0.6221, 0.7564, 0.6875, 0.7348, 0.7200, 0.6235, 0.6576,\n",
      "        0.6907, 0.5577, 0.6474, 0.7624, 0.6787, 0.7529, 0.7621, 0.5093, 0.7092,\n",
      "        0.6788], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.8296, 1.1507,    nan, 1.2566,    nan, 0.8703,    nan,    nan,    nan,\n",
      "           nan,    nan, 3.2854, 0.8332, 1.1548,    nan,    nan,    nan,    nan,\n",
      "        1.1349, 1.0474, 1.0592,    nan, 1.3474,    nan,    nan, 1.0581,    nan,\n",
      "        0.9222, 1.0014,    nan,    nan, 1.5777,    nan, 1.4135, 1.0116, 1.4641,\n",
      "        0.8571, 1.4022,    nan,    nan, 2.7390, 1.6707,    nan,    nan,    nan,\n",
      "        1.4990, 1.8661,    nan, 2.0917,    nan, 1.0591, 2.3356,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.1091,    nan, 0.9653, 1.4165,    nan, 1.2651,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6383, 0.7171, 0.7444, 0.6915, 0.6770, 0.6607, 0.7763, 0.5517, 0.7100,\n",
      "        0.7346, 0.7143, 0.7945, 0.6260, 0.7890, 0.5274, 0.7337, 0.5982, 0.7212,\n",
      "        0.8210, 0.5070, 0.6499, 0.7641, 0.7583, 0.7898, 0.6148, 0.6662, 0.8056,\n",
      "        0.7132, 0.7259, 0.5787, 0.8211, 0.6889, 0.7560, 0.7837, 0.7055, 0.6269,\n",
      "        0.7025, 0.7597, 0.7276, 0.7133, 0.7254, 0.5252, 0.7089, 0.5955, 0.7446,\n",
      "        0.7213, 0.7342, 0.5519, 0.5966, 0.5941, 0.7256, 0.7011, 0.5690, 0.7254,\n",
      "        0.6282, 0.6346, 0.7791, 0.7628, 0.8335, 0.7060, 0.7248, 0.5790, 0.7258,\n",
      "        0.7132], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.6901, 1.4136, 2.4681, 0.0000, 4.0592, 2.5795, 1.8425, 2.9555, 0.9757,\n",
      "        1.7490, 1.0396, 2.1604, 2.1261, 2.6313, 2.8059, 1.1693, 3.5766, 1.9329,\n",
      "        1.6246, 3.1056, 2.3165, 2.0211, 1.8134, 1.9685, 2.7309, 1.6774, 1.4892,\n",
      "        1.0525, 0.9784, 2.4841, 2.3024, 1.1358, 1.4284, 1.4840, 0.9755, 3.3215,\n",
      "        1.1076, 1.8957, 0.9071, 1.1584, 1.7291, 3.4947, 1.1299, 2.4541, 1.4688,\n",
      "        3.6876, 1.2810, 2.5585, 4.1361, 2.2063, 1.2586, 1.2907, 3.1797, 3.5577,\n",
      "        2.2568, 2.7780, 1.3533, 1.0842, 1.2385, 1.4086, 2.5103, 2.3558, 2.9378,\n",
      "        1.6403], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6386, 0.6142, 0.7396, 0.6121, 0.6486, 0.6924, 0.6266, 0.6186, 0.7507,\n",
      "        0.7741, 0.7014, 0.6376, 0.6300, 0.6216, 0.5983, 0.7123, 0.8034, 0.8069,\n",
      "        0.6696, 0.5965, 0.7667, 0.6985, 0.6624, 0.6796, 0.6186, 0.7715, 0.7553,\n",
      "        0.6401, 0.6526, 0.7216, 0.6700, 0.7929, 0.7112, 0.7437, 0.7660, 0.6880,\n",
      "        0.7554, 0.6517, 0.7332, 0.7685, 0.7442, 0.5971, 0.6320, 0.9130, 0.7782,\n",
      "        0.6926, 0.5519, 0.5896, 0.7879, 0.6804, 0.6704, 0.6570, 0.5439, 0.5375,\n",
      "        0.4960, 0.6031, 0.6428, 0.6407, 0.6569, 0.7188, 0.6104, 0.8188, 0.6804,\n",
      "        0.5972], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.1034, 1.9695, 1.9158, 2.9237, 3.0278, 2.7468, 3.1054, 2.8894, 1.2382,\n",
      "        1.3460, 2.1454, 2.9721, 2.7896, 1.9488, 2.9960, 1.3885, 1.4053, 1.6081,\n",
      "        1.3149, 2.9058, 1.2027, 3.0446, 3.5416, 2.8269, 2.5182, 1.1657, 1.4363,\n",
      "        2.3336, 3.4964, 1.1860, 1.4162, 1.9429, 1.6629, 0.7656, 1.4587, 1.3337,\n",
      "        0.9014, 2.6168, 1.1345, 1.7397, 1.7710, 3.3460, 3.1678, 2.5581, 1.4285,\n",
      "        1.0654, 3.3829, 2.7202, 3.0206, 2.8258, 1.7033, 2.8299, 3.6136, 3.5015,\n",
      "        3.0488, 2.9522, 2.7709, 4.1317, 2.6860, 1.0741, 0.0000, 1.8930, 2.4812,\n",
      "        3.6901], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7258, 0.7681, 0.6087, 0.7548, 0.6605, 0.7264, 0.5976, 0.7514, 0.6678,\n",
      "        0.6729, 0.6095, 0.6108, 0.7156, 0.5723, 0.7275, 0.6974, 0.7271, 0.6168,\n",
      "        0.5878, 0.6279, 0.6212, 0.8069, 0.7366, 0.5694, 0.6151, 0.7555, 0.7429,\n",
      "        0.6916, 0.7971, 0.7750, 0.7108, 0.6569, 0.7690, 0.7091, 0.7173, 0.5950,\n",
      "        0.7523, 0.6786, 0.6153, 0.6929, 0.6204, 0.6108, 0.7501, 0.7690, 0.7414,\n",
      "        0.6641, 0.6943, 0.8335, 0.6195, 0.7529, 0.7114, 0.6234, 0.6850, 0.7438,\n",
      "        0.7510, 0.6280, 0.7054, 0.7526, 0.7610, 0.8305, 0.7289, 0.6799, 0.5321,\n",
      "        0.8056], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.4528,    nan, 0.8744,    nan, 0.8713,    nan, 1.7952,    nan,\n",
      "        1.0274,    nan,    nan, 1.3732,    nan, 1.3251, 0.9483, 0.8685,    nan,\n",
      "           nan,    nan,    nan, 1.5069, 1.0570,    nan,    nan, 1.7249, 1.0014,\n",
      "        0.9378, 3.0013,    nan, 1.0325,    nan, 1.6591, 1.2215, 0.9196,    nan,\n",
      "        1.6219,    nan,    nan, 1.0115,    nan,    nan, 1.3918, 2.7340, 1.4385,\n",
      "           nan, 0.9318, 2.3477,    nan, 1.8037, 1.7327,    nan, 2.2906, 1.0354,\n",
      "        1.4926,    nan, 1.5301, 1.0965, 1.1390, 2.0475, 1.1027, 1.3096,    nan,\n",
      "        2.4838], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6279, 0.7708, 0.7454, 0.5738, 0.6938, 0.5725, 0.7006, 0.8513, 0.7522,\n",
      "        0.6808, 0.7529, 0.6613, 0.6493, 0.7926, 0.6215, 0.7394, 0.6751, 0.8149,\n",
      "        0.6356, 0.7454, 0.7144, 0.7348, 0.9135, 0.6693, 0.7641, 0.6993, 0.6569,\n",
      "        0.5630, 0.5781, 0.6251, 0.7283, 0.5951, 0.6174, 0.6821, 0.5379, 0.6079,\n",
      "        0.8102, 0.6887, 0.7571, 0.6324, 0.6876, 0.7662, 0.5790, 0.7935, 0.6330,\n",
      "        0.6202, 0.7737, 0.6349, 0.7537, 0.5851, 0.6901, 0.7179, 0.6451, 0.7005,\n",
      "        0.6792, 0.7475, 0.6564, 0.7617, 0.7594, 0.5371, 0.7444, 0.6326, 0.6582,\n",
      "        0.6720], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Distance\n",
      "tensor([2.2695, 1.8076, 1.6731, 2.8876, 2.2041, 1.7040, 1.6246, 2.2201, 1.1105,\n",
      "        0.8366, 1.6278, 1.7117, 3.5674, 1.9450, 2.5822, 1.3902, 1.1990, 1.4050,\n",
      "        2.3217, 1.0849, 1.9276, 1.2853, 2.9911, 2.1755, 1.1824, 1.5054, 1.7366,\n",
      "        2.5692, 1.6844, 1.8588, 1.6405, 1.4267, 2.8694, 1.8759, 2.0112, 1.9127,\n",
      "        1.2185, 1.9580, 1.5937, 2.5462, 2.1940, 1.4033, 2.0665, 1.2850, 1.9635,\n",
      "        2.0185, 1.1124, 2.9488, 2.5211, 2.8693, 2.2231, 1.0368, 2.0095, 1.3712,\n",
      "        2.1094, 1.2337, 2.4917, 0.9233, 1.4401, 2.7859, 2.4632, 1.7040, 1.7020,\n",
      "        1.0562], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7280, 0.7862, 0.5992, 0.7383, 0.6572, 0.6452, 0.7184, 0.6748, 0.7178,\n",
      "        0.6683, 0.7259, 0.6572, 0.7179, 0.6702, 0.7938, 0.5410, 0.7856, 0.7172,\n",
      "        0.7167, 0.8307, 0.6174, 0.7430, 0.6275, 0.5906, 0.6609, 0.6611, 0.7509,\n",
      "        0.7382, 0.6783, 0.7051, 0.7546, 0.5992, 0.7668, 0.5724, 0.7222, 0.6548,\n",
      "        0.4607, 0.7460, 0.7431, 0.8643, 0.4726, 0.7782, 0.6951, 0.5894, 0.6611,\n",
      "        0.6569, 0.8043, 0.6005, 0.6125, 0.6392, 0.7764, 0.6029, 0.5914, 0.6802,\n",
      "        0.5555, 0.7162, 0.6205, 0.6809, 0.6836, 0.7182, 0.6393, 0.5745, 0.5444,\n",
      "        0.7596], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.7707, 1.3777, 1.7494, 1.7386, 2.3538, 1.3227, 1.6266, 2.0090, 0.8571,\n",
      "        3.0989, 2.7637, 1.8662, 1.2606, 2.3316, 2.2047, 2.4480, 1.3143, 1.7700,\n",
      "        1.4470, 2.0491, 2.6288, 1.4058, 1.7333, 2.6493, 1.7212, 1.3195, 1.3866,\n",
      "        1.5527, 3.1728, 1.0446, 2.1624, 2.9170, 1.5333, 2.2763, 1.1083, 1.7508,\n",
      "        3.1899, 1.0423, 1.1953, 2.1848, 2.9358, 1.4611, 1.8228, 1.6888, 2.2779,\n",
      "        1.9570, 1.1837, 2.9624, 1.9273, 1.3227, 1.8828, 1.9224, 2.6077, 3.1453,\n",
      "        2.3414, 1.0132, 2.0656, 1.1136, 2.2799, 1.1383, 2.3081, 2.1574, 3.6872,\n",
      "        2.1155], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7601, 0.6764, 0.7141, 0.6341, 0.7372, 0.6045, 0.6697, 0.7376, 0.6009,\n",
      "        0.7073, 0.5475, 0.7418, 0.6036, 0.6605, 0.7459, 0.7441, 0.7863, 0.6599,\n",
      "        0.5404, 0.6735, 0.6619, 0.7348, 0.7194, 0.7105, 0.8211, 0.7198, 0.6312,\n",
      "        0.7196, 0.6616, 0.7141, 0.7203, 0.6447, 0.7044, 0.7447, 0.7472, 0.6877,\n",
      "        0.7614, 0.6328, 0.5640, 0.6596, 0.6739, 0.7583, 0.7300, 0.6745, 0.7542,\n",
      "        0.6792, 0.6730, 0.5647, 0.6005, 0.6172, 0.6741, 0.7231, 0.7232, 0.6509,\n",
      "        0.6289, 0.7012, 0.6252, 0.5291, 0.5568, 0.7758, 0.7299, 0.6084, 0.7174,\n",
      "        0.7433], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8105, 1.6078, 1.2295, 2.4259, 2.0791, 2.2600, 1.8902, 1.5550, 1.8343,\n",
      "        1.5950, 2.6983, 1.1359, 2.7218, 0.0000, 1.4294, 1.3673, 1.3787, 3.0640,\n",
      "        3.0234, 2.5785, 2.8197, 1.0618, 1.1733, 1.4434, 1.7143, 1.0134, 2.5809,\n",
      "        1.7335, 1.5730, 1.0045, 2.0253, 2.3508, 1.3275, 1.2931, 1.1190, 2.2711,\n",
      "        0.9270, 2.1300, 3.0507, 2.3708, 1.4422, 1.9983, 1.1676, 3.3570, 1.2872,\n",
      "        3.5770, 2.0448, 3.8593, 3.0373, 3.3752, 2.5148, 1.0618, 1.0297, 1.8939,\n",
      "        2.2565, 1.1905, 1.3957, 2.8030, 2.2537, 1.2564, 0.9817, 1.6477, 1.3083,\n",
      "        1.4135], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7469, 0.5831, 0.5732, 0.6479, 0.5905, 0.6276, 0.5340, 0.7392, 0.7687,\n",
      "        0.6833, 0.7602, 0.6503, 0.7080, 0.5271, 0.7387, 0.6432, 0.6419, 0.7276,\n",
      "        0.6422, 0.7341, 0.5969, 0.5588, 0.7364, 0.6953, 0.6954, 0.7255, 0.6641,\n",
      "        0.6255, 0.7075, 0.7037, 0.6259, 0.5614, 0.5571, 0.6800, 0.7395, 0.6252,\n",
      "        0.7295, 0.7647, 0.6470, 0.6822, 0.6471, 0.7291, 0.7688, 0.6844, 0.6045,\n",
      "        0.7625, 0.4275, 0.7184, 0.7232, 0.7473, 0.6859, 0.7605, 0.7399, 0.7421,\n",
      "        0.6608, 0.7069, 0.6620, 0.5786, 0.5767, 0.6101, 0.4848, 0.7421, 0.6519,\n",
      "        0.6221], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.3584,    nan,    nan,    nan,    nan,    nan,    nan, 0.9994, 0.9873,\n",
      "           nan, 1.2743,    nan, 1.4518,    nan, 1.1919,    nan,    nan, 1.4595,\n",
      "           nan, 0.8254,    nan,    nan, 1.8975,    nan, 1.1588, 2.7583,    nan,\n",
      "           nan, 1.8524, 2.4834,    nan,    nan,    nan,    nan, 1.2235,    nan,\n",
      "        1.3732, 0.8605,    nan, 1.5172,    nan, 1.3693, 1.1106, 2.4161,    nan,\n",
      "        1.5671,    nan, 1.5026, 1.0497, 2.5230,    nan, 0.9973, 1.2311, 1.2365,\n",
      "           nan, 2.5115,    nan,    nan,    nan,    nan,    nan, 1.5084,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6278, 0.7364, 0.6661, 0.6060, 0.7052, 0.5939, 0.7219, 0.8612, 0.5690,\n",
      "        0.6045, 0.7080, 0.7705, 0.6747, 0.6285, 0.6910, 0.7770, 0.9120, 0.6424,\n",
      "        0.6266, 0.5594, 0.6643, 0.5784, 0.6610, 0.7062, 0.6800, 0.7347, 0.6806,\n",
      "        0.7526, 0.6464, 0.6385, 0.6593, 0.5932, 0.6695, 0.6507, 0.7261, 0.7080,\n",
      "        0.7527, 0.5350, 0.7508, 0.7395, 0.5703, 0.6141, 0.6400, 0.6857, 0.7808,\n",
      "        0.7552, 0.7241, 0.8510, 0.6438, 0.7000, 0.6630, 0.7556, 0.6902, 0.7429,\n",
      "        0.7413, 0.7054, 0.7612, 0.5630, 0.6330, 0.6031, 0.6692, 0.7538, 0.7615,\n",
      "        0.6105], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.4506,    nan, 1.0350,    nan,    nan, 3.2907,    nan,\n",
      "           nan, 1.2433, 1.1436,    nan,    nan,    nan, 1.1997, 2.4457,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.9937,    nan, 1.2145,    nan,\n",
      "        0.8825,    nan,    nan,    nan,    nan, 2.5414,    nan, 1.0234, 1.0679,\n",
      "        1.2838,    nan, 1.3975, 1.3064,    nan,    nan,    nan, 1.1782, 1.5498,\n",
      "        1.2152, 1.0650, 1.5533,    nan,    nan,    nan, 1.8372,    nan, 1.1808,\n",
      "        1.3122, 0.9918, 0.7596,    nan,    nan,    nan,    nan, 1.3733, 1.2971,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7301, 0.7910, 0.8286, 0.6233, 0.7342, 0.7586, 0.6967, 0.6255, 0.7176,\n",
      "        0.7040, 0.7562, 0.6229, 0.7092, 0.6725, 0.6370, 0.7179, 0.6611, 0.7599,\n",
      "        0.7308, 0.6149, 0.5852, 0.7018, 0.6137, 0.6733, 0.7558, 0.6424, 0.8048,\n",
      "        0.6889, 0.6989, 0.7147, 0.5589, 0.8411, 0.6594, 0.7672, 0.7485, 0.6329,\n",
      "        0.6240, 0.7429, 0.5962, 0.8461, 0.7360, 0.6291, 0.6760, 0.6359, 0.8215,\n",
      "        0.7472, 0.6388, 0.6826, 0.6578, 0.6044, 0.7563, 0.7473, 0.6807, 0.6054,\n",
      "        0.5433, 0.5748, 0.6943, 0.5777, 0.6673, 0.7524, 0.6873, 0.5806, 0.9361,\n",
      "        0.5978], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2798, 1.9367, 1.9647,    nan, 2.7855, 2.1569, 1.0555,    nan, 1.2783,\n",
      "        1.6591, 1.8064,    nan, 1.2097, 2.3889,    nan, 1.1565,    nan,    nan,\n",
      "        1.0301,    nan,    nan, 1.0455,    nan,    nan, 1.5795,    nan, 1.5999,\n",
      "        1.0261, 1.2244, 2.0060,    nan, 1.8658,    nan, 1.9639, 1.7108,    nan,\n",
      "           nan, 0.9381,    nan,    nan, 0.9813,    nan, 1.9301,    nan, 1.5639,\n",
      "        1.5183,    nan,    nan,    nan,    nan, 1.0925, 1.6272,    nan,    nan,\n",
      "           nan,    nan, 1.9707,    nan,    nan,    nan, 1.0696,    nan, 2.8511,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6939, 0.7054, 0.5695, 0.7226, 0.7333, 0.7300, 0.5480, 0.6830, 0.6421,\n",
      "        0.8297, 0.5874, 0.7396, 0.7564, 0.6689, 0.6165, 0.7890, 0.7232, 0.6489,\n",
      "        0.6557, 0.6327, 0.7235, 0.7408, 0.6757, 0.5433, 0.6084, 0.7541, 0.7867,\n",
      "        0.6818, 0.7132, 0.8066, 0.6304, 0.7829, 0.6935, 0.5430, 0.7512, 0.6344,\n",
      "        0.6900, 0.7135, 0.5760, 0.7192, 0.6787, 0.6173, 0.7140, 0.6853, 0.7133,\n",
      "        0.6356, 0.5699, 0.6514, 0.5297, 0.8444, 0.7336, 0.6450, 0.6536, 0.7242,\n",
      "        0.7501, 0.6965, 0.6386, 0.8273, 0.7064, 0.7339, 0.7384, 0.7255, 0.6867,\n",
      "        0.6843], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.9862, 1.0044, 4.4975, 0.8340, 0.8305, 1.5223, 2.8928, 2.5539, 1.9528,\n",
      "        1.6215, 2.3511, 1.0066, 1.7875, 1.7767, 2.5552, 1.3528, 1.3329, 1.6863,\n",
      "        2.3746, 1.6206, 0.8937, 1.8927, 1.4441, 2.6529, 3.0849, 1.1977, 1.0523,\n",
      "        1.4333, 3.4732, 1.7245, 1.9678, 4.1964, 3.2941, 2.4157, 1.0814, 2.7803,\n",
      "        1.9519, 1.5351, 2.9667, 1.4881, 2.2695, 2.4995, 1.3763, 1.2202, 2.0481,\n",
      "        1.1978, 3.4363, 3.0488, 2.1727, 2.3203, 0.9578, 0.0000, 1.9108, 1.7270,\n",
      "        0.9831, 2.2653, 2.7419, 2.1973, 0.8319, 0.9412, 1.1978, 0.9126, 1.2504,\n",
      "        1.2888], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7194, 0.7792, 0.6927, 0.6525, 0.7761, 0.5462, 0.7776, 0.5186, 0.7892,\n",
      "        0.6713, 0.6421, 0.7094, 0.5347, 0.7707, 0.6522, 0.6993, 0.7297, 0.6893,\n",
      "        0.6956, 0.7525, 0.6316, 0.6039, 0.6258, 0.7162, 0.5854, 0.7261, 0.7929,\n",
      "        0.8284, 0.7433, 0.6173, 0.8002, 0.6188, 0.5926, 0.6134, 0.6085, 0.7079,\n",
      "        0.7340, 0.6166, 0.6252, 0.6740, 0.6918, 0.7069, 0.5725, 0.8077, 0.6894,\n",
      "        0.6527, 0.7240, 0.7307, 0.7160, 0.6242, 0.6779, 0.6717, 0.5699, 0.6848,\n",
      "        0.5678, 0.7215, 0.6857, 0.6627, 0.6883, 0.7995, 0.6810, 0.6128, 0.6544,\n",
      "        0.6195], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.2585, 1.4638,    nan,    nan, 1.3874,    nan, 1.4372,    nan, 1.9578,\n",
      "           nan,    nan, 1.2219,    nan, 1.4230,    nan,    nan,    nan,    nan,\n",
      "           nan, 1.3094,    nan,    nan,    nan, 0.9464,    nan, 0.9195, 1.9688,\n",
      "        2.1209, 2.3289,    nan, 2.1433,    nan,    nan,    nan,    nan, 0.9608,\n",
      "           nan,    nan, 2.2022,    nan, 1.0254, 1.0776,    nan, 1.2311, 1.2556,\n",
      "           nan, 1.1469, 1.1166, 1.3060,    nan, 1.2961, 2.1649,    nan,    nan,\n",
      "           nan, 1.1741,    nan,    nan,    nan, 2.0674,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6569, 0.6942, 0.7727, 0.5931, 0.6784, 0.7506, 0.5880, 0.7176, 0.7093,\n",
      "        0.7017, 0.7847, 0.7405, 0.7033, 0.5711, 0.6224, 0.7779, 0.8560, 0.7253,\n",
      "        0.7979, 0.6588, 0.6880, 0.7317, 0.8350, 0.6897, 0.7356, 0.5953, 0.7115,\n",
      "        0.7292, 0.6390, 0.7667, 0.5950, 0.5626, 0.8167, 0.6742, 0.7345, 0.6576,\n",
      "        0.6490, 0.7451, 0.8105, 0.7132, 0.7025, 0.7253, 0.6782, 0.7795, 0.6594,\n",
      "        0.6662, 0.7414, 0.7269, 0.6701, 0.6606, 0.6727, 0.6892, 0.6173, 0.7308,\n",
      "        0.5810, 0.7258, 0.4728, 0.7037, 0.6241, 0.6983, 0.5378, 0.7720, 0.7505,\n",
      "        0.7759], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 0.9522, 2.1075,    nan,    nan, 1.0303,    nan, 1.1198, 1.5599,\n",
      "        1.0618, 1.3931, 1.2730, 1.1547,    nan,    nan, 1.6643, 1.9867, 1.4462,\n",
      "        1.8932,    nan, 1.4665, 1.2237, 2.5575,    nan, 1.1563,    nan, 1.3270,\n",
      "        1.3712,    nan, 1.4491,    nan,    nan, 1.9319, 1.5875, 1.0236,    nan,\n",
      "           nan, 1.3915, 1.8363, 1.4153, 1.9721,    nan,    nan, 1.3550,    nan,\n",
      "           nan, 1.3640, 1.5784,    nan,    nan,    nan, 1.3774,    nan, 2.0117,\n",
      "           nan, 0.8758,    nan,    nan,    nan,    nan,    nan, 1.9269, 2.1865,\n",
      "        1.3490], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6122, 0.6235, 0.7466, 0.6983, 0.7570, 0.6294, 0.7629, 0.6685, 0.5734,\n",
      "        0.7136, 0.6858, 0.7656, 0.5978, 0.5686, 0.5475, 0.6875, 0.7704, 0.8817,\n",
      "        0.6831, 0.8423, 0.7004, 0.6906, 0.6815, 0.7446, 0.7233, 0.6744, 0.7580,\n",
      "        0.7478, 0.6023, 0.7036, 0.7010, 0.7604, 0.6436, 0.7088, 0.5924, 0.6948,\n",
      "        0.7500, 0.7811, 0.5225, 0.6434, 0.7202, 0.7245, 0.6666, 0.7890, 0.7026,\n",
      "        0.7690, 0.6751, 0.6147, 0.7438, 0.7127, 0.6549, 0.6773, 0.5519, 0.6760,\n",
      "        0.6564, 0.7402, 0.6011, 0.7748, 0.6062, 0.5567, 0.8881, 0.5220, 0.6686,\n",
      "        0.5821], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 0.9618,    nan, 1.3623,    nan, 1.1458,    nan,    nan,\n",
      "        1.0747, 1.2342, 2.0679,    nan,    nan,    nan, 1.2682, 1.5114, 2.8659,\n",
      "           nan, 1.4697,    nan, 2.0288, 0.9931, 2.4471, 1.0496,    nan, 1.5357,\n",
      "        0.9107,    nan, 0.9308, 1.1529, 1.0544,    nan, 0.9025,    nan, 1.5127,\n",
      "        1.3156, 1.1328,    nan,    nan, 0.9283, 1.4213,    nan, 1.3609, 2.5751,\n",
      "           nan, 1.0786,    nan, 2.2212, 1.7210,    nan,    nan,    nan, 1.4038,\n",
      "           nan, 1.3793,    nan, 1.5014,    nan,    nan, 2.8702,    nan, 1.2748,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7071, 0.5242, 0.5726, 0.6877, 0.5161, 0.7546, 0.7542, 0.6951, 0.7156,\n",
      "        0.7102, 0.6851, 0.6286, 0.7593, 0.6672, 0.8273, 0.5524, 0.6901, 0.5737,\n",
      "        0.7750, 0.6983, 0.7400, 0.6600, 0.6865, 0.7418, 0.8873, 0.6959, 0.7937,\n",
      "        0.8289, 0.7171, 0.7774, 0.7079, 0.7521, 0.6169, 0.7042, 0.6765, 0.6755,\n",
      "        0.7230, 0.6720, 0.8627, 0.6997, 0.7710, 0.6749, 0.7015, 0.7124, 0.5871,\n",
      "        0.5951, 0.7488, 0.6403, 0.6267, 0.7956, 0.6094, 0.7811, 0.6714, 0.7068,\n",
      "        0.7369, 0.7162, 0.7079, 0.7295, 0.6217, 0.5714, 0.6743, 0.6745, 0.6471,\n",
      "        0.9220], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.4691,    nan, 0.9216, 2.1732, 2.0632, 1.0836,\n",
      "        1.3111,    nan,    nan, 1.1142, 1.4358, 2.2890,    nan,    nan,    nan,\n",
      "        1.2100,    nan, 2.5337,    nan,    nan, 0.8658, 3.4100,    nan, 1.7103,\n",
      "        1.2635, 1.2721, 2.0235, 1.9665, 1.5356,    nan, 1.2195, 1.6612,    nan,\n",
      "        1.2800, 1.8234, 1.9982,    nan, 1.8217, 2.3210, 0.8360, 1.2120,    nan,\n",
      "           nan, 0.9683,    nan,    nan, 1.3754,    nan, 1.0341, 2.3489, 1.2343,\n",
      "        0.9291, 1.1489, 0.9277, 1.1436,    nan,    nan, 0.9771,    nan,    nan,\n",
      "        2.7887], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7240, 0.7115, 0.5243, 0.6873, 0.6555, 0.5974, 0.7727, 0.7707, 0.5108,\n",
      "        0.7134, 0.5425, 0.4908, 0.5942, 0.7196, 0.7379, 0.7367, 0.6640, 0.6140,\n",
      "        0.6814, 0.8362, 0.5846, 0.7958, 0.7313, 0.7703, 0.6834, 0.7733, 0.7042,\n",
      "        0.6327, 0.7403, 0.6490, 0.6563, 0.7932, 0.6931, 0.6267, 0.6792, 0.7445,\n",
      "        0.6718, 0.7445, 0.6310, 0.6413, 0.7195, 0.8783, 0.6282, 0.8038, 0.5508,\n",
      "        0.6266, 0.7716, 0.5647, 0.7023, 0.7336, 0.5810, 0.7171, 0.5917, 0.7332,\n",
      "        0.6546, 0.6018, 0.7009, 0.6234, 0.7420, 0.7687, 0.5505, 0.6197, 0.7164,\n",
      "        0.6121], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 2.0619,    nan,    nan,    nan,    nan, 1.8417, 1.8687,    nan,\n",
      "        1.0346,    nan,    nan,    nan, 0.9047, 2.4032, 1.1272,    nan,    nan,\n",
      "        1.3013, 1.7929,    nan, 1.2343, 1.3966, 0.9944, 1.9008, 1.7391, 0.9746,\n",
      "           nan, 1.6391,    nan,    nan, 1.1984,    nan,    nan, 1.3555, 1.6815,\n",
      "           nan, 1.4960,    nan,    nan, 0.8898, 1.7917,    nan, 2.1663,    nan,\n",
      "           nan, 1.7811,    nan, 1.0751, 1.0968,    nan, 1.0657,    nan, 1.4341,\n",
      "           nan,    nan,    nan,    nan, 1.1669, 0.9587,    nan,    nan, 0.9896,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6122, 0.7273, 0.7419, 0.8095, 0.6779, 0.7176, 0.6601, 0.6225, 0.7000,\n",
      "        0.5694, 0.7181, 0.6200, 0.7041, 0.6724, 0.5374, 0.7729, 0.6628, 0.6002,\n",
      "        0.7219, 0.7700, 0.6954, 0.6994, 0.6414, 0.6847, 0.7364, 0.7434, 0.7095,\n",
      "        0.7990, 0.6592, 0.6581, 0.7179, 0.6851, 0.7345, 0.7187, 0.7428, 0.7124,\n",
      "        0.7350, 0.7104, 0.7973, 0.6041, 0.6293, 0.6412, 0.6902, 0.6896, 0.6519,\n",
      "        0.7772, 0.6585, 0.6873, 0.7161, 0.7253, 0.7137, 0.6523, 0.7301, 0.6750,\n",
      "        0.7009, 0.8191, 0.6500, 0.6262, 0.5743, 0.6917, 0.5888, 0.7757, 0.6962,\n",
      "        0.7337], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.0288, 0.9672, 1.3597, 0.9224, 2.5154,    nan,    nan, 1.3975,\n",
      "           nan, 2.0612,    nan, 1.1720,    nan,    nan, 1.8506,    nan,    nan,\n",
      "        2.5304, 1.4431, 1.0492,    nan, 1.4896, 1.4708, 0.9667, 1.2725, 1.7693,\n",
      "        2.3431, 1.2254,    nan, 1.1227,    nan, 0.9921,    nan, 1.8773, 1.2504,\n",
      "        1.1155, 1.0033, 1.9614,    nan,    nan,    nan,    nan, 1.7306,    nan,\n",
      "        2.1914,    nan,    nan, 2.1430,    nan, 1.3242,    nan, 1.1747,    nan,\n",
      "        0.9602, 3.2697,    nan,    nan,    nan, 1.5807,    nan, 1.7277, 1.0425,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7146, 0.7114, 0.6230, 0.5741, 0.6097, 0.5405, 0.7828, 0.6514, 0.6853,\n",
      "        0.7861, 0.6464, 0.7200, 0.6985, 0.7132, 0.8263, 0.6586, 0.6970, 0.5969,\n",
      "        0.7599, 0.6544, 0.6000, 0.7459, 0.7218, 0.7820, 0.7155, 0.7808, 0.6241,\n",
      "        0.7268, 0.7237, 0.8019, 0.6590, 0.6332, 0.7884, 0.6841, 0.5951, 0.8137,\n",
      "        0.4837, 0.7441, 0.7035, 0.6383, 0.7358, 0.7199, 0.5563, 0.7011, 0.5514,\n",
      "        0.7113, 0.5737, 0.7176, 0.7100, 0.7338, 0.7594, 0.7098, 0.6533, 0.4874,\n",
      "        0.7277, 0.6377, 0.6404, 0.7646, 0.5847, 0.7080, 0.6122, 0.5988, 0.7097,\n",
      "        0.6755], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.3813, 1.5248,    nan,    nan,    nan,    nan, 1.4622,    nan,    nan,\n",
      "        1.4273,    nan, 1.4140,    nan, 1.2020, 1.6309,    nan, 0.7950,    nan,\n",
      "        1.7802,    nan,    nan, 1.7610, 0.9478, 1.5410, 0.8580, 1.0397,    nan,\n",
      "           nan, 1.0695, 1.7579,    nan,    nan, 1.6013,    nan,    nan, 1.5268,\n",
      "           nan, 1.3967, 1.2160,    nan, 1.3345, 0.9571,    nan, 2.2616,    nan,\n",
      "        1.1298,    nan, 1.5283, 0.9241, 1.1320, 1.5102, 1.2604,    nan,    nan,\n",
      "        0.9979,    nan,    nan, 1.1690,    nan,    nan,    nan,    nan, 1.0332,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7208, 0.5915, 0.7007, 0.6617, 0.7669, 0.7237, 0.7493, 0.7197, 0.6671,\n",
      "        0.7572, 0.6436, 0.7621, 0.6614, 0.7475, 0.6967, 0.7265, 0.7225, 0.7107,\n",
      "        0.6082, 0.6050, 0.5142, 0.7506, 0.6301, 0.6120, 0.7571, 0.6643, 0.5501,\n",
      "        0.6494, 0.7552, 0.6953, 0.6266, 0.6980, 0.7465, 0.6089, 0.7920, 0.6076,\n",
      "        0.7198, 0.7118, 0.7209, 0.7071, 0.6601, 0.8837, 0.8052, 0.6898, 0.6162,\n",
      "        0.7372, 0.7583, 0.6429, 0.7141, 0.7503, 0.6988, 0.6041, 0.6498, 0.7251,\n",
      "        0.6217, 0.5379, 0.6418, 0.6017, 0.6631, 0.5743, 0.7801, 0.6728, 0.6175,\n",
      "        0.7561], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.7604,    nan, 1.6603,    nan, 1.6970, 1.3641, 1.1259, 1.6877,    nan,\n",
      "        1.2766,    nan, 1.1768,    nan, 1.4063, 1.2654, 1.2145, 2.0811, 1.7917,\n",
      "           nan,    nan,    nan, 2.3849,    nan,    nan, 1.3441,    nan,    nan,\n",
      "           nan, 1.4739, 1.1836, 2.4103, 1.2477, 1.6242,    nan, 1.7148,    nan,\n",
      "        1.4311, 1.2586, 1.8826, 1.2217,    nan, 1.7902, 1.7354, 1.7264,    nan,\n",
      "        1.2193, 1.4169,    nan, 1.1838, 1.8481, 1.0346,    nan,    nan, 1.2794,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 2.1628, 1.3037,    nan,\n",
      "        1.1669], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6341, 0.7695, 0.6860, 0.7171, 0.7729, 0.6821, 0.6076, 0.5814, 0.7353,\n",
      "        0.7780, 0.6954, 0.7411, 0.6274, 0.6013, 0.5890, 0.6669, 0.6133, 0.5906,\n",
      "        0.7205, 0.7235, 0.6679, 0.7064, 0.6444, 0.7351, 0.8460, 0.8476, 0.6670,\n",
      "        0.7445, 0.7715, 0.5591, 0.5263, 0.7133, 0.7243, 0.6579, 0.7660, 0.6635,\n",
      "        0.7083, 0.6453, 0.6734, 0.7946, 0.8021, 0.6858, 0.7550, 0.7823, 0.5791,\n",
      "        0.6879, 0.7360, 0.6808, 0.5989, 0.6165, 0.6479, 0.7460, 0.7481, 0.7255,\n",
      "        0.7403, 0.7290, 0.5526, 0.6950, 0.7505, 0.7304, 0.7216, 0.6266, 0.7830,\n",
      "        0.7389], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 0.9571,    nan, 1.2482, 1.0913,    nan,    nan,    nan, 1.2846,\n",
      "        1.9629,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.4737, 1.2938,    nan, 1.4359,    nan, 1.8470, 2.0634, 2.2252,    nan,\n",
      "        2.5480, 1.4407,    nan,    nan, 0.8797, 0.9461, 1.3342, 1.7088,    nan,\n",
      "        1.3511,    nan,    nan, 1.8670, 1.0793,    nan, 1.4524, 1.9552,    nan,\n",
      "           nan, 1.0634,    nan, 2.0688,    nan,    nan, 1.3680, 0.9108, 1.3920,\n",
      "        1.9174, 1.1510,    nan, 1.1301, 1.6291, 0.8351, 1.0340,    nan, 1.4622,\n",
      "        1.1740], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7620, 0.6389, 0.6009, 0.7639, 0.6614, 0.6183, 0.7422, 0.6812, 0.7405,\n",
      "        0.6505, 0.6579, 0.6955, 0.7994, 0.6007, 0.6155, 0.6980, 0.6876, 0.5523,\n",
      "        0.7153, 0.6079, 0.7066, 0.6658, 0.8173, 0.7368, 0.6960, 0.7042, 0.6939,\n",
      "        0.7118, 0.5210, 0.7449, 0.4961, 0.7954, 0.6675, 0.6509, 0.6161, 0.6506,\n",
      "        0.5845, 0.6181, 0.5713, 0.6824, 0.7211, 0.7354, 0.6056, 0.5499, 0.6995,\n",
      "        0.5038, 0.6220, 0.7643, 0.5703, 0.5608, 0.6984, 0.6481, 0.6925, 0.7488,\n",
      "        0.6569, 0.7728, 0.7378, 0.7609, 0.6266, 0.7383, 0.7092, 0.5787, 0.6339,\n",
      "        0.8587], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.4805,    nan,    nan, 1.1481,    nan,    nan, 1.9093, 1.1481, 1.3642,\n",
      "           nan,    nan, 1.0789, 1.5337,    nan,    nan, 1.2741, 1.3804,    nan,\n",
      "        1.9720,    nan, 0.9626, 2.0613, 1.5325, 0.8681, 0.8264, 1.1358, 1.6226,\n",
      "        1.2091,    nan,    nan,    nan, 1.4587,    nan,    nan,    nan, 1.8200,\n",
      "           nan,    nan,    nan, 2.0880, 1.3149, 1.0852,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.6420,    nan,    nan, 1.1584,    nan,    nan, 1.2879,\n",
      "           nan, 1.6537, 1.9416, 0.9316,    nan, 1.0160, 1.2453,    nan,    nan,\n",
      "        2.5355], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.5393, 0.6625, 0.5712, 0.7425, 0.7489, 0.6582, 0.6366, 0.6380, 0.7130,\n",
      "        0.7153, 0.7167, 0.7650, 0.6490, 0.7441, 0.7161, 0.6211, 0.7286, 0.6949,\n",
      "        0.6266, 0.7803, 0.7070, 0.5745, 0.6288, 0.6644, 0.6962, 0.6907, 0.7824,\n",
      "        0.7253, 0.7836, 0.5840, 0.7611, 0.7428, 0.6641, 0.6894, 0.6082, 0.5422,\n",
      "        0.7587, 0.6427, 0.7331, 0.7446, 0.7152, 0.7602, 0.6442, 0.7032, 0.6732,\n",
      "        0.6007, 0.6835, 0.7616, 0.6647, 0.7370, 0.5691, 0.6424, 0.5855, 0.6404,\n",
      "        0.5757, 0.6712, 0.7047, 0.7180, 0.7510, 0.7026, 0.5259, 0.6409, 0.6210,\n",
      "        0.7832], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.6249, 2.7516, 2.1933, 1.2047, 1.8954, 1.9942, 2.3097, 2.1836, 1.0669,\n",
      "        1.3966, 0.9793, 0.9195, 3.6909, 1.5019, 0.9803, 2.3680, 1.2655, 1.0388,\n",
      "        2.0074, 1.6510, 2.1096, 2.8602, 2.5718, 1.7828, 2.8138, 1.6575, 2.5598,\n",
      "        0.9908, 1.9662, 2.5387, 1.2217, 1.3216, 2.2033, 1.8209, 2.8258, 3.2755,\n",
      "        0.8392, 3.4491, 1.3516, 1.7048, 1.1767, 1.2487, 3.0657, 1.6265, 1.6213,\n",
      "        2.2681, 1.7440, 1.1409, 1.4552, 1.3106, 1.7182, 2.6247, 3.3696, 0.0000,\n",
      "        2.9040, 1.2612, 0.8596, 2.2737, 1.1159, 0.9952, 2.6759, 3.0828, 1.9084,\n",
      "        1.5249], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6574, 0.5947, 0.5629, 0.7587, 0.6259, 0.5372, 0.6962, 0.6169, 0.5973,\n",
      "        0.6034, 0.6756, 0.6638, 0.6922, 0.7598, 0.5663, 0.7275, 0.7039, 0.7807,\n",
      "        0.7032, 0.8172, 0.7372, 0.6280, 0.5773, 0.8408, 0.7308, 0.6278, 0.7665,\n",
      "        0.6878, 0.6583, 0.6419, 0.5243, 0.7073, 0.6443, 0.6282, 0.8612, 0.6224,\n",
      "        0.6618, 0.7661, 0.6658, 0.7497, 0.5859, 0.6119, 0.7078, 0.6446, 0.6471,\n",
      "        0.5821, 0.7724, 0.7517, 0.7000, 0.7865, 0.6974, 0.6860, 0.7627, 0.6356,\n",
      "        0.8304, 0.7612, 0.7142, 0.7602, 0.7206, 0.6729, 0.7788, 0.6686, 0.6830,\n",
      "        0.7155], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.4994, 2.7485, 2.7732, 1.5087, 2.0959, 3.3043, 1.8925, 4.1329, 2.7158,\n",
      "        2.3909, 2.3452, 2.7973, 1.3551, 1.4762, 3.2796, 1.6686, 1.3387, 2.5032,\n",
      "        0.8791, 1.6229, 0.9775, 2.6478, 3.2483, 1.5073, 0.9535, 3.3588, 1.4060,\n",
      "        2.0042, 1.6961, 2.5527, 2.0307, 1.3755, 0.0000, 3.1581, 3.2087, 2.9923,\n",
      "        2.3407, 1.6437, 4.3951, 1.9263, 2.9776, 1.8488, 1.2957, 2.7147, 2.3747,\n",
      "        3.0575, 2.8595, 1.0428, 1.9940, 1.7613, 2.5487, 1.4928, 3.4549, 2.2611,\n",
      "        1.6557, 2.2792, 1.2296, 1.5907, 1.0621, 1.1233, 1.4515, 2.4397, 1.6709,\n",
      "        0.9003], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6424, 0.5851, 0.6687, 0.6582, 0.7623, 0.7194, 0.6911, 0.7827, 0.6281,\n",
      "        0.7798, 0.6275, 0.5907, 0.7121, 0.6130, 0.8029, 0.6233, 0.7752, 0.5655,\n",
      "        0.7404, 0.7186, 0.5601, 0.5542, 0.5531, 0.6726, 0.5949, 0.7239, 0.7645,\n",
      "        0.7119, 0.5956, 0.7307, 0.6246, 0.7069, 0.6724, 0.7425, 0.7629, 0.5049,\n",
      "        0.6981, 0.6716, 0.6068, 0.6937, 0.6872, 0.6090, 0.7795, 0.7957, 0.6244,\n",
      "        0.5800, 0.5556, 0.6778, 0.6105, 0.6909, 0.7755, 0.7330, 0.6283, 0.5617,\n",
      "        0.6090, 0.7820, 0.5834, 0.7004, 0.7633, 0.6985, 0.7448, 0.6620, 0.7196,\n",
      "        0.7493], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5231,    nan,    nan,    nan, 1.0335, 1.2102,    nan, 1.1632,    nan,\n",
      "        0.9978,    nan,    nan,    nan,    nan, 1.9779,    nan, 1.3749,    nan,\n",
      "        1.1403, 1.1908,    nan,    nan,    nan,    nan,    nan, 1.4290, 1.2188,\n",
      "           nan,    nan, 0.9864,    nan, 0.9327,    nan, 1.2316, 1.2838,    nan,\n",
      "        2.2266,    nan,    nan, 1.1592,    nan,    nan, 1.1735, 1.8915,    nan,\n",
      "           nan,    nan,    nan,    nan, 1.7698, 1.3707, 1.1417,    nan,    nan,\n",
      "           nan, 1.7098, 2.5179, 0.9575, 1.3148, 1.2965, 3.1606,    nan, 1.0686,\n",
      "        1.1673], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7962, 0.8115, 0.5538, 0.6240, 0.6060, 0.7515, 0.6565, 0.6360, 0.6438,\n",
      "        0.7324, 0.7742, 0.6699, 0.6629, 0.7168, 0.7213, 0.5776, 0.6791, 0.6410,\n",
      "        0.5872, 0.6759, 0.7600, 0.6920, 0.7442, 0.8198, 0.5786, 0.5987, 0.7147,\n",
      "        0.8533, 0.5847, 0.6395, 0.5883, 0.7397, 0.6817, 0.7147, 0.6981, 0.7762,\n",
      "        0.7093, 0.5830, 0.6807, 0.5662, 0.7280, 0.7189, 0.7579, 0.7126, 0.7209,\n",
      "        0.6901, 0.6255, 0.7486, 0.7010, 0.7585, 0.6148, 0.7873, 0.7274, 0.6502,\n",
      "        0.7542, 0.7290, 0.7055, 0.7322, 0.6863, 0.7879, 0.6179, 0.6842, 0.5342,\n",
      "        0.7001], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.4637, 1.8293,    nan,    nan,    nan, 1.7200, 1.2540,    nan,    nan,\n",
      "        0.9615, 1.4291,    nan,    nan, 1.0024, 1.2265,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.3983, 1.5291, 1.4724, 1.7707,    nan,    nan, 1.8523,\n",
      "        1.4831,    nan,    nan,    nan,    nan,    nan,    nan, 1.9748, 1.3279,\n",
      "        1.4607,    nan,    nan,    nan, 1.0031, 1.0700, 1.4610, 0.8174, 1.1492,\n",
      "        1.2896,    nan, 1.4023, 1.8693, 2.1848,    nan, 1.2487, 1.9078,    nan,\n",
      "        1.4483, 1.4290, 0.9524, 2.1696, 1.5756, 1.5144,    nan, 1.4552,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.5659, 0.5724, 0.6767, 0.5512, 0.6199, 0.7019, 0.7156, 0.7840, 0.6942,\n",
      "        0.8380, 0.7226, 0.7192, 0.6931, 0.6380, 0.7277, 0.7964, 0.8215, 0.5972,\n",
      "        0.7833, 0.7780, 0.7736, 0.6453, 0.6204, 0.7259, 0.6531, 0.6639, 0.7482,\n",
      "        0.5619, 0.7756, 0.5837, 0.6447, 0.6518, 0.6956, 0.6853, 0.7371, 0.7352,\n",
      "        0.6111, 0.6609, 0.7004, 0.5879, 0.7506, 0.7766, 0.6440, 0.6488, 0.5788,\n",
      "        0.6896, 0.6438, 0.7298, 0.5767, 0.7229, 0.7801, 0.7313, 0.7137, 0.6314,\n",
      "        0.6354, 0.7022, 0.5712, 0.6893, 0.7100, 0.7730, 0.6747, 0.7241, 0.7374,\n",
      "        0.6104], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.5946, 2.0397, 3.4240, 3.9739, 1.9266, 1.6008, 0.7535, 1.1849, 1.0571,\n",
      "        2.2254, 1.9175, 1.9185, 0.9670, 1.9572, 0.8528, 1.7459, 1.5568, 3.1081,\n",
      "        1.1601, 2.7235, 1.9087, 3.1906, 2.0484, 1.1151, 1.4196, 0.0000, 1.1932,\n",
      "        1.6689, 1.6099, 1.7187, 3.1190, 2.0363, 2.8226, 2.3669, 0.9250, 1.2309,\n",
      "        2.7250, 1.9940, 1.0300, 3.2089, 1.2724, 2.8442, 1.8880, 1.6725, 2.9621,\n",
      "        3.7794, 1.6903, 1.2636, 2.3796, 1.1729, 1.6613, 1.0178, 1.8174, 2.7623,\n",
      "        2.5112, 0.8393, 3.6435, 1.7300, 1.1798, 1.1028, 2.7835, 1.0508, 0.9937,\n",
      "        4.3020], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7439, 0.6869, 0.5744, 0.7043, 0.7506, 0.6430, 0.6979, 0.5527, 0.6738,\n",
      "        0.7507, 0.6887, 0.7028, 0.6336, 0.7009, 0.8518, 0.6876, 0.6750, 0.6705,\n",
      "        0.6060, 0.8141, 0.6660, 0.5780, 0.7504, 0.8222, 0.7270, 0.6430, 0.7473,\n",
      "        0.7427, 0.7929, 0.6112, 0.7432, 0.6429, 0.7371, 0.6246, 0.6981, 0.7073,\n",
      "        0.6959, 0.7565, 0.6522, 0.7681, 0.6882, 0.6491, 0.7729, 0.6971, 0.5798,\n",
      "        0.5509, 0.7144, 0.9031, 0.7129, 0.7039, 0.6811, 0.7262, 0.6326, 0.7917,\n",
      "        0.6459, 0.6611, 0.6649, 0.6714, 0.6799, 0.7419, 0.7461, 0.7188, 0.6571,\n",
      "        0.6774], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.8634,    nan,    nan, 0.8300, 1.3915,    nan, 1.0435,    nan, 1.7158,\n",
      "        1.9159, 1.3367, 1.0778,    nan,    nan, 2.3967, 1.0896,    nan, 1.3298,\n",
      "           nan, 1.5999,    nan,    nan, 0.7456, 2.2434, 1.5643,    nan, 1.1455,\n",
      "        1.4059, 1.1700,    nan, 1.1555,    nan, 1.4743,    nan,    nan, 0.9107,\n",
      "        0.8287, 0.9043,    nan, 1.7896, 1.6220,    nan, 1.7393, 1.0239,    nan,\n",
      "           nan, 0.9009, 2.5910, 1.2793, 0.8881, 1.4277, 0.7729,    nan, 1.9759,\n",
      "        1.4768, 0.8920,    nan, 1.1171, 0.7760, 1.7567, 1.5378, 1.0761, 1.2811,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6206, 0.6336, 0.7208, 0.7245, 0.7468, 0.6051, 0.7092, 0.6683, 0.7101,\n",
      "        0.7005, 0.6698, 0.7234, 0.7057, 0.7538, 0.7331, 0.6674, 0.7126, 0.7642,\n",
      "        0.7877, 0.6217, 0.7171, 0.6664, 0.7238, 0.6816, 0.6304, 0.7677, 0.8306,\n",
      "        0.7065, 0.6714, 0.6893, 0.7022, 0.7498, 0.6315, 0.6597, 0.7120, 0.6193,\n",
      "        0.6996, 0.7080, 0.6884, 0.6148, 0.6181, 0.6047, 0.7378, 0.6536, 0.7217,\n",
      "        0.7185, 0.7717, 0.6619, 0.7353, 0.7955, 0.6196, 0.6998, 0.7899, 0.7154,\n",
      "        0.8138, 0.7311, 0.7376, 0.5844, 0.7077, 0.7086, 0.7377, 0.6612, 0.6415,\n",
      "        0.8253], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 0.9652, 1.1038, 1.2178,    nan, 0.9541, 3.0563, 1.3405,\n",
      "        0.7988,    nan, 1.3259,    nan, 1.4373, 0.9004,    nan, 0.9812, 1.2453,\n",
      "        1.1822,    nan, 1.1933,    nan, 1.6504, 2.0468,    nan, 1.0165, 2.2157,\n",
      "        0.8099,    nan, 1.2840, 1.2377, 1.0022,    nan,    nan,    nan,    nan,\n",
      "        0.8541, 0.9087, 0.8976,    nan,    nan,    nan, 1.2804,    nan, 1.5351,\n",
      "        2.6103, 2.0246,    nan, 1.2909, 1.2674,    nan, 1.0195, 1.1358, 1.2796,\n",
      "        2.1078, 0.8600, 1.5222,    nan,    nan,    nan, 1.1518,    nan,    nan,\n",
      "        1.1554], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7529, 0.6197, 0.7758, 0.4761, 0.6372, 0.7491, 0.8120, 0.5989, 0.5988,\n",
      "        0.5501, 0.7283, 0.6237, 0.6404, 0.7197, 0.6995, 0.6587, 0.7384, 0.7295,\n",
      "        0.5297, 0.6719, 0.6330, 0.6044, 0.7690, 0.6386, 0.7125, 0.7800, 0.6461,\n",
      "        0.6389, 0.6358, 0.7477, 0.7774, 0.6202, 0.7156, 0.5486, 0.6285, 0.6406,\n",
      "        0.5981, 0.5959, 0.6834, 0.6701, 0.6689, 0.7143, 0.6135, 0.6874, 0.7599,\n",
      "        0.7503, 0.6613, 0.7028, 0.7746, 0.5514, 0.6571, 0.8087, 0.6146, 0.7439,\n",
      "        0.6946, 0.6451, 0.6943, 0.5368, 0.6554, 0.7452, 0.7517, 0.7630, 0.5960,\n",
      "        0.6501], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2896,    nan, 1.8460,    nan,    nan, 1.2591, 1.4472,    nan,    nan,\n",
      "           nan, 1.5532,    nan,    nan, 0.9770, 1.4516,    nan, 0.9737, 0.9984,\n",
      "           nan, 0.8210,    nan,    nan, 0.9537,    nan, 1.0398, 1.9725,    nan,\n",
      "           nan,    nan, 1.5461, 1.1435,    nan, 0.8729,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.8082,    nan, 1.2150, 1.5639,\n",
      "        1.5643,    nan, 0.7713, 1.2612,    nan,    nan,    nan,    nan, 2.6657,\n",
      "           nan,    nan, 1.2562,    nan,    nan, 1.3090, 1.6784, 2.2526,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.5496, 0.8404, 0.6482, 0.5075, 0.6470, 0.6517, 0.6658, 0.6568, 0.7126,\n",
      "        0.6977, 0.6047, 0.6445, 0.6566, 0.7388, 0.7240, 0.6735, 0.7289, 0.7321,\n",
      "        0.7676, 0.6647, 0.6734, 0.7128, 0.5955, 0.7061, 0.8257, 0.7201, 0.6999,\n",
      "        0.7334, 0.5461, 0.7150, 0.6390, 0.5937, 0.7397, 0.7409, 0.6436, 0.6589,\n",
      "        0.6475, 0.7402, 0.7062, 0.8099, 0.6814, 0.6185, 0.6668, 0.6699, 0.7762,\n",
      "        0.7748, 0.7614, 0.8132, 0.6511, 0.6918, 0.7519, 0.5780, 0.7635, 0.7503,\n",
      "        0.5646, 0.7658, 0.7746, 0.6953, 0.7068, 0.7540, 0.6527, 0.5045, 0.5388,\n",
      "        0.7333], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.5862,    nan,    nan,    nan,    nan,    nan,    nan, 1.1970,\n",
      "        1.7024,    nan,    nan,    nan, 0.8568, 1.1786,    nan, 1.5570, 1.0935,\n",
      "        1.4711,    nan, 1.3469, 1.0291,    nan, 1.1070, 2.0207, 1.4739, 1.2847,\n",
      "        1.7615,    nan, 1.1152,    nan,    nan, 1.9510, 1.1473,    nan,    nan,\n",
      "           nan, 1.6550, 0.9636, 1.4926, 1.2164,    nan,    nan,    nan, 1.9042,\n",
      "        2.1181, 2.0581, 2.7898,    nan,    nan, 1.6652,    nan, 2.3733, 1.1698,\n",
      "           nan, 1.0831, 1.5486, 1.6475,    nan, 1.0133,    nan,    nan,    nan,\n",
      "        0.8675], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6527, 0.7427, 0.7650, 0.6903, 0.5858, 0.6538, 0.5933, 0.6566, 0.7151,\n",
      "        0.7498, 0.6171, 0.5567, 0.8897, 0.7323, 0.6316, 0.5913, 0.7734, 0.6659,\n",
      "        0.7651, 0.5198, 0.4700, 0.6730, 0.6842, 0.7868, 0.7890, 0.6791, 0.7340,\n",
      "        0.7307, 0.6311, 0.6669, 0.5724, 0.6633, 0.5999, 0.5880, 0.7874, 0.7046,\n",
      "        0.5867, 0.5975, 0.5927, 0.6412, 0.6027, 0.7099, 0.9370, 0.5725, 0.7426,\n",
      "        0.6227, 0.6871, 0.6884, 0.7278, 0.7067, 0.7300, 0.7554, 0.7298, 0.6188,\n",
      "        0.4809, 0.8049, 0.7429, 0.6544, 0.6180, 0.7714, 0.6254, 0.7227, 0.7129,\n",
      "        0.6437], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.9790, 1.3277, 1.5190, 1.8514, 1.0832, 1.8503, 2.1738, 2.1792, 1.8050,\n",
      "        0.9636, 3.0790, 3.1083, 1.6390, 2.2915, 1.2020, 1.5083, 1.2610, 1.9560,\n",
      "        1.9421, 3.0509, 4.4422, 1.0117, 2.4245, 1.7546, 1.7283, 1.8687, 1.4773,\n",
      "        1.4566, 2.7966, 1.3844, 2.2190, 1.5267, 2.3395, 2.1785, 0.9737, 1.4362,\n",
      "        3.3021, 2.6295, 1.5241, 1.0832, 2.4097, 1.0196, 2.3866, 1.9497, 1.9141,\n",
      "        3.5951, 1.6286, 3.0365, 1.0610, 1.3021, 1.1714, 1.7963, 1.8838, 2.7974,\n",
      "        2.7115, 1.3750, 2.3251, 3.3966, 2.4289, 1.2847, 1.2684, 1.2583, 1.2558,\n",
      "        2.0712], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6686, 0.5732, 0.6652, 0.7580, 0.6331, 0.7687, 0.6094, 0.6423, 0.6603,\n",
      "        0.6665, 0.5860, 0.7318, 0.7505, 0.7224, 0.5693, 0.6947, 0.7579, 0.7265,\n",
      "        0.7334, 0.7525, 0.5986, 0.6270, 0.7223, 0.7013, 0.7623, 0.6325, 0.6260,\n",
      "        0.7505, 0.7301, 0.5564, 0.6485, 0.5288, 0.6495, 0.7220, 0.5816, 0.9272,\n",
      "        0.7259, 0.7278, 0.8265, 0.7695, 0.7545, 0.6384, 0.5738, 0.7236, 0.6862,\n",
      "        0.6878, 0.6588, 0.7138, 0.6263, 0.7341, 0.5973, 0.7921, 0.6165, 0.8088,\n",
      "        0.5342, 0.4985, 0.7095, 0.7009, 0.7876, 0.7236, 0.5819, 0.7218, 0.7152,\n",
      "        0.7275], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.7557, 1.5385,    nan, 1.0794,    nan,    nan,    nan,\n",
      "        1.5568,    nan, 1.6621, 1.1532, 1.7182,    nan,    nan, 0.8025, 1.0034,\n",
      "        1.1259, 1.8562,    nan,    nan,    nan, 1.0504, 1.8975,    nan,    nan,\n",
      "        1.2441, 1.1153,    nan,    nan,    nan,    nan, 0.9026,    nan, 3.6338,\n",
      "        1.2953, 2.6931, 1.4019, 1.8140, 0.9741,    nan,    nan,    nan, 1.3973,\n",
      "           nan, 2.6098, 2.8652,    nan, 0.7343,    nan, 1.5603,    nan, 0.9035,\n",
      "           nan,    nan, 1.0653, 0.9441, 1.5806, 0.9917,    nan, 1.0668, 1.5019,\n",
      "        0.9147], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6153, 0.6566, 0.6399, 0.7552, 0.7476, 0.5364, 0.6904, 0.7229, 0.7754,\n",
      "        0.6457, 0.5731, 0.5341, 0.7741, 0.7078, 0.5765, 0.6822, 0.7323, 0.5852,\n",
      "        0.6065, 0.7392, 0.8589, 0.7676, 0.5759, 0.4909, 0.6875, 0.7538, 0.6592,\n",
      "        0.5754, 0.7345, 0.7388, 0.5978, 0.6029, 0.7023, 0.6330, 0.8063, 0.6224,\n",
      "        0.7405, 0.7507, 0.6894, 0.6416, 0.5499, 0.7072, 0.8001, 0.6404, 0.7035,\n",
      "        0.6639, 0.6915, 0.5664, 0.7623, 0.7096, 0.6536, 0.7454, 0.6617, 0.6101,\n",
      "        0.6042, 0.7282, 0.6955, 0.7346, 0.6349, 0.7425, 0.6083, 0.5401, 0.6547,\n",
      "        0.7169], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.2231, 1.1870,    nan, 1.3385, 0.9404, 1.1500,\n",
      "           nan,    nan,    nan, 1.2141,    nan,    nan,    nan, 1.0232,    nan,\n",
      "           nan, 2.3887, 2.8782, 1.1363,    nan,    nan, 1.2358, 1.1350,    nan,\n",
      "           nan, 1.7021,    nan,    nan,    nan, 1.0765,    nan, 2.2579,    nan,\n",
      "        1.2501, 1.2452, 1.4784,    nan,    nan, 1.7675, 1.6482,    nan, 2.2741,\n",
      "           nan,    nan,    nan, 1.5368,    nan,    nan, 1.0485,    nan,    nan,\n",
      "           nan, 0.8843, 1.0776, 1.0169,    nan, 0.7794,    nan,    nan,    nan,\n",
      "        1.0064], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6290, 0.6076, 0.5772, 0.7100, 0.6392, 0.7152, 0.8114, 0.6758, 0.7400,\n",
      "        0.6445, 0.6524, 0.6363, 0.7073, 0.6598, 0.6465, 0.5883, 0.5854, 0.8112,\n",
      "        0.7880, 0.6367, 0.6083, 0.7684, 0.7401, 0.6387, 0.7268, 0.6544, 0.7041,\n",
      "        0.6566, 0.6350, 0.5888, 0.5812, 0.8053, 0.6782, 0.8604, 0.6248, 0.6518,\n",
      "        0.6002, 0.6746, 0.7305, 0.6770, 0.7402, 0.7942, 0.6946, 0.7574, 0.6781,\n",
      "        0.7549, 0.6338, 0.6797, 0.7181, 0.7112, 0.5827, 0.7013, 0.5986, 0.7784,\n",
      "        0.8169, 0.7239, 0.7211, 0.7847, 0.6375, 0.7774, 0.5570, 0.7513, 0.7879,\n",
      "        0.6995], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.6618, 3.5244, 2.0911, 0.8302, 2.0441, 0.9809, 2.5365, 1.9485, 1.2802,\n",
      "        1.6828, 2.1515, 2.6708, 1.8734, 0.0000, 1.1531, 3.1854, 1.3142, 1.7007,\n",
      "        2.6053, 2.4444, 1.9484, 2.8893, 1.1225, 3.0991, 1.1807, 2.9482, 1.0651,\n",
      "        1.6146, 1.3152, 2.5895, 2.9785, 1.5526, 3.0201, 1.9303, 2.2937, 2.2868,\n",
      "        1.4950, 1.6226, 1.0576, 1.2301, 1.1240, 1.3311, 1.2080, 0.8553, 1.1237,\n",
      "        0.8626, 1.5818, 1.2211, 1.1997, 1.2270, 2.3706, 1.2077, 2.5322, 1.9594,\n",
      "        1.5709, 2.1838, 0.8882, 2.6999, 2.2581, 1.8050, 2.9401, 1.4446, 1.6779,\n",
      "        1.6875], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6939, 0.7209, 0.8030, 0.6531, 0.8011, 0.7803, 0.7458, 0.7678, 0.7751,\n",
      "        0.5664, 0.7719, 0.6316, 0.7824, 0.6382, 0.7047, 0.6084, 0.6909, 0.7075,\n",
      "        0.7492, 0.6089, 0.7331, 0.7217, 0.6419, 0.7089, 0.6923, 0.6286, 0.6107,\n",
      "        0.7064, 0.7649, 0.7149, 0.7083, 0.7474, 0.6765, 0.6004, 0.6036, 0.6201,\n",
      "        0.6591, 0.6838, 0.6816, 0.4345, 0.7668, 0.6382, 0.7745, 0.7887, 0.6885,\n",
      "        0.6069, 0.7979, 0.6152, 0.7395, 0.7143, 0.7133, 0.5996, 0.6826, 0.5637,\n",
      "        0.5499, 0.8279, 0.7121, 0.7381, 0.7061, 0.6378, 0.8090, 0.7136, 0.6968,\n",
      "        0.7420], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.2558, 1.2232, 1.6674, 1.4749,    nan, 1.0088, 2.0454, 2.1609,\n",
      "           nan, 1.3344,    nan, 1.5195,    nan, 0.9326,    nan, 1.3949, 0.9592,\n",
      "        0.8735,    nan, 1.3847, 0.7486,    nan, 1.9948, 1.3297,    nan,    nan,\n",
      "           nan, 1.0978, 0.9227, 1.0118, 1.2031, 1.0162,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.3391,    nan, 1.9882,    nan, 1.8509, 2.0427, 1.2827,\n",
      "           nan, 2.7108,    nan, 1.2088, 1.3955, 1.8585,    nan, 0.8940,    nan,\n",
      "           nan, 1.7399, 1.0451, 1.1190, 1.1098,    nan, 1.6905, 0.9720,    nan,\n",
      "        1.3411], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.5920, 0.5997, 0.5741, 0.6587, 0.6158, 0.7300, 0.6417, 0.8204, 0.7038,\n",
      "        0.5822, 0.7016, 0.5536, 0.7047, 0.4920, 0.5008, 0.7754, 0.5754, 0.7240,\n",
      "        0.5751, 0.6814, 0.7771, 0.7807, 0.6097, 0.6190, 0.6794, 0.6019, 0.6437,\n",
      "        0.7034, 0.7527, 0.6339, 0.5636, 0.5672, 0.6181, 0.5072, 0.6486, 0.7756,\n",
      "        0.5692, 0.7685, 0.8108, 0.7429, 0.7325, 0.6899, 0.6299, 0.7152, 0.5538,\n",
      "        0.7268, 0.5985, 0.7840, 0.7486, 0.6956, 0.7841, 0.6840, 0.4974, 0.7752,\n",
      "        0.7331, 0.7085, 0.6690, 0.7121, 0.6316, 0.5514, 0.6728, 0.5729, 0.8203,\n",
      "        0.8032], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan,    nan,    nan, 1.4291,    nan, 1.0879, 1.4019,\n",
      "           nan, 1.9825,    nan, 1.5605,    nan,    nan, 1.4806,    nan, 1.3436,\n",
      "           nan,    nan, 1.8174, 2.1887,    nan,    nan, 1.6109,    nan,    nan,\n",
      "           nan, 1.1418,    nan,    nan,    nan,    nan,    nan,    nan, 1.1019,\n",
      "           nan, 1.7001, 1.9025, 0.9489, 0.9621, 1.5219,    nan, 1.1675,    nan,\n",
      "        1.0248,    nan, 1.0570, 1.2386, 1.5300, 1.0721, 1.0111,    nan, 1.3446,\n",
      "        1.1960, 1.5307,    nan,    nan,    nan,    nan,    nan,    nan, 1.4146,\n",
      "        1.5552], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.6256, 0.6520, 0.6551, 0.6943, 0.8201, 0.5644, 0.8646, 0.6809, 0.7479,\n",
      "        0.6818, 0.6685, 0.6161, 0.6755, 0.6755, 0.7494, 0.6811, 0.6278, 0.7095,\n",
      "        0.7089, 0.7241, 0.6463, 0.8001, 0.5918, 0.8573, 0.7356, 0.6650, 0.9409,\n",
      "        0.6293, 0.7328, 0.7465, 0.7210, 0.5874, 0.6237, 0.6706, 0.6995, 0.8008,\n",
      "        0.8010, 0.7190, 0.6558, 0.6688, 0.6991, 0.7792, 0.7017, 0.5886, 0.6498,\n",
      "        0.6096, 0.6055, 0.6653, 0.6573, 0.7507, 0.7404, 0.6732, 0.7690, 0.7436,\n",
      "        0.6388, 0.6167, 0.5735, 0.8358, 0.5701, 0.7427, 0.7281, 0.7534, 0.6666,\n",
      "        0.7914], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.4193, 2.4552,    nan, 2.2887, 1.1243, 1.9213,\n",
      "        1.2196,    nan,    nan,    nan, 1.2962,    nan,    nan,    nan, 1.6022,\n",
      "        1.4006, 1.9411,    nan, 1.6438,    nan, 1.8478, 1.7003,    nan, 2.9885,\n",
      "           nan, 1.0583, 1.4242,    nan,    nan,    nan, 1.3167,    nan, 1.8053,\n",
      "        2.0775, 1.3524,    nan,    nan, 1.0537, 1.4067, 1.0393,    nan,    nan,\n",
      "           nan,    nan, 1.3302,    nan, 1.4790, 1.1573, 1.1953, 1.5261, 1.5045,\n",
      "           nan,    nan,    nan, 2.5776,    nan, 1.3399, 1.1499, 1.5728,    nan,\n",
      "        2.1007], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7430, 0.6195, 0.6529, 0.6264, 0.6087, 0.7270, 0.7240, 0.7374, 0.7181,\n",
      "        0.6368, 0.6778, 0.6863, 0.6578, 0.6455, 0.7276, 0.6310, 0.5710, 0.6133,\n",
      "        0.7273, 0.6137, 0.6175, 0.7307, 0.7795, 0.6295, 0.5546, 0.8690, 0.6756,\n",
      "        0.6020, 0.8772, 0.7088, 0.7645, 0.6788, 0.6946, 0.6964, 0.6369, 0.5993,\n",
      "        0.7304, 0.7351, 0.5660, 0.6041, 0.6130, 0.7153, 0.7545, 0.7019, 0.6853,\n",
      "        0.7432, 0.7609, 0.7672, 0.6983, 0.7199, 0.7287, 0.6848, 0.7146, 0.5910,\n",
      "        0.7305, 0.7711, 0.6004, 0.7395, 0.6563, 0.6631, 0.7215, 0.6798, 0.5735,\n",
      "        0.7282], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.9790,    nan, 1.2896,    nan,    nan, 1.0493, 0.9542, 1.7482, 1.2279,\n",
      "           nan,    nan, 1.3351,    nan,    nan, 0.7518,    nan,    nan,    nan,\n",
      "        1.2743,    nan,    nan, 1.2529, 1.9319,    nan,    nan, 1.8050,    nan,\n",
      "           nan, 1.6142, 1.2728, 1.2999, 2.3648, 2.2352, 1.0726,    nan,    nan,\n",
      "        1.5163, 1.1431,    nan,    nan,    nan, 1.2820, 0.9825, 0.8954,    nan,\n",
      "        2.0161, 1.6883, 1.2704, 0.9043, 0.9868, 1.6581,    nan, 1.0306,    nan,\n",
      "        1.0511, 1.1529,    nan, 1.5984,    nan,    nan, 1.0844,    nan,    nan,\n",
      "        1.0777], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6944, 0.7607, 0.7320, 0.7248, 0.7623, 0.7053, 0.6437, 0.7499, 0.6130,\n",
      "        0.6567, 0.7822, 0.6915, 0.7093, 0.5715, 0.7136, 0.5087, 0.7419, 0.7277,\n",
      "        0.5951, 0.7265, 0.6487, 0.6413, 0.7303, 0.7263, 0.6832, 0.6998, 0.5524,\n",
      "        0.6598, 0.8694, 0.6972, 0.6584, 0.6569, 0.7319, 0.6931, 0.5814, 0.6089,\n",
      "        0.6827, 0.5612, 0.5277, 0.6492, 0.6939, 0.5973, 0.7172, 0.7331, 0.6341,\n",
      "        0.6930, 0.6559, 0.6617, 0.6992, 0.6649, 0.7343, 0.7112, 0.6159, 0.6965,\n",
      "        0.7709, 0.5370, 0.6132, 0.6443, 0.7422, 0.7812, 0.5958, 0.8244, 0.7486,\n",
      "        0.6303], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.0144, 1.6233, 1.7711, 1.0022, 2.1515, 1.1273, 2.4729, 0.7415, 3.4584,\n",
      "        2.8334, 1.4686, 0.9771, 2.0787, 2.9767, 1.2088, 4.5325, 1.4541, 2.5576,\n",
      "        3.0878, 1.2067, 3.2834, 3.1585, 1.0333, 1.0037, 2.7868, 1.2899, 0.0000,\n",
      "        2.9374, 3.2315, 1.7205, 3.2513, 2.7030, 1.1716, 1.1833, 3.7968, 2.6927,\n",
      "        2.4445, 2.9589, 3.5615, 2.8991, 0.8397, 3.5851, 1.6271, 3.5917, 1.9869,\n",
      "        2.2044, 2.2199, 2.9837, 1.0204, 2.4749, 1.5561, 3.5329, 2.4532, 1.0210,\n",
      "        1.1350, 3.9811, 2.4473, 2.8780, 0.8496, 2.0142, 2.8925, 1.4412, 1.8672,\n",
      "        2.6729], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7347, 0.6504, 0.7890, 0.7462, 0.7240, 0.7438, 0.7415, 0.7923, 0.6618,\n",
      "        0.5982, 0.7296, 0.7179, 0.6643, 0.7336, 0.5646, 0.7206, 0.7354, 0.6973,\n",
      "        0.6539, 0.6128, 0.7613, 0.7416, 0.7030, 0.7518, 0.5219, 0.5441, 0.6815,\n",
      "        0.7359, 0.8012, 0.5744, 0.5800, 0.6756, 0.5546, 0.5427, 0.6525, 0.6759,\n",
      "        0.7284, 0.7194, 0.7368, 0.7877, 0.6323, 0.6406, 0.6675, 0.6655, 0.7507,\n",
      "        0.6950, 0.5969, 0.7069, 0.5896, 0.7147, 0.7040, 0.6081, 0.6129, 0.6895,\n",
      "        0.6348, 0.5504, 0.7029, 0.7550, 0.7186, 0.6902, 0.7520, 0.6940, 0.6675,\n",
      "        0.8216], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.7894,    nan, 1.0472, 1.0473, 0.8828, 1.8553, 1.4856, 1.6773,    nan,\n",
      "           nan, 1.0296, 0.9510, 1.1238, 1.3495,    nan,    nan, 1.2757, 0.9835,\n",
      "           nan,    nan, 1.1073, 0.8177, 1.2845, 1.3128,    nan,    nan,    nan,\n",
      "        1.0242, 1.4407,    nan,    nan, 1.0734,    nan,    nan,    nan, 1.3629,\n",
      "        1.2348, 0.8092,    nan, 2.3604,    nan,    nan, 1.4857, 2.4577, 0.8945,\n",
      "        1.5778,    nan, 0.9254,    nan, 1.3841, 0.8698,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.3244, 0.8553, 1.9019,    nan, 1.0181, 0.9777,    nan,\n",
      "        2.0387], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6086, 0.6142, 0.7835, 0.8800, 0.6497, 0.7156, 0.5544, 0.8018, 0.7959,\n",
      "        0.6785, 0.6337, 0.6302, 0.7356, 0.7294, 0.6914, 0.6204, 0.6909, 0.6553,\n",
      "        0.7644, 0.8204, 0.6522, 0.7536, 0.4873, 0.5770, 0.7291, 0.6307, 0.6472,\n",
      "        0.7327, 0.7692, 0.7671, 0.6040, 0.7333, 0.6427, 0.6167, 0.5390, 0.5600,\n",
      "        0.7137, 0.6731, 0.7209, 0.7637, 0.6616, 0.6847, 0.6921, 0.8362, 0.5858,\n",
      "        0.7652, 0.7673, 0.5990, 0.6237, 0.6810, 0.7283, 0.5910, 0.7346, 0.7565,\n",
      "        0.7277, 0.6787, 0.6940, 0.6600, 0.6061, 0.6402, 0.8059, 0.6993, 0.6919,\n",
      "        0.7210], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.5585, 2.2604,    nan, 1.1464,    nan, 3.1901, 1.3725,\n",
      "        1.0780,    nan,    nan, 1.3769, 1.0755, 2.0334,    nan,    nan,    nan,\n",
      "        1.9788, 2.6029,    nan, 1.2895,    nan,    nan, 0.9138,    nan,    nan,\n",
      "        0.8653, 1.2486, 1.3270,    nan, 0.9619,    nan,    nan,    nan,    nan,\n",
      "        1.0259,    nan, 0.9198, 1.4006,    nan,    nan, 1.3045, 2.7571,    nan,\n",
      "        0.9860, 1.2826,    nan,    nan,    nan, 1.0044,    nan, 0.9957, 1.8367,\n",
      "        1.1283,    nan, 1.1363,    nan,    nan,    nan, 2.2503, 1.1088,    nan,\n",
      "        1.2032], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7629, 0.7807, 0.6827, 0.8040, 0.7492, 0.6247, 0.4846, 0.6697, 0.7222,\n",
      "        0.6192, 0.7403, 0.6172, 0.7229, 0.6405, 0.6897, 0.6989, 0.6554, 0.5796,\n",
      "        0.7743, 0.7548, 0.6369, 0.7064, 0.5190, 0.7298, 0.6995, 0.5595, 0.7623,\n",
      "        0.6111, 0.7327, 0.7631, 0.6843, 0.6067, 0.7214, 0.7003, 0.7129, 0.7933,\n",
      "        0.6796, 0.6867, 0.6905, 0.7341, 0.6680, 0.7053, 0.7361, 0.6445, 0.6986,\n",
      "        0.6051, 0.7093, 0.7242, 0.6224, 0.7925, 0.8149, 0.6896, 0.5885, 0.5348,\n",
      "        0.6716, 0.7296, 0.7357, 0.8007, 0.6121, 0.6066, 0.6786, 0.6408, 0.7574,\n",
      "        0.6681], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.0751,    nan, 1.6206, 2.5746, 2.2634,    nan,    nan,    nan, 1.1283,\n",
      "           nan, 1.5942,    nan, 0.8358,    nan,    nan, 0.9910,    nan,    nan,\n",
      "        1.4672, 1.0107,    nan, 1.9638,    nan, 1.0088, 1.7605,    nan, 1.1195,\n",
      "           nan, 1.0555, 0.9475, 1.0478,    nan,    nan, 1.2861, 0.8063, 0.9939,\n",
      "           nan,    nan,    nan, 1.2373,    nan, 1.4874, 2.0997,    nan, 1.6018,\n",
      "           nan, 1.1820, 1.3124,    nan, 1.2726, 2.2571,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.5683, 1.7675,    nan,    nan,    nan,    nan, 2.1701,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7148, 0.6033, 0.7272, 0.6781, 0.7360, 0.5289, 0.6651, 0.7082, 0.6501,\n",
      "        0.6107, 0.6848, 0.6312, 0.7817, 0.7339, 0.6348, 0.7213, 0.7519, 0.7100,\n",
      "        0.6860, 0.6956, 0.7883, 0.8361, 0.6558, 0.7408, 0.6297, 0.5195, 0.9486,\n",
      "        0.7095, 0.7035, 0.6235, 0.7123, 0.7405, 0.7122, 0.5970, 0.7572, 0.7323,\n",
      "        0.7622, 0.6905, 0.6472, 0.6470, 0.7541, 0.7160, 0.6206, 0.7456, 0.5940,\n",
      "        0.6378, 0.7509, 0.8875, 0.6999, 0.7489, 0.6379, 0.7376, 0.6036, 0.6431,\n",
      "        0.6529, 0.7328, 0.7528, 0.7162, 0.6775, 0.7595, 0.7132, 0.6116, 0.7612,\n",
      "        0.6144], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.4201,    nan, 1.1362,    nan, 2.4817,    nan, 1.2287, 1.1157,    nan,\n",
      "           nan,    nan,    nan, 1.5461, 1.3946,    nan, 1.0188, 1.2822, 1.4111,\n",
      "           nan,    nan, 1.6120, 2.0464,    nan, 1.5555,    nan,    nan, 3.1939,\n",
      "        1.8488, 1.1642,    nan, 0.8890, 1.0389, 1.1050,    nan, 1.8424, 1.5068,\n",
      "        1.2239,    nan,    nan,    nan, 2.2375, 1.1332,    nan, 1.6855,    nan,\n",
      "           nan, 1.3602, 2.2418, 1.2647, 1.1522,    nan, 1.4176,    nan,    nan,\n",
      "        1.6994, 1.0338, 1.1073, 1.2639,    nan, 1.8916, 2.0122,    nan, 1.6659,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.6839, 0.6874, 0.7495, 0.5968, 0.5854, 0.5950, 0.6346, 0.7589, 0.6323,\n",
      "        0.6006, 0.5769, 0.6720, 0.7173, 0.6993, 0.7450, 0.7548, 0.6807, 0.6757,\n",
      "        0.7551, 0.6301, 0.7548, 0.6280, 0.6320, 0.5918, 0.6066, 0.6590, 0.6472,\n",
      "        0.7072, 0.6255, 0.6592, 0.5400, 0.8463, 0.5831, 0.6492, 0.4835, 0.6359,\n",
      "        0.7356, 0.8209, 0.6405, 0.5882, 0.6070, 0.6204, 0.6123, 0.7752, 0.6770,\n",
      "        0.5867, 0.6495, 0.6149, 0.7170, 0.5548, 0.8017, 0.7411, 0.6419, 0.6227,\n",
      "        0.5639, 0.7824, 0.5935, 0.5766, 0.6676, 0.6450, 0.7528, 0.7957, 0.6670,\n",
      "        0.7002], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.6004, 1.3396, 2.3326, 2.3236, 2.5488, 1.8762, 2.6543, 2.0734, 3.0370,\n",
      "        2.4733, 2.5117, 1.9344, 0.8865, 1.3966, 0.9913, 1.9859, 2.1581, 1.6373,\n",
      "        1.3000, 1.9446, 2.0488, 2.3594, 1.7079, 2.4435, 2.0216, 1.3746, 1.6985,\n",
      "        2.0146, 2.6624, 1.6566, 2.5113, 2.3281, 2.9296, 2.5357, 3.2012, 2.1223,\n",
      "        1.4720, 2.2440, 2.6402, 3.7268, 2.2742, 2.7799, 1.8786, 1.6165, 2.4030,\n",
      "        1.1768, 1.7894, 2.7438, 1.0428, 2.5399, 2.2184, 1.4485, 2.2899, 2.4828,\n",
      "        2.1316, 1.0903, 2.4451, 2.0806, 1.3104, 2.2205, 1.2752, 1.5943, 1.6234,\n",
      "        1.3654], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7856, 0.6212, 0.6920, 0.7605, 0.6326, 0.5446, 0.6507, 0.5758, 0.6612,\n",
      "        0.6131, 0.6889, 0.5961, 0.7016, 0.6938, 0.5865, 0.7237, 0.7604, 0.8602,\n",
      "        0.5239, 0.6362, 0.6814, 0.6874, 0.5873, 0.7975, 0.7833, 0.7173, 0.5856,\n",
      "        0.6661, 0.6044, 0.5505, 0.7022, 0.6612, 0.6715, 0.7399, 0.8208, 0.7277,\n",
      "        0.7217, 0.6414, 0.7302, 0.6110, 0.8361, 0.6675, 0.7308, 0.6910, 0.6325,\n",
      "        0.7285, 0.5972, 0.6405, 0.6029, 0.5941, 0.7433, 0.7115, 0.7872, 0.7099,\n",
      "        0.6723, 0.5396, 0.5941, 0.7231, 0.4834, 0.7341, 0.6230, 0.7247, 0.7501,\n",
      "        0.5512], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.7746,    nan, 1.5794, 2.3406,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.3794,    nan,    nan, 1.5303, 1.8085, 2.3469,\n",
      "           nan,    nan, 1.1407, 0.8990,    nan, 1.9572, 1.7188, 0.8931,    nan,\n",
      "           nan,    nan,    nan, 1.9481,    nan, 1.8153, 1.0638, 2.2484, 1.2716,\n",
      "        0.9686,    nan, 2.0181,    nan, 2.4008,    nan, 0.9316, 1.7904,    nan,\n",
      "        0.8567,    nan,    nan,    nan,    nan,    nan, 1.7232,    nan, 0.9123,\n",
      "           nan,    nan,    nan, 1.2854,    nan, 1.0668,    nan, 1.2285, 1.2317,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6411, 0.7362, 0.6710, 0.6451, 0.7536, 0.7024, 0.7478, 0.7269, 0.7255,\n",
      "        0.6093, 0.6401, 0.7082, 0.7770, 0.8772, 0.5950, 0.7305, 0.6984, 0.8383,\n",
      "        0.6854, 0.6110, 0.7380, 0.7147, 0.8025, 0.7144, 0.6664, 0.6207, 0.6941,\n",
      "        0.6731, 0.6269, 0.7113, 0.7354, 0.7024, 0.7785, 0.6233, 0.6211, 0.4520,\n",
      "        0.6530, 0.6076, 0.5975, 0.7564, 0.7410, 0.7376, 0.5516, 0.6298, 0.5181,\n",
      "        0.7239, 0.5296, 0.5480, 0.5496, 0.6677, 0.6872, 0.5928, 0.6227, 0.8680,\n",
      "        0.6094, 0.7551, 0.6387, 0.7597, 0.4867, 0.8862, 0.6719, 0.6683, 0.7070,\n",
      "        0.6845], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 0.9241, 1.1951,    nan, 1.0342, 0.8916, 1.4062, 1.3707, 1.4740,\n",
      "           nan,    nan, 1.1689, 1.7310, 2.1387,    nan, 2.1091, 1.6455, 1.3537,\n",
      "           nan,    nan, 0.9943, 1.1848, 1.5071, 0.8172,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.5139, 1.6362, 1.8749, 1.1689,    nan,    nan,    nan,\n",
      "        1.3455,    nan,    nan, 1.4894, 1.1173, 2.9922,    nan,    nan,    nan,\n",
      "        2.3049,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.0912,\n",
      "           nan,    nan,    nan, 0.9706,    nan, 2.3924,    nan,    nan, 1.0744,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7029, 0.5891, 0.7398, 0.7220, 0.6493, 0.6641, 0.6967, 0.6902, 0.6741,\n",
      "        0.6320, 0.7763, 0.8375, 0.6256, 0.7889, 0.6662, 0.7454, 0.6074, 0.6284,\n",
      "        0.7140, 0.6122, 0.7549, 0.7369, 0.6048, 0.9090, 0.7053, 0.7225, 0.6724,\n",
      "        0.6845, 0.6747, 0.6851, 0.7207, 0.5667, 0.6588, 0.4903, 0.7518, 0.7380,\n",
      "        0.6814, 0.7304, 0.5502, 0.6532, 0.7152, 0.5990, 0.7573, 0.6669, 0.7975,\n",
      "        0.7709, 0.7716, 0.5214, 0.7162, 0.6563, 0.5796, 0.7681, 0.7084, 0.6571,\n",
      "        0.7242, 0.6692, 0.7431, 0.7019, 0.7736, 0.9168, 0.7151, 0.7911, 0.8154,\n",
      "        0.7017], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.0029,    nan, 1.0827, 1.1644,    nan,    nan, 1.9623, 1.3985,    nan,\n",
      "           nan, 2.2891, 1.8862,    nan, 1.3134,    nan, 1.2508,    nan,    nan,\n",
      "        1.0848,    nan, 1.9385, 0.8001,    nan, 2.1706, 1.2210, 1.4262, 2.2899,\n",
      "           nan, 1.5704,    nan, 2.1855,    nan,    nan,    nan, 1.1155, 2.0656,\n",
      "        1.0797, 1.2996,    nan,    nan, 1.8345,    nan, 1.4919,    nan, 1.4021,\n",
      "        1.3770, 1.7284,    nan, 1.0650,    nan,    nan, 1.2428, 1.1536,    nan,\n",
      "        1.1374,    nan, 1.1329, 1.0029, 0.7205, 2.8942, 1.2060, 2.8782, 1.7789,\n",
      "        2.1797], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.5622, 0.5699, 0.8065, 0.7041, 0.5829, 0.7239, 0.8670, 0.7516, 0.8384,\n",
      "        0.6939, 0.7794, 0.6760, 0.7036, 0.8924, 0.6618, 0.6076, 0.7226, 0.7542,\n",
      "        0.7816, 0.5888, 0.6788, 0.7533, 0.7039, 0.5989, 0.6963, 0.6585, 0.8299,\n",
      "        0.7942, 0.6364, 0.5764, 0.7320, 0.7436, 0.6312, 0.6557, 0.6849, 0.6917,\n",
      "        0.7433, 0.6341, 0.6779, 0.6812, 0.7126, 0.5888, 0.7533, 0.7480, 0.6399,\n",
      "        0.6234, 0.5878, 0.5930, 0.6254, 0.6306, 0.7209, 0.5850, 0.6702, 0.6583,\n",
      "        0.7070, 0.8224, 0.7030, 0.7800, 0.7314, 0.6465, 0.5502, 0.6020, 0.6617,\n",
      "        0.8763], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.3260, 0.9814,    nan, 1.3180, 2.3910, 1.9247, 2.3490,\n",
      "        0.9130, 1.1185,    nan, 1.6290, 2.6002,    nan,    nan, 1.1212, 1.0489,\n",
      "        1.5342,    nan,    nan, 1.1217, 0.9645,    nan, 1.0244,    nan, 2.7718,\n",
      "        1.0426,    nan,    nan, 1.3572, 1.0382,    nan,    nan,    nan, 0.8134,\n",
      "        1.0286,    nan, 1.5251, 1.5215, 0.8775,    nan, 2.6667, 1.5827,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan, 2.5675,    nan, 1.3491, 0.8871,    nan,    nan,    nan,    nan,\n",
      "        2.6103], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.5264, 0.6953, 0.5922, 0.7854, 0.5798, 0.5837, 0.6498, 0.6037, 0.6388,\n",
      "        0.6893, 0.5843, 0.7004, 0.8698, 0.7127, 0.5392, 0.7087, 0.6388, 0.6114,\n",
      "        0.8131, 0.5636, 0.6342, 0.7735, 0.6099, 0.6009, 0.7956, 0.6338, 0.6356,\n",
      "        0.6997, 0.5981, 0.7137, 0.5775, 0.5953, 0.6853, 0.6570, 0.6934, 0.5769,\n",
      "        0.7593, 0.7513, 0.6138, 0.6064, 0.5852, 0.6051, 0.6401, 0.6156, 0.7395,\n",
      "        0.6215, 0.6380, 0.7510, 0.6183, 0.6070, 0.6905, 0.7379, 0.6294, 0.7497,\n",
      "        0.6168, 0.5807, 0.6699, 0.7085, 0.7137, 0.6438, 0.7273, 0.7754, 0.5937,\n",
      "        0.5177], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.3735,    nan, 1.4588,    nan,    nan,    nan,    nan,    nan,\n",
      "        1.3264,    nan, 1.0665, 2.6606, 1.0015,    nan, 1.6375,    nan,    nan,\n",
      "        2.1852,    nan,    nan, 1.8353,    nan,    nan, 2.3844,    nan,    nan,\n",
      "        1.1653,    nan, 1.4676,    nan,    nan,    nan,    nan, 1.0244,    nan,\n",
      "        1.2383, 1.1506,    nan,    nan,    nan,    nan,    nan,    nan, 1.0019,\n",
      "           nan,    nan, 0.9610,    nan,    nan, 1.2588, 2.0421,    nan, 1.4086,\n",
      "           nan,    nan, 2.2212, 1.1349, 1.1662,    nan, 1.6869, 1.0379,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6029, 0.7008, 0.6285, 0.7123, 0.6908, 0.7138, 0.7161, 0.6176, 0.5707,\n",
      "        0.6985, 0.4844, 0.7004, 0.5899, 0.6933, 0.7836, 0.6325, 0.7217, 0.7426,\n",
      "        0.7512, 0.6088, 0.7083, 0.6112, 0.5528, 0.8262, 0.5959, 0.7850, 0.4527,\n",
      "        0.5313, 0.5930, 0.8585, 0.7722, 0.5869, 0.7858, 0.5646, 0.7427, 0.7350,\n",
      "        0.5428, 0.6958, 0.8036, 0.6573, 0.7769, 0.7355, 0.6827, 0.6650, 0.6510,\n",
      "        0.8225, 0.6022, 0.6358, 0.7673, 0.7134, 0.7065, 0.6761, 0.5868, 0.7266,\n",
      "        0.7126, 0.6046, 0.6831, 0.7120, 0.7758, 0.7650, 0.6428, 0.7630, 0.5923,\n",
      "        0.7085], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.8007,    nan, 0.8109,    nan, 1.1840, 1.2218,    nan,    nan,\n",
      "        0.8939,    nan, 1.1008,    nan, 1.1445, 1.7100,    nan, 0.9720, 1.6562,\n",
      "        1.0385,    nan, 1.8019,    nan,    nan, 1.9554,    nan, 1.1690,    nan,\n",
      "           nan,    nan, 2.1887, 2.3074,    nan, 1.0760,    nan, 1.6204, 1.8882,\n",
      "           nan,    nan, 1.1968,    nan, 1.4548, 1.0944,    nan,    nan,    nan,\n",
      "        1.9402,    nan,    nan, 1.0464, 0.9906, 1.0127,    nan,    nan, 0.8001,\n",
      "        1.0603,    nan,    nan, 1.1845, 1.7044, 1.5899,    nan, 1.5592,    nan,\n",
      "        1.3756], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6562, 0.6308, 0.6227, 0.7705, 0.5637, 0.7910, 0.5294, 0.5932, 0.7415,\n",
      "        0.7385, 0.6302, 0.5762, 0.6999, 0.5935, 0.7283, 0.7830, 0.6801, 0.6695,\n",
      "        0.8415, 0.6591, 0.6014, 0.6597, 0.8072, 0.7580, 0.7650, 0.7266, 0.7650,\n",
      "        0.7976, 0.7389, 0.7749, 0.7641, 0.7304, 0.5640, 0.6614, 0.5557, 0.6800,\n",
      "        0.7036, 0.7288, 0.5913, 0.7706, 0.7166, 0.7325, 0.7205, 0.7388, 0.7641,\n",
      "        0.5013, 0.6420, 0.6919, 0.6803, 0.7698, 0.7553, 0.6267, 0.6367, 0.5441,\n",
      "        0.6236, 0.8186, 0.6049, 0.6322, 0.6633, 0.7900, 0.7368, 0.7920, 0.6232,\n",
      "        0.5826], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.4685,    nan, 2.6762,    nan,    nan, 1.0063,\n",
      "        1.1188,    nan,    nan, 1.2519,    nan, 1.3080, 1.7463, 1.0618,    nan,\n",
      "        2.0361,    nan,    nan,    nan, 1.5591, 1.8073, 1.6835, 1.1391, 2.2175,\n",
      "        1.5436, 1.3109, 2.7393,    nan, 1.8298,    nan,    nan,    nan, 2.1018,\n",
      "        1.0236, 1.3187,    nan, 1.7289, 1.9265, 1.4367, 1.0112, 2.0927, 1.4162,\n",
      "           nan,    nan, 1.5875,    nan, 2.2091, 1.3223,    nan,    nan,    nan,\n",
      "           nan, 1.4618,    nan,    nan,    nan, 1.3627, 1.2738, 1.3990,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7083, 0.6300, 0.7076, 0.6912, 0.7240, 0.6708, 0.8431, 0.7499, 0.6685,\n",
      "        0.6949, 0.7504, 0.6203, 0.6982, 0.6598, 0.7211, 0.7407, 0.6261, 0.6716,\n",
      "        0.6820, 0.7399, 0.6716, 0.6385, 0.7807, 0.6300, 0.7056, 0.7373, 0.8217,\n",
      "        0.8069, 0.7086, 0.8181, 0.7446, 0.6269, 0.6517, 0.7110, 0.6312, 0.7242,\n",
      "        0.6624, 0.8263, 0.6153, 0.6917, 0.6225, 0.8387, 0.7603, 0.8051, 0.6140,\n",
      "        0.6310, 0.7235, 0.7358, 0.6798, 0.7722, 0.7365, 0.7546, 0.6070, 0.6248,\n",
      "        0.7080, 0.6383, 0.7246, 0.4826, 0.6870, 0.7473, 0.7138, 0.7259, 0.6365,\n",
      "        0.7147], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.6008,    nan, 0.7854, 0.9005, 1.2081,    nan, 2.3496,    nan,    nan,\n",
      "        1.0844, 1.0408,    nan, 0.9278,    nan, 0.9687, 1.2571,    nan,    nan,\n",
      "           nan, 0.9942, 1.0018,    nan, 1.7339,    nan, 1.1069, 1.4764, 1.8543,\n",
      "        1.5785, 1.0379, 1.7791,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan, 1.7641,    nan, 1.7782,    nan, 1.5916, 2.4559, 1.3397,    nan,\n",
      "           nan, 1.6736, 2.0352,    nan, 1.4415, 1.1538, 0.8618,    nan,    nan,\n",
      "        1.3900,    nan,    nan,    nan,    nan, 2.1102, 2.2404, 0.9698,    nan,\n",
      "        0.8955], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6292, 0.7312, 0.5153, 0.6337, 0.6388, 0.8267, 0.9207, 0.6034, 0.5297,\n",
      "        0.6928, 0.7843, 0.7548, 0.6710, 0.5829, 0.7316, 0.6635, 0.6898, 0.6256,\n",
      "        0.6907, 0.5445, 0.6959, 0.5707, 0.7693, 0.5670, 0.7114, 0.5435, 0.7380,\n",
      "        0.6476, 0.6749, 0.6840, 0.5825, 0.6743, 0.7115, 0.6222, 0.7785, 0.7156,\n",
      "        0.6255, 0.5861, 0.7506, 0.6129, 0.6775, 0.5988, 0.6829, 0.6408, 0.6210,\n",
      "        0.5126, 0.6159, 0.6858, 0.7911, 0.7165, 0.7190, 0.5915, 0.7307, 0.6343,\n",
      "        0.7662, 0.7424, 0.5958, 0.5753, 0.8491, 0.7030, 0.7632, 0.7271, 0.6808,\n",
      "        0.8168], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.5362,    nan,    nan,    nan, 2.1175, 2.6015,    nan,    nan,\n",
      "        1.2933, 2.5550, 2.1903,    nan,    nan, 3.2014,    nan, 1.0628,    nan,\n",
      "        1.2429,    nan,    nan,    nan, 1.5302,    nan, 0.9186,    nan, 0.9187,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.3611, 1.0772,\n",
      "           nan,    nan, 0.8964,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan, 2.2364, 2.2866, 1.5227,    nan, 0.9932,    nan,\n",
      "        1.8486, 1.3052,    nan,    nan, 2.0026, 1.1194, 0.9900, 1.0429,    nan,\n",
      "        1.3728], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7119, 0.7780, 0.5780, 0.5498, 0.9141, 0.6790, 0.7296, 0.6370, 0.6121,\n",
      "        0.6541, 0.5590, 0.5650, 0.6901, 0.6202, 0.6306, 0.7774, 0.7242, 0.5623,\n",
      "        0.6011, 0.5615, 0.7529, 0.7555, 0.7911, 0.5658, 0.7359, 0.8008, 0.7342,\n",
      "        0.7128, 0.8304, 0.7000, 0.7359, 0.6357, 0.7364, 0.7498, 0.5840, 0.5429,\n",
      "        0.6293, 0.7001, 0.7521, 0.7480, 0.6904, 0.8151, 0.6010, 0.7732, 0.5622,\n",
      "        0.6659, 0.6805, 0.5733, 0.6874, 0.7394, 0.6102, 0.7019, 0.6634, 0.7312,\n",
      "        0.6763, 0.6575, 0.7574, 0.6755, 0.7847, 0.7372, 0.7589, 0.5637, 0.6453,\n",
      "        0.7679], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.0209, 1.3320,    nan,    nan, 2.3217,    nan, 1.1652,    nan,    nan,\n",
      "           nan,    nan,    nan, 3.4261,    nan,    nan, 1.0769, 0.8629,    nan,\n",
      "           nan,    nan, 1.7807, 1.1619, 1.0510,    nan, 1.0717, 1.4268, 1.2169,\n",
      "           nan, 1.1372, 0.8798, 1.0663,    nan, 1.8462, 2.7338,    nan,    nan,\n",
      "           nan, 0.9261, 1.0492, 1.1398,    nan, 1.1529,    nan, 1.1905,    nan,\n",
      "           nan,    nan,    nan, 0.9891, 0.9388,    nan, 1.1359,    nan, 0.8492,\n",
      "        1.4758,    nan, 2.1701, 2.5732, 1.3692, 1.3907, 0.9498,    nan,    nan,\n",
      "        1.3771], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.8155, 0.6002, 0.8149, 0.6968, 0.8213, 0.6428, 0.7516, 0.7160, 0.6902,\n",
      "        0.6559, 0.7302, 0.7677, 0.7750, 0.6369, 0.6698, 0.7323, 0.6666, 0.6170,\n",
      "        0.6019, 0.6624, 0.7266, 0.7186, 0.6883, 0.7110, 0.6942, 0.7114, 0.7750,\n",
      "        0.7614, 0.8991, 0.8034, 0.7836, 0.6907, 0.7627, 0.4928, 0.6594, 0.7641,\n",
      "        0.6964, 0.7504, 0.7450, 0.6150, 0.6200, 0.7445, 0.7193, 0.7039, 0.7623,\n",
      "        0.8703, 0.6549, 0.6899, 0.7231, 0.5665, 0.5936, 0.6312, 0.6168, 0.7183,\n",
      "        0.6529, 0.5569, 0.6805, 0.7150, 0.5255, 0.7604, 0.7224, 0.7326, 0.6446,\n",
      "        0.5832], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5012, 2.6364, 1.2790, 1.1067, 1.2559, 1.3346, 1.1972, 1.1354, 0.5705,\n",
      "        3.5133, 0.7882, 1.4031, 1.7898, 1.8046, 0.0000, 0.8588, 1.2605, 2.2822,\n",
      "        3.7132, 2.1274, 1.2529, 1.6934, 1.0691, 1.0074, 1.1214, 1.0089, 1.1247,\n",
      "        1.7393, 3.5751, 0.9018, 1.4856, 2.8363, 2.9026, 3.6129, 3.0184, 1.7335,\n",
      "        1.6011, 1.0456, 1.5374, 2.1095, 2.0479, 2.1100, 0.9999, 1.0409, 2.3384,\n",
      "        1.4906, 1.9123, 1.8752, 1.2708, 4.0902, 1.8866, 1.6503, 1.3609, 1.2584,\n",
      "        3.4283, 2.8134, 1.3461, 1.3621, 2.6732, 1.1548, 0.9407, 1.3161, 2.0226,\n",
      "        2.2607], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7202, 0.5695, 0.7245, 0.5387, 0.7433, 0.6207, 0.6287, 0.7065, 0.7189,\n",
      "        0.5251, 0.7213, 0.7972, 0.7195, 0.6337, 0.7618, 0.5098, 0.7346, 0.6229,\n",
      "        0.6774, 0.5579, 0.7211, 0.5914, 0.6620, 0.7593, 0.6901, 0.5950, 0.5811,\n",
      "        0.7159, 0.7021, 0.5761, 0.6888, 0.7159, 0.5601, 0.6838, 0.5613, 0.7362,\n",
      "        0.7125, 0.6943, 0.6856, 0.6018, 0.7216, 0.4842, 0.6559, 0.8197, 0.6652,\n",
      "        0.6414, 0.6615, 0.7085, 0.6954, 0.7161, 0.7277, 0.8770, 0.6396, 0.7452,\n",
      "        0.7438, 0.5915, 0.6672, 0.7128, 0.5907, 0.5942, 0.7241, 0.6374, 0.6071,\n",
      "        0.5664], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8738,    nan, 0.8547,    nan, 1.5379,    nan,    nan, 1.1718, 1.2159,\n",
      "           nan, 1.4143, 1.2486, 2.4055,    nan, 1.1376,    nan, 0.9288,    nan,\n",
      "           nan,    nan, 1.5878,    nan,    nan, 1.3702, 1.2122,    nan,    nan,\n",
      "        1.0384, 1.0194,    nan,    nan, 1.0880,    nan,    nan,    nan, 1.2218,\n",
      "        2.0148, 1.1809, 1.6272,    nan, 1.1262,    nan, 1.6595, 2.7326, 2.4127,\n",
      "           nan, 2.0386, 1.4076,    nan, 0.9140, 1.5016, 2.9258,    nan, 0.8571,\n",
      "        1.3567,    nan, 1.8029, 0.7667,    nan,    nan, 1.1464,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6137, 0.6491, 0.7703, 0.5992, 0.7113, 0.7119, 0.4899, 0.7155, 0.7539,\n",
      "        0.7229, 0.7313, 0.7559, 0.5453, 0.7377, 0.6748, 0.7485, 0.7357, 0.5378,\n",
      "        0.6256, 0.7342, 0.7065, 0.5986, 0.5909, 0.4930, 0.7730, 0.7034, 0.6068,\n",
      "        0.6247, 0.7212, 0.5976, 0.6202, 0.6276, 0.5279, 0.6354, 0.6459, 0.6320,\n",
      "        0.7049, 0.6769, 0.6901, 0.7713, 0.7079, 0.6233, 0.6387, 0.7248, 0.7643,\n",
      "        0.7389, 0.6387, 0.6146, 0.6381, 0.7247, 0.5752, 0.5566, 0.5766, 0.6621,\n",
      "        0.7632, 0.8354, 0.7317, 0.7674, 0.8510, 0.5968, 0.7330, 0.7878, 0.6725,\n",
      "        0.6662], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.4807,    nan, 1.3086, 1.5699,    nan, 1.7650, 1.5999,\n",
      "        1.0782, 0.8787,    nan,    nan, 1.1800,    nan, 2.4692, 1.0799,    nan,\n",
      "           nan, 1.2784, 1.8230,    nan,    nan,    nan, 1.2999, 1.0339,    nan,\n",
      "           nan, 1.8704,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan, 1.2322, 0.9971, 0.9952,    nan,    nan, 1.4112, 2.6465,\n",
      "        1.3098,    nan,    nan,    nan, 1.5479,    nan,    nan,    nan,    nan,\n",
      "        1.7901, 2.2154, 1.1233, 1.2345, 2.0022,    nan, 1.0389, 2.2210,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7569, 0.6179, 0.6692, 0.6345, 0.6814, 0.5578, 0.7106, 0.6575, 0.6364,\n",
      "        0.5847, 0.6841, 0.6969, 0.5876, 0.5638, 0.5801, 0.5506, 0.7694, 0.7152,\n",
      "        0.6281, 0.6459, 0.6767, 0.6977, 0.6442, 0.8873, 0.7601, 0.7094, 0.5371,\n",
      "        0.7466, 0.5662, 0.6803, 0.7659, 0.7518, 0.6031, 0.6984, 0.6043, 0.6867,\n",
      "        0.7499, 0.6785, 0.6610, 0.6880, 0.5749, 0.6124, 0.7687, 0.5963, 0.7236,\n",
      "        0.7183, 0.7425, 0.6762, 0.6473, 0.7254, 0.6809, 0.6889, 0.6384, 0.5375,\n",
      "        0.6203, 0.7006, 0.6994, 0.8099, 0.7774, 0.7274, 0.6069, 0.7333, 0.5178,\n",
      "        0.7267], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8480,    nan,    nan,    nan, 2.2461,    nan, 0.9566,    nan,    nan,\n",
      "           nan,    nan, 1.0667,    nan,    nan,    nan,    nan, 2.0531, 0.8670,\n",
      "           nan,    nan,    nan, 1.0092,    nan, 2.8417, 1.0534, 0.8907,    nan,\n",
      "        1.6001,    nan, 1.4665, 1.3225, 2.0798,    nan, 1.2597,    nan,    nan,\n",
      "        1.8976,    nan,    nan, 1.5888,    nan,    nan, 2.0474,    nan, 1.5126,\n",
      "        1.5733, 1.0491,    nan,    nan, 0.8271, 1.0908, 1.5728,    nan,    nan,\n",
      "           nan, 1.7494, 0.9311, 1.6070, 1.8793, 2.3753,    nan, 0.7870,    nan,\n",
      "        1.1088], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6303, 0.6947, 0.7147, 0.6369, 0.6220, 0.7522, 0.7267, 0.6167, 0.5926,\n",
      "        0.5912, 0.5950, 0.5528, 0.5790, 0.7284, 0.7548, 0.7289, 0.7373, 0.7180,\n",
      "        0.7162, 0.7656, 0.7519, 0.4599, 0.5328, 0.7390, 0.6136, 0.8070, 0.7555,\n",
      "        0.6199, 0.7934, 0.8351, 0.7981, 0.6355, 0.6218, 0.7255, 0.7348, 0.6179,\n",
      "        0.6812, 0.5430, 0.7081, 0.7067, 0.6438, 0.5561, 0.7707, 0.6035, 0.6939,\n",
      "        0.7775, 0.5970, 0.7644, 0.6283, 0.7461, 0.6321, 0.6647, 0.6371, 0.6249,\n",
      "        0.7434, 0.5905, 0.6209, 0.6154, 0.6353, 0.6395, 0.6763, 0.6944, 0.5542,\n",
      "        0.5892], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.9824, 1.1800,    nan,    nan, 1.3423, 1.2849,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 1.7478, 1.0415, 0.9016, 1.9176, 1.5498,\n",
      "        1.1674, 1.7289, 1.1969,    nan,    nan, 1.1840,    nan, 2.1937, 1.6487,\n",
      "           nan, 1.5795, 1.8344, 1.2690,    nan,    nan, 1.4416, 1.1607,    nan,\n",
      "           nan,    nan,    nan, 1.2868,    nan,    nan, 2.0671,    nan, 1.0718,\n",
      "        1.6766,    nan, 1.0989,    nan, 1.2042,    nan,    nan,    nan,    nan,\n",
      "        1.4175,    nan,    nan,    nan,    nan,    nan,    nan, 1.8500,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6319, 0.7454, 0.5241, 0.7887, 0.6024, 0.6828, 0.7266, 0.6044, 0.6220,\n",
      "        0.7871, 0.6331, 0.7415, 0.7843, 0.7393, 0.8277, 0.7847, 0.6397, 0.6306,\n",
      "        0.6864, 0.5582, 0.7016, 0.8678, 0.7901, 0.6324, 0.6466, 0.7246, 0.5453,\n",
      "        0.5513, 0.7098, 0.7372, 0.6549, 0.5090, 0.7569, 0.6449, 0.6911, 0.7103,\n",
      "        0.7442, 0.7355, 0.7278, 0.5395, 0.7207, 0.6073, 0.7469, 0.5773, 0.7002,\n",
      "        0.5812, 0.6143, 0.6154, 0.6828, 0.7514, 0.7327, 0.6743, 0.6892, 0.7370,\n",
      "        0.6243, 0.6688, 0.7654, 0.6050, 0.6071, 0.7063, 0.6982, 0.6368, 0.6748,\n",
      "        0.6500], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 0.8588,    nan, 1.0874,    nan, 1.8512, 0.8030,    nan,    nan,\n",
      "        1.1597,    nan, 0.8052, 1.7683,    nan, 1.6075, 2.0841, 1.4065,    nan,\n",
      "        1.1765,    nan, 1.3016, 2.6117, 1.7660,    nan,    nan, 1.2611,    nan,\n",
      "           nan, 1.0729, 1.0689,    nan,    nan, 2.1108,    nan,    nan, 1.9311,\n",
      "        1.3872, 1.0051, 1.7312,    nan, 1.1254,    nan, 1.5309,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan, 0.8731, 1.0043,    nan, 0.9250, 0.9025,\n",
      "           nan,    nan, 1.7445,    nan,    nan, 1.9257, 1.9184,    nan, 1.0150,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6730, 0.7611, 0.5310, 0.7227, 0.7859, 0.7174, 0.6846, 0.7827, 0.6981,\n",
      "        0.6536, 0.6079, 0.7243, 0.7432, 0.8179, 0.6695, 0.7234, 0.7573, 0.7889,\n",
      "        0.7209, 0.6914, 0.7415, 0.6363, 0.6500, 0.5938, 0.7462, 0.6428, 0.6766,\n",
      "        0.5596, 0.8352, 0.7418, 0.7196, 0.8463, 0.8085, 0.6717, 0.6160, 0.6867,\n",
      "        0.7326, 0.7120, 0.6717, 0.6813, 0.6928, 0.7065, 0.6793, 0.5977, 0.6885,\n",
      "        0.4789, 0.5226, 0.6076, 0.7178, 0.7112, 0.7718, 0.5927, 0.4771, 0.5998,\n",
      "        0.6512, 0.7029, 0.7479, 0.7982, 0.6154, 0.7246, 0.6537, 0.7334, 0.5797,\n",
      "        0.6489], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.6995,    nan, 1.0941, 1.7812, 1.6392,    nan, 1.7585,    nan,\n",
      "           nan,    nan, 1.1514, 1.8855, 2.0737,    nan, 1.2799, 1.1348, 1.5815,\n",
      "           nan, 1.5164, 2.1478,    nan,    nan,    nan, 1.1136,    nan, 2.2145,\n",
      "           nan, 3.8733, 1.0778, 0.9030, 1.2764, 1.5903, 1.7737,    nan, 1.3692,\n",
      "        1.0549, 1.1300, 1.3397,    nan,    nan, 1.1001, 1.5177,    nan, 1.5611,\n",
      "           nan,    nan,    nan, 1.0995, 0.9501, 2.1613,    nan,    nan,    nan,\n",
      "           nan, 2.4115, 0.9653, 2.4205,    nan, 1.1354,    nan, 1.2927,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7452, 0.7668, 0.6320, 0.7334, 0.7026, 0.6085, 0.6841, 0.7132, 0.7459,\n",
      "        0.5934, 0.8191, 0.8322, 0.6325, 0.6801, 0.5397, 0.7497, 0.5969, 0.6297,\n",
      "        0.7530, 0.6538, 0.6796, 0.5769, 0.6140, 0.5638, 0.7459, 0.5066, 0.6381,\n",
      "        0.6558, 0.6952, 0.6134, 0.6952, 0.6209, 0.7424, 0.6761, 0.6856, 0.6062,\n",
      "        0.7738, 0.7131, 0.6981, 0.7106, 0.7847, 0.7695, 0.6186, 0.7211, 0.5529,\n",
      "        0.6929, 0.7347, 0.6405, 0.8081, 0.7004, 0.7631, 0.6371, 0.6938, 0.7255,\n",
      "        0.7667, 0.6255, 0.7078, 0.6446, 0.6877, 0.6495, 0.7082, 0.7372, 0.6534,\n",
      "        0.7396], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.2815, 1.3973,    nan, 1.1170, 1.4607,    nan, 1.0251, 1.1030, 1.6748,\n",
      "           nan, 1.7719, 2.0306,    nan,    nan,    nan, 1.2271,    nan,    nan,\n",
      "        1.3930,    nan,    nan,    nan,    nan,    nan, 1.0378,    nan,    nan,\n",
      "           nan, 1.0308,    nan, 2.4965,    nan, 1.4118,    nan,    nan,    nan,\n",
      "        2.0075, 0.8714, 1.3365, 1.2367, 1.8700, 1.3812,    nan, 0.9136,    nan,\n",
      "        0.8834, 1.5882,    nan, 1.3159, 1.3018, 1.1988,    nan, 0.9993, 1.0737,\n",
      "        1.0459,    nan, 0.8656,    nan,    nan,    nan, 1.4007, 1.1424,    nan,\n",
      "        1.7107], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6188, 0.6515, 0.5827, 0.7210, 0.7470, 0.7639, 0.5460, 0.7010, 0.7184,\n",
      "        0.7457, 0.7183, 0.7416, 0.5337, 0.6607, 0.7276, 0.4884, 0.7453, 0.6128,\n",
      "        0.6027, 0.8271, 0.7258, 0.6052, 0.5657, 0.6858, 0.7414, 0.8172, 0.7162,\n",
      "        0.6535, 0.7629, 0.6259, 0.6569, 0.7350, 0.5835, 0.6434, 0.7227, 0.7199,\n",
      "        0.6072, 0.7815, 0.6263, 0.6568, 0.6800, 0.6438, 0.6305, 0.7598, 0.4488,\n",
      "        0.6126, 0.5642, 0.7503, 0.7661, 0.6567, 0.7180, 0.5775, 0.7259, 0.6108,\n",
      "        0.7455, 0.6073, 0.7724, 0.6870, 0.7124, 0.6727, 0.4933, 0.7561, 0.6610,\n",
      "        0.5988], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 0.8792, 1.7275, 1.5126,    nan, 1.2305, 1.4277,\n",
      "        0.9862, 1.2938, 2.2276,    nan,    nan, 1.2958,    nan, 0.9232,    nan,\n",
      "           nan, 1.9729, 1.3841,    nan,    nan,    nan, 1.9794, 2.8475, 1.1907,\n",
      "           nan, 1.9726,    nan,    nan, 1.4664,    nan,    nan, 1.1015, 1.9880,\n",
      "           nan, 1.4304,    nan,    nan, 1.8748,    nan,    nan, 1.5691,    nan,\n",
      "           nan,    nan, 1.2684, 1.4205,    nan, 1.2555,    nan,    nan,    nan,\n",
      "        1.4438,    nan, 2.8077, 1.2078, 1.9243,    nan,    nan, 2.2344, 1.7127,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6662, 0.7159, 0.6998, 0.6471, 0.7319, 0.5656, 0.7967, 0.8629, 0.6116,\n",
      "        0.7605, 0.8191, 0.6925, 0.6691, 0.7410, 0.7766, 0.8030, 0.7073, 0.6694,\n",
      "        0.6425, 0.7432, 0.6920, 0.6844, 0.7779, 0.6809, 0.7086, 0.7900, 0.7337,\n",
      "        0.6568, 0.5953, 0.6063, 0.7224, 0.7961, 0.7512, 0.8181, 0.5661, 0.7148,\n",
      "        0.5442, 0.8175, 0.6733, 0.8895, 0.7731, 0.7055, 0.6909, 0.5433, 0.7639,\n",
      "        0.8283, 0.6450, 0.6513, 0.7208, 0.7343, 0.6600, 0.5921, 0.7056, 0.5470,\n",
      "        0.7032, 0.6316, 0.5048, 0.7028, 0.7278, 0.5978, 0.8752, 0.6552, 0.5732,\n",
      "        0.7497], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.5956, 1.2508, 1.3494, 1.7051, 1.1084, 3.7906, 1.9006, 1.5541, 1.7896,\n",
      "        1.2286, 2.0596, 1.1657, 2.0931, 2.2911, 1.5884, 1.5714, 1.0498, 1.7475,\n",
      "        3.8055, 1.1483, 0.9570, 0.9443, 1.1498, 0.0000, 0.9492, 1.7861, 1.2615,\n",
      "        2.2353, 2.8152, 3.9327, 1.0190, 2.1485, 1.6287, 1.4890, 3.0758, 1.8225,\n",
      "        3.2338, 1.6483, 1.3727, 2.5076, 1.1680, 2.1558, 1.0470, 3.0901, 1.2582,\n",
      "        1.5222, 2.5804, 1.8958, 1.5611, 1.2917, 1.3138, 2.9574, 0.9411, 3.8188,\n",
      "        1.0154, 2.9499, 3.4677, 1.1400, 1.9877, 3.1839, 2.6446, 0.7819, 2.6312,\n",
      "        1.0287], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.5719, 0.7592, 0.5938, 0.6207, 0.6268, 0.6442, 0.6039, 0.6690, 0.6063,\n",
      "        0.6029, 0.6769, 0.6250, 0.8591, 0.8523, 0.7175, 0.5853, 0.7797, 0.6649,\n",
      "        0.7004, 0.6667, 0.6651, 0.7343, 0.7055, 0.6374, 0.7166, 0.6838, 0.6384,\n",
      "        0.7254, 0.4215, 0.7267, 0.7845, 0.8024, 0.7624, 0.6952, 0.6090, 0.7025,\n",
      "        0.6174, 0.6625, 0.6146, 0.7192, 0.6826, 0.6679, 0.6391, 0.5887, 0.5695,\n",
      "        0.6238, 0.6918, 0.6129, 0.6520, 0.6688, 0.8142, 0.6652, 0.6747, 0.5863,\n",
      "        0.7478, 0.7904, 0.6035, 0.7521, 0.8399, 0.7015, 0.8450, 0.5604, 0.7396,\n",
      "        0.6746], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.]\n",
      "Mask\n",
      "[1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Distance\n",
      "tensor([1.1533, 2.0105, 2.6722, 3.0839, 1.5529, 2.2276, 2.4800, 2.3993, 2.1162,\n",
      "        1.3334, 2.7115, 2.3761, 1.7001, 2.0396, 1.2790, 2.3793, 1.5981, 2.2673,\n",
      "        1.0301, 2.6018, 2.1188, 1.2961, 2.3644, 1.7791, 2.2028, 2.5726, 2.0061,\n",
      "        1.6209, 4.1505, 1.1740, 2.7955, 2.0321, 1.1751, 1.1432, 2.3605, 1.2792,\n",
      "        1.9712, 2.1836, 1.9861, 1.2064, 1.5186, 2.5405, 2.3603, 2.7660, 3.5493,\n",
      "        2.2041, 2.2663, 1.8163, 2.6071, 2.0882, 1.8733, 1.7737, 2.1742, 2.8393,\n",
      "        1.5587, 1.5599, 1.5987, 0.9958, 1.6697, 2.2076, 1.9319, 2.4536, 1.9696,\n",
      "        1.0192], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.8403, 0.8364, 0.6325, 0.6132, 0.6993, 0.8649, 0.7655, 0.6918, 0.7812,\n",
      "        0.8193, 0.6550, 0.6850, 0.6893, 0.7265, 0.7580, 0.6449, 0.7788, 0.7927,\n",
      "        0.5804, 0.7807, 0.5411, 0.7431, 0.6372, 0.7411, 0.7113, 0.8252, 0.8667,\n",
      "        0.6695, 0.6502, 0.6015, 0.7259, 0.6413, 0.5631, 0.6336, 0.7872, 0.7424,\n",
      "        0.6297, 0.6519, 0.7230, 0.7214, 0.6004, 0.7546, 0.6168, 0.7385, 0.7331,\n",
      "        0.7006, 0.7048, 0.7054, 0.6130, 0.5866, 0.5709, 0.8096, 0.7012, 0.7616,\n",
      "        0.7410, 0.6700, 0.6071, 0.7436, 0.5920, 0.6184, 0.7494, 0.7947, 0.7470,\n",
      "        0.6836], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.2193, 2.3699,    nan,    nan, 1.2780, 1.7192, 1.3118, 2.1034, 2.7108,\n",
      "        1.1388, 1.9411,    nan, 1.5029, 1.3497, 0.8356,    nan, 1.8265, 1.8872,\n",
      "           nan, 1.0847,    nan, 1.2337,    nan, 1.0740, 1.5570, 2.5545, 1.6244,\n",
      "           nan,    nan,    nan, 1.2419,    nan,    nan,    nan, 1.3977, 1.2930,\n",
      "           nan,    nan, 1.7428, 1.2555,    nan, 0.7746,    nan, 1.9814, 1.4275,\n",
      "        1.3759, 1.6683, 1.0556,    nan,    nan,    nan, 2.0353,    nan, 1.1605,\n",
      "        1.0987,    nan,    nan, 2.1166,    nan,    nan, 1.0014, 1.7374, 1.6198,\n",
      "        1.4771], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6955, 0.6827, 0.6634, 0.5525, 0.8117, 0.7286, 0.6729, 0.7567, 0.7613,\n",
      "        0.7499, 0.6245, 0.7208, 0.5903, 0.6415, 0.5291, 0.5901, 0.6166, 0.6647,\n",
      "        0.7400, 0.6487, 0.7999, 0.6204, 0.6856, 0.5390, 0.5407, 0.6704, 0.6166,\n",
      "        0.7287, 0.7529, 0.7524, 0.6201, 0.6530, 0.7757, 0.7024, 0.6668, 0.7556,\n",
      "        0.7887, 0.7087, 0.6155, 0.6862, 0.7250, 0.7476, 0.6049, 0.8437, 0.6829,\n",
      "        0.5758, 0.7800, 0.7560, 0.7023, 0.6970, 0.8605, 0.5498, 0.8289, 0.7112,\n",
      "        0.7255, 0.8557, 0.6295, 0.7770, 0.5972, 0.6112, 0.5661, 0.7364, 0.6837,\n",
      "        0.5484], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan, 1.2876,    nan,    nan, 1.9436,    nan, 1.7025, 1.3477, 1.5108,\n",
      "        1.6567,    nan, 1.2198,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "        2.6322,    nan, 1.6139,    nan, 1.3879,    nan,    nan,    nan,    nan,\n",
      "        0.9889, 1.8458, 1.2712,    nan,    nan, 1.5944,    nan,    nan, 1.4837,\n",
      "        1.5877, 1.0262,    nan, 0.9391, 1.1556, 1.9411,    nan, 1.6460, 1.3833,\n",
      "           nan, 1.5455, 2.1638, 1.8037, 1.0977, 1.8289,    nan, 2.2000, 1.3078,\n",
      "        1.0959, 2.1150,    nan, 1.0564,    nan,    nan,    nan, 1.6494,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6893, 0.6104, 0.6688, 0.8401, 0.4314, 0.7113, 0.6306, 0.6288, 0.6770,\n",
      "        0.6916, 0.6157, 0.7897, 0.5842, 0.6512, 0.7238, 0.6014, 0.7229, 0.7177,\n",
      "        0.6766, 0.6407, 0.7596, 0.6680, 0.6006, 0.5901, 0.6942, 0.5334, 0.7449,\n",
      "        0.7319, 0.6933, 0.7478, 0.7129, 0.6920, 0.6488, 0.5321, 0.5870, 0.7375,\n",
      "        0.6716, 0.5967, 0.6168, 0.6946, 0.7225, 0.6596, 0.6120, 0.7547, 0.7561,\n",
      "        0.8211, 0.7481, 0.7350, 0.5966, 0.6065, 0.5777, 0.9410, 0.6860, 0.5292,\n",
      "        0.5766, 0.6128, 0.5133, 0.7789, 0.6386, 0.6034, 0.7216, 0.8468, 0.6408,\n",
      "        0.6025], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.8229,    nan,    nan, 4.0752,    nan, 1.9899,    nan,    nan,    nan,\n",
      "        2.2870,    nan, 1.9174, 2.4364, 2.4973, 2.1273,    nan, 1.0583, 1.5220,\n",
      "           nan,    nan, 1.2744,    nan,    nan,    nan, 1.2354,    nan,    nan,\n",
      "        0.8329,    nan, 1.3210, 1.0940,    nan,    nan,    nan,    nan, 1.4966,\n",
      "        1.8125,    nan,    nan, 1.1232, 1.5416,    nan,    nan, 1.1069, 1.5438,\n",
      "        2.0826, 1.8368, 2.8558,    nan,    nan,    nan, 2.5402,    nan,    nan,\n",
      "           nan,    nan,    nan, 1.9714, 1.9083,    nan, 1.1924, 2.4014,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7156, 0.6817, 0.7427, 0.7184, 0.6595, 0.5830, 0.5780, 0.7350, 0.7465,\n",
      "        0.6266, 0.6535, 0.6988, 0.7513, 0.6573, 0.6587, 0.7252, 0.6805, 0.6377,\n",
      "        0.9008, 0.5515, 0.7005, 0.7168, 0.6304, 0.5821, 0.6497, 0.7650, 0.7724,\n",
      "        0.7109, 0.8071, 0.5991, 0.5312, 0.5851, 0.6220, 0.6981, 0.8049, 0.7673,\n",
      "        0.5650, 0.7177, 0.6776, 0.6864, 0.6372, 0.7071, 0.8012, 0.6747, 0.7160,\n",
      "        0.6201, 0.7139, 0.7714, 0.7982, 0.6747, 0.8572, 0.5379, 0.9811, 0.5905,\n",
      "        0.5990, 0.6652, 0.6045, 0.7554, 0.7257, 0.7140, 0.7225, 0.6370, 0.8115,\n",
      "        0.7202], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.1264,    nan, 1.8355, 0.9652,    nan,    nan,    nan, 0.9353,    nan,\n",
      "           nan,    nan, 1.1200, 0.9007,    nan,    nan, 1.0720,    nan,    nan,\n",
      "        3.2095,    nan, 1.4898, 0.8559,    nan,    nan,    nan, 1.1673, 1.8078,\n",
      "        1.1974, 1.8677,    nan,    nan,    nan,    nan,    nan, 1.6964, 1.0764,\n",
      "           nan, 1.1178, 1.6255, 2.1786,    nan, 1.1260, 1.6697,    nan,    nan,\n",
      "           nan, 1.1040, 1.6158, 1.7055, 1.1547, 2.2996,    nan, 2.9339,    nan,\n",
      "           nan,    nan,    nan, 1.2365, 0.9970, 1.0420, 1.6486,    nan, 1.5720,\n",
      "        1.1240], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before cap\n",
      "tensor([0.7709, 0.6721, 0.7480, 0.5557, 0.7172, 0.8102, 0.5077, 0.5995, 0.6562,\n",
      "        0.7406, 0.7264, 0.5174, 0.7890, 0.7636, 0.5579, 0.7150, 0.6640, 0.6387,\n",
      "        0.7328, 0.8401, 0.7294, 0.7157, 0.5634, 0.6481, 0.5322, 0.7172, 0.5146,\n",
      "        0.6718, 0.4895, 0.6621, 0.6719, 0.6042, 0.8009, 0.7526, 0.6535, 0.6816,\n",
      "        0.7598, 0.7673, 0.7440, 0.6534, 0.7452, 0.7290, 0.6888, 0.6454, 0.6952,\n",
      "        0.6034, 0.7177, 0.5928, 0.6423, 0.7548, 0.6276, 0.7005, 0.5778, 0.7126,\n",
      "        0.7189, 0.6796, 0.7445, 0.6992, 0.7870, 0.6349, 0.5714, 0.7122, 0.6540,\n",
      "        0.7933], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([1.9111, 2.3712, 1.4384, 3.3588, 0.9912, 1.5103, 3.3993, 3.0555, 2.2892,\n",
      "        0.9553, 0.7361, 2.6076, 1.8437, 2.0654, 2.3485, 2.6800, 3.1119, 1.9519,\n",
      "        1.5903, 1.5120, 1.0785, 1.0603, 2.6221, 2.5131, 3.0516, 1.7427, 3.2520,\n",
      "        2.2218, 3.4082, 0.0000, 0.6739, 2.2789, 1.3682, 1.4525, 2.3159, 1.5956,\n",
      "        1.3623, 1.7449, 1.9140, 1.6732, 0.9287, 0.9664, 2.0887, 3.2875, 0.9901,\n",
      "        2.3971, 1.2765, 2.8235, 2.8062, 0.9178, 2.0987, 1.0809, 2.1268, 1.3415,\n",
      "        0.9372, 2.0726, 0.9275, 0.7544, 2.1495, 2.0088, 4.1654, 0.7935, 1.7619,\n",
      "        0.9366], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6885, 0.6032, 0.7081, 0.5520, 0.7257, 0.7117, 0.6389, 0.7406, 0.6277,\n",
      "        0.7280, 0.6295, 0.7204, 0.6304, 0.7035, 0.5969, 0.6259, 0.6340, 0.6647,\n",
      "        0.7526, 0.7947, 0.6697, 0.7172, 0.7301, 0.7550, 0.7151, 0.7000, 0.6351,\n",
      "        0.6737, 0.7007, 0.6798, 0.6603, 0.5717, 0.5903, 0.7898, 0.7284, 0.7307,\n",
      "        0.7632, 0.6074, 0.7544, 0.7378, 0.7547, 0.7198, 0.7394, 0.5576, 0.5358,\n",
      "        0.7045, 0.6855, 0.7387, 0.7468, 0.8134, 0.5993, 0.6196, 0.7200, 0.6156,\n",
      "        0.7329, 0.6410, 0.7471, 0.6610, 0.5526, 0.6739, 0.6314, 0.6820, 0.6180,\n",
      "        0.4964], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan, 1.9206,    nan, 1.4218, 1.0015,    nan, 1.8957,    nan,\n",
      "        1.6238,    nan, 0.7912,    nan, 1.0695,    nan,    nan,    nan, 1.5285,\n",
      "        1.4211, 1.6587,    nan, 1.1181, 2.1133, 1.7068, 0.7475, 1.5222,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan, 2.0308, 1.3800, 1.2789,\n",
      "        0.8991,    nan, 1.7646, 1.2659, 1.0317, 1.0520, 2.0117,    nan,    nan,\n",
      "        1.3403, 1.4306, 1.0718, 1.2702, 1.8957,    nan,    nan, 1.3043,    nan,\n",
      "           nan,    nan, 1.5142,    nan,    nan, 1.0847,    nan,    nan,    nan,\n",
      "           nan], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7215, 0.5991, 0.6464, 0.6923, 0.6202, 0.7312, 0.7686, 0.6472, 0.7261,\n",
      "        0.7458, 0.6194, 0.6862, 0.7303, 0.6784, 0.6122, 0.7256, 0.5852, 0.6871,\n",
      "        0.7702, 0.7204, 0.4742, 0.5825, 0.7434, 0.7474, 0.7021, 0.7665, 0.6628,\n",
      "        0.7831, 0.7271, 0.7122, 0.5714, 0.6324, 0.7591, 0.5900, 0.7620, 0.7145,\n",
      "        0.7384, 0.5781, 0.7345, 0.5373, 0.7152, 0.6139, 0.7831, 0.5824, 0.6852,\n",
      "        0.8522, 0.5636, 0.7804, 0.8769, 0.7078, 0.8001, 0.8424, 0.6067, 0.5972,\n",
      "        0.6984, 0.5911, 0.6424, 0.7089, 0.6254, 0.7190, 0.6870, 0.8677, 0.6358,\n",
      "        0.6652], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([   nan,    nan,    nan, 1.7711,    nan, 1.4233, 1.4207,    nan, 1.2878,\n",
      "        1.7972,    nan, 1.7310, 1.2527, 1.5979,    nan, 2.5988,    nan,    nan,\n",
      "        1.5664, 1.2388,    nan,    nan, 1.1834, 1.5193, 1.6708, 1.0969,    nan,\n",
      "        1.7025, 1.1461, 1.7293,    nan,    nan, 1.2890,    nan, 1.7938, 1.9030,\n",
      "        1.0236,    nan, 1.2303,    nan, 2.3089,    nan, 1.3085,    nan,    nan,\n",
      "        1.9781,    nan, 1.4925, 2.1644, 1.1311, 1.6501, 2.3563,    nan,    nan,\n",
      "        1.4766,    nan,    nan, 1.0683,    nan, 1.3120, 1.7257, 1.7376,    nan,\n",
      "        1.7057], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.7456, 0.5299, 0.6812, 0.5541, 0.5085, 0.7730, 0.7969, 0.7130, 0.6079,\n",
      "        0.7660, 0.6650, 0.5996, 0.6550, 0.6832, 0.7451, 0.7349, 0.6810, 0.7016,\n",
      "        0.6199, 0.5910, 0.7314, 0.7330, 0.8016, 0.6925, 0.7076, 0.7358, 0.7076,\n",
      "        0.7514, 0.7237, 0.6034, 0.6379, 0.7437, 0.6099, 0.6858, 0.7557, 0.7496,\n",
      "        0.7059, 0.5343, 0.6268, 0.5837, 0.7881, 0.4908, 0.6913, 0.7324, 0.7133,\n",
      "        0.6317, 0.6570, 0.6384, 0.7592, 0.7153, 0.6639, 0.7368, 0.6271, 0.7340,\n",
      "        0.5789, 0.6894, 0.5676, 0.7500, 0.7271, 0.7497, 0.6631, 0.7554, 0.7519,\n",
      "        0.6733], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 0., 0., 1.], dtype=torch.float64)\n",
      "Mask\n",
      "[1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([0.9674, 3.3831, 0.9928, 2.5860, 4.3056, 1.4505, 2.8066, 1.4861, 2.5876,\n",
      "        1.2851, 4.2034, 3.0854, 0.0000, 1.1178, 1.7658, 1.4011, 2.9936, 3.6230,\n",
      "        4.1969, 3.8245, 1.2691, 2.0002, 1.1479, 1.0135, 1.1140, 1.2222, 0.9608,\n",
      "        1.2891, 1.2391, 2.6484, 3.2459, 1.1122, 4.5678, 1.0742, 2.2064, 1.2946,\n",
      "        1.0353, 3.1357, 3.0642, 3.2625, 1.4374, 3.4435, 0.9774, 1.4880, 1.0225,\n",
      "        3.6721, 3.5965, 3.9437, 1.5310, 0.8681, 3.7151, 0.8592, 3.5407, 1.3358,\n",
      "        3.0977, 4.0205, 3.0528, 1.9344, 0.6321, 1.7330, 4.0620, 1.7652, 1.4385,\n",
      "        2.4717], grad_fn=<IndexPutBackward0>)\n",
      "Loss before cap\n",
      "tensor([0.6455, 0.6534, 0.7421, 0.5596, 0.7086, 0.6753, 0.6646, 0.6633, 0.7675,\n",
      "        0.6540, 0.5381, 0.7433, 0.6200, 0.7674, 0.5939, 0.7090, 0.7728, 0.6371,\n",
      "        0.7146, 0.5273, 0.7246, 0.5935, 0.6185, 0.6269, 0.6288, 0.7007, 0.7460,\n",
      "        0.6716, 0.7153, 0.6735, 0.6603, 0.7454, 0.7184, 0.6717, 0.7092, 0.7220,\n",
      "        0.5634, 0.6460, 0.7066, 0.7952, 0.7156, 0.7262, 0.6034, 0.6617, 0.6699,\n",
      "        0.8434, 0.7675, 0.5867, 0.7408, 0.7141, 0.7764, 0.7826, 0.6576, 0.8969,\n",
      "        0.7912, 0.6503, 0.6771, 0.5897, 0.7132, 0.8245, 0.6447, 0.6379, 0.5516,\n",
      "        0.6910], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "SMOTE Labels\n",
      "tensor([1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 1., 0.], dtype=torch.float64)\n",
      "Mask\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      "Mask\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Distance\n",
      "tensor([2.2871, 1.5481, 0.8777, 2.0981, 2.2996, 2.1814, 3.2119, 3.9022, 2.0197,\n",
      "        2.1563, 2.5487, 1.0430, 2.0215, 1.7160, 2.3641, 1.1965, 1.3101, 1.4815,\n",
      "        1.0689, 2.9524, 1.1147, 1.7593, 1.8315, 1.6167, 2.2269, 1.3146, 0.9169,\n",
      "        1.5884, 1.6542, 2.4813, 2.4294, 1.4285, 1.0179, 1.9534, 1.4845, 1.7479,\n",
      "        2.2674, 2.1602, 1.5600, 1.5973, 1.5527, 1.2434, 2.5109, 1.8089, 2.4519,\n",
      "        2.6683, 2.0176, 3.2578, 1.3397, 1.0404, 2.5934, 1.3309, 1.4626, 1.5260,\n",
      "        1.4324, 3.1008, 1.9458, 3.8149, 1.5485, 1.9580, 0.0000, 2.3517, 2.8056,\n",
      "        1.9614], grad_fn=<IndexPutBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_cap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_cap\n\u001b[1;32m     28\u001b[0m loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprint_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     31\u001b[0m     _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:94\u001b[0m, in \u001b[0;36mtrain_sigmoid_with_embeddings\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat(), smote_target, embeds\u001b[38;5;241m=\u001b[39membeds)\n\u001b[1;32m     93\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 94\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-3, 5e-4, 1e-4]\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_cap = 5.0\n",
    "loss_fn_args['distance'] = 'euclidean'\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10):\n",
    "        model_aucs = []\n",
    "        network = models.ConvNetWithEmbeddings(2)\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(start_epoch):\n",
    "            loss_fn_args['loss_cap'] = None\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "        for epoch in range(start_epoch, n_epochs + 1):\n",
    "            loss_fn_args['loss_cap'] = loss_cap\n",
    "            loss_fn_args['print_loss']=True\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                model_aucs.append(auc)\n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], loss_fn_args['loss_cap'], norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd4a6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2e6fbeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0014198713898658753, AUC: 0.451065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006098311543464661, AUC: 0.847083\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006315751373767853, AUC: 0.8435989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006297500133514404, AUC: 0.844198\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017327705025672912, AUC: 0.5950120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000637341558933258, AUC: 0.8359190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006388148665428162, AUC: 0.8510049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006574186682701111, AUC: 0.853085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012659112215042114, AUC: 0.43644\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594046652317047, AUC: 0.7787320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005366420149803162, AUC: 0.8493799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005151920020580291, AUC: 0.858651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027553889751434327, AUC: 0.336005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001109073281288147, AUC: 0.7928120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005557639598846436, AUC: 0.8875319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006022202372550964, AUC: 0.888411\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014263404607772828, AUC: 0.6500199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006320845186710358, AUC: 0.8542780000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006900891065597534, AUC: 0.8110909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006846602261066436, AUC: 0.8432999999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016641780138015748, AUC: 0.44776099999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006417481899261474, AUC: 0.8287990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00066646608710289, AUC: 0.8155870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006345544755458832, AUC: 0.8390515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014404525756835937, AUC: 0.6425224999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437364220619201, AUC: 0.8231025000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006399289667606354, AUC: 0.849208\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006986283361911774, AUC: 0.8307280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010312366783618927, AUC: 0.582079\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007324255108833313, AUC: 0.859283\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007882888317108154, AUC: 0.868118\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009245118200778961, AUC: 0.859822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035424033403396605, AUC: 0.667023\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008463492095470428, AUC: 0.6891720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006510006785392761, AUC: 0.883409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010489607453346252, AUC: 0.8788579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016034974455833436, AUC: 0.640663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006083254218101501, AUC: 0.8374775000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005971337854862213, AUC: 0.838539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005949685275554656, AUC: 0.8469325000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002063215970993042, AUC: 0.36694800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007396736443042756, AUC: 0.8523210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006499601602554322, AUC: 0.8782370000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000505506083369255, AUC: 0.882285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00898159122467041, AUC: 0.3251135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005490610301494599, AUC: 0.872494\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007775983214378357, AUC: 0.860663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008666215240955352, AUC: 0.8703140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007056966125965118, AUC: 0.6303349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004791547954082489, AUC: 0.8712529999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008168165981769561, AUC: 0.862139\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012381401658058165, AUC: 0.83989\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002178061127662659, AUC: 0.688668\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005293150842189788, AUC: 0.8715550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005632279515266419, AUC: 0.8809009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005377834141254425, AUC: 0.8862920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011907556056976317, AUC: 0.5053259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008127495944499969, AUC: 0.8328549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000580844908952713, AUC: 0.8724010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004932623505592346, AUC: 0.8592310000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002042359471321106, AUC: 0.4824105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004950977861881256, AUC: 0.8882220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006151865720748902, AUC: 0.8784679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009141118228435517, AUC: 0.856539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011459320187568664, AUC: 0.5128619999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005015419721603393, AUC: 0.8941689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012670758962631226, AUC: 0.835214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008579679429531098, AUC: 0.885494\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001195651412010193, AUC: 0.543917\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007103778123855591, AUC: 0.842845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007760584950447083, AUC: 0.846993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007986690402030944, AUC: 0.8647680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016751147508621215, AUC: 0.5162939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006603367924690246, AUC: 0.8068130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006797640323638916, AUC: 0.7734930000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007222494184970856, AUC: 0.7611465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00622358751296997, AUC: 0.3957805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006196927726268768, AUC: 0.8462829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008106493949890137, AUC: 0.853784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007530889809131623, AUC: 0.870214\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007424936592578888, AUC: 0.655138\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005944547951221466, AUC: 0.8601880000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005795938670635223, AUC: 0.873889\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012109710574150087, AUC: 0.871498\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016114301085472108, AUC: 0.5586935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005855483710765838, AUC: 0.8533870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006129019260406494, AUC: 0.8494610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006279450953006744, AUC: 0.844464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010840089917182921, AUC: 0.500482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328210234642029, AUC: 0.8519900000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006721376776695252, AUC: 0.8138985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007224285006523132, AUC: 0.7606109999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010986847281455993, AUC: 0.436407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006343813240528107, AUC: 0.8488079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006782922744750976, AUC: 0.81045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006853617429733276, AUC: 0.8290789999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005651942253112793, AUC: 0.47250600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000698545902967453, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000702589213848114, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007060193121433258, AUC: 0.5005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037395615577697754, AUC: 0.35740150000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007717423737049103, AUC: 0.719531\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005390990376472473, AUC: 0.861662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007405863404273987, AUC: 0.845405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011026110053062438, AUC: 0.377658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005867840051651001, AUC: 0.8438680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006260591149330139, AUC: 0.8217395000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366168856620788, AUC: 0.8237019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003935699224472046, AUC: 0.5247170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005908787846565246, AUC: 0.8571975000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000668457418680191, AUC: 0.878807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017554450035095215, AUC: 0.8339190000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0047048511505126955, AUC: 0.5402735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006595627665519715, AUC: 0.8053465000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006501442790031434, AUC: 0.8215595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005906422734260559, AUC: 0.849113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038282995223999023, AUC: 0.609432\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047881823778152466, AUC: 0.8762059999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005948439538478852, AUC: 0.8719060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000482245534658432, AUC: 0.880326\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008005986511707306, AUC: 0.60026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000627616286277771, AUC: 0.852942\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006261325478553772, AUC: 0.856194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000577428251504898, AUC: 0.860119\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008093255460262298, AUC: 0.583607\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838938951492309, AUC: 0.863928\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005060004591941834, AUC: 0.8754770000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0007867655456066132, AUC: 0.874499\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002550248980522156, AUC: 0.3425595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006506231725215912, AUC: 0.8157204999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688809335231781, AUC: 0.7407119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000664184421300888, AUC: 0.8420155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013287771344184876, AUC: 0.40437999999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006519326567649841, AUC: 0.8055395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006656957268714905, AUC: 0.8392145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006588440239429474, AUC: 0.8623560000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008678085207939148, AUC: 0.7130485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006232837736606598, AUC: 0.8150320000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006579527854919434, AUC: 0.772242\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006889859437942505, AUC: 0.7655250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002165350556373596, AUC: 0.592628\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006192966401576996, AUC: 0.849678\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006441470086574555, AUC: 0.8424444999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006180997788906098, AUC: 0.8584069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008590065538883209, AUC: 0.538184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006923441290855408, AUC: 0.6003129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006756714284420014, AUC: 0.8060095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006255923509597779, AUC: 0.8498665000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002802884817123413, AUC: 0.588238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006434243321418763, AUC: 0.854419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006768024265766144, AUC: 0.8395885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007178871631622314, AUC: 0.8143415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011476865410804749, AUC: 0.34010700000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006510407626628876, AUC: 0.826593\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005826923549175262, AUC: 0.870284\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006089577972888947, AUC: 0.8700264999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009369367122650147, AUC: 0.629057\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006300823390483856, AUC: 0.857437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006616899073123932, AUC: 0.8484705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007092366218566894, AUC: 0.8207805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012602503895759582, AUC: 0.3094255\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005826809108257293, AUC: 0.855375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005839522182941437, AUC: 0.860249\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005868832767009735, AUC: 0.863428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006463340520858765, AUC: 0.355613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004943882673978805, AUC: 0.8716770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004916025400161743, AUC: 0.8786\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044041329622268675, AUC: 0.8917309999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003861288905143738, AUC: 0.555539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000618169367313385, AUC: 0.8260325000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000600385695695877, AUC: 0.8531995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006066961288452149, AUC: 0.8561500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021939518451690675, AUC: 0.689289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005301822423934937, AUC: 0.864827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007152001857757568, AUC: 0.8696879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005630481541156768, AUC: 0.8863179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001332865834236145, AUC: 0.47160199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006086540520191193, AUC: 0.845723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007248914837837219, AUC: 0.8395429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007225278317928314, AUC: 0.86096\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003105708122253418, AUC: 0.450519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000990282714366913, AUC: 0.8783050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000604690283536911, AUC: 0.837953\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005354639291763305, AUC: 0.8966510000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013736059069633484, AUC: 0.529721\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006109735369682312, AUC: 0.8395815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006337304711341858, AUC: 0.82777\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542203426361083, AUC: 0.8329340000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004010795831680298, AUC: 0.499224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006410621702671051, AUC: 0.8082895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006498238742351532, AUC: 0.796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006713790893554687, AUC: 0.797765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013430283069610597, AUC: 0.41947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006666264533996582, AUC: 0.805104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006859858930110931, AUC: 0.818878\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007417443990707398, AUC: 0.7116679999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008566050827503205, AUC: 0.465723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000631780743598938, AUC: 0.83158\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008227962255477905, AUC: 0.84409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00042727728188037873, AUC: 0.892396\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011288530349731445, AUC: 0.4654745\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006097352206707001, AUC: 0.83574\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006588205695152282, AUC: 0.80519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006278530061244965, AUC: 0.833839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008316256403923034, AUC: 0.615653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006282751262187958, AUC: 0.8484440000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006840136647224426, AUC: 0.872604\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006117210984230042, AUC: 0.8773779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017066192030906677, AUC: 0.481103\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004959966987371445, AUC: 0.8738189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005284667313098908, AUC: 0.880588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006083690524101257, AUC: 0.876713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001013946831226349, AUC: 0.6996389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005797637104988098, AUC: 0.872007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006616230309009552, AUC: 0.8486210000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007291432321071625, AUC: 0.7856705\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018843981623649598, AUC: 0.383455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004677594304084778, AUC: 0.876982\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006365638375282288, AUC: 0.8645519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005609051585197449, AUC: 0.8798929999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002036388039588928, AUC: 0.52929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006330843269824982, AUC: 0.8002540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006417470574378967, AUC: 0.8168764999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005766240358352662, AUC: 0.8604850000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010189328789710999, AUC: 0.396606\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006107074320316314, AUC: 0.854683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016524976491928101, AUC: 0.789723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012129902839660644, AUC: 0.8413890000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009676826298236847, AUC: 0.48654600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006424934267997742, AUC: 0.8685830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005021446794271469, AUC: 0.880862\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009814752340316772, AUC: 0.8435370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026284849643707277, AUC: 0.411123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006374098658561706, AUC: 0.8531124999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000699331134557724, AUC: 0.7287570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366600692272186, AUC: 0.8696269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002646610975265503, AUC: 0.296652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006235100328922272, AUC: 0.8257840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007332272529602051, AUC: 0.879613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013861932754516602, AUC: 0.8297760000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004123385071754456, AUC: 0.274127\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006032724380493164, AUC: 0.87715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009024456143379211, AUC: 0.838318\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006362656354904175, AUC: 0.8914160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004180976867675781, AUC: 0.318702\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005513870120048523, AUC: 0.8667579999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005225455164909363, AUC: 0.881675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008377074003219604, AUC: 0.855631\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004013432741165161, AUC: 0.381366\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006159068346023559, AUC: 0.8326155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007503931224346161, AUC: 0.870955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004752726703882217, AUC: 0.8971779999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001221836805343628, AUC: 0.502861\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00048343952000141145, AUC: 0.892434\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946669220924377, AUC: 0.8825240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00070845428109169, AUC: 0.879296\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0009617435038089753, AUC: 0.571137\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006213328838348389, AUC: 0.8300860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006127798557281494, AUC: 0.8618219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328286826610565, AUC: 0.86167\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002272887945175171, AUC: 0.56\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006930478215217591, AUC: 0.5049964999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005803787112236022, AUC: 0.8708610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006699923872947693, AUC: 0.8045645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012381322383880615, AUC: 0.522879\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006178278923034668, AUC: 0.8554980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006043855845928192, AUC: 0.868462\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007215982377529144, AUC: 0.7719394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0040275962352752685, AUC: 0.4332245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006618426740169525, AUC: 0.7646095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006699328422546387, AUC: 0.8055145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006972967684268952, AUC: 0.8090809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001835023581981659, AUC: 0.6147625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006031805276870728, AUC: 0.834449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006142882108688355, AUC: 0.834355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006176780760288238, AUC: 0.842155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004285195589065552, AUC: 0.599126\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005818487405776977, AUC: 0.862466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005499475598335266, AUC: 0.867428\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000528552383184433, AUC: 0.8774799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0033577979803085327, AUC: 0.633099\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006300189197063446, AUC: 0.8512609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000713239848613739, AUC: 0.766321\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006845631897449494, AUC: 0.8529065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002596183896064758, AUC: 0.22154749999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006192827820777893, AUC: 0.860851\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006290613114833831, AUC: 0.8721305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007155753970146179, AUC: 0.8054324999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006812658071517944, AUC: 0.4404055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006001050472259522, AUC: 0.864227\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000635467141866684, AUC: 0.8634305\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000745204120874405, AUC: 0.7401655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001915307402610779, AUC: 0.45875350000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965535581111908, AUC: 0.641465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006786578595638275, AUC: 0.8041025\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007540034651756286, AUC: 0.6870584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0020421632528305053, AUC: 0.385485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005864342451095581, AUC: 0.8577809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006117432117462158, AUC: 0.8329199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437638401985169, AUC: 0.820496\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038264979124069216, AUC: 0.48541399999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006775495111942291, AUC: 0.696378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005684538781642914, AUC: 0.8537359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007266710698604584, AUC: 0.6672289999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004798722267150879, AUC: 0.507733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005957903265953064, AUC: 0.873107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006726228296756744, AUC: 0.824431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007165955901145935, AUC: 0.8003615000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002519894242286682, AUC: 0.465378\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000610281527042389, AUC: 0.847796\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006129370331764221, AUC: 0.841613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006000222861766815, AUC: 0.8485075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017671875953674317, AUC: 0.42463049999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005308540761470795, AUC: 0.8763000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007038898468017578, AUC: 0.8720159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045277650654315947, AUC: 0.8903119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017627280354499818, AUC: 0.6297520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006543383300304413, AUC: 0.8206420000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006448293924331665, AUC: 0.8382910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007587307095527648, AUC: 0.7118745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0052219958305358884, AUC: 0.6273915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000618040293455124, AUC: 0.8216339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005898671150207519, AUC: 0.876845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007222522795200348, AUC: 0.874812\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00101617032289505, AUC: 0.362302\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005581985116004944, AUC: 0.8897725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004957733303308487, AUC: 0.8960560000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006914250254631042, AUC: 0.883623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001678795099258423, AUC: 0.306545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000676459550857544, AUC: 0.7260420000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006247651875019073, AUC: 0.8429315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006085613369941712, AUC: 0.8542139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002397128105163574, AUC: 0.39124549999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005707669258117675, AUC: 0.8724479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006699002981185913, AUC: 0.864266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006986479163169861, AUC: 0.8772280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0029314124584198, AUC: 0.59495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000630675345659256, AUC: 0.8332210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006818929612636566, AUC: 0.84173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009583902060985566, AUC: 0.8410070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027749264240264893, AUC: 0.5831415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000646762490272522, AUC: 0.8320599999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006633898615837097, AUC: 0.834303\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000747113436460495, AUC: 0.6756570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011962635517120362, AUC: 0.6538869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047616446018218994, AUC: 0.8817849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007488038241863251, AUC: 0.867336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005635308623313903, AUC: 0.8799949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01182343053817749, AUC: 0.6857319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006898700594902039, AUC: 0.591681\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006469100415706635, AUC: 0.8122425\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000733231395483017, AUC: 0.6367039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007393660545349122, AUC: 0.6295999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001100497841835022, AUC: 0.834608\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009219813942909241, AUC: 0.8697330000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008188122510910034, AUC: 0.8843859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004811126232147217, AUC: 0.507374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000670862853527069, AUC: 0.752193\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006564914584159851, AUC: 0.812299\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007597101628780365, AUC: 0.6073940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023648302555084226, AUC: 0.4334135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006265238225460053, AUC: 0.820979\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006176306307315826, AUC: 0.8471565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007318066656589508, AUC: 0.6445435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011640989780426025, AUC: 0.6228480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000872620016336441, AUC: 0.7787639999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000531396210193634, AUC: 0.87406\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010335972011089325, AUC: 0.8492069999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009764487743377686, AUC: 0.46751299999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005866183638572693, AUC: 0.865449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005844505727291107, AUC: 0.873135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006343661844730377, AUC: 0.8744689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001215645432472229, AUC: 0.5352199999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005054746270179749, AUC: 0.88104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006692035794258117, AUC: 0.863291\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009965209364891053, AUC: 0.8300509999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008414895534515381, AUC: 0.596807\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004810747653245926, AUC: 0.8637339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007146734595298767, AUC: 0.85673\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005788100361824036, AUC: 0.870335\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009259199142456055, AUC: 0.462373\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006635361313819886, AUC: 0.7601950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000671507328748703, AUC: 0.7928585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006826875209808349, AUC: 0.8245310000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.006901352167129516, AUC: 0.3253755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006336435377597809, AUC: 0.8540920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009281464219093323, AUC: 0.857488\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007411845028400422, AUC: 0.882375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.014742729663848876, AUC: 0.48548349999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005652723610401154, AUC: 0.866356\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006532040238380432, AUC: 0.8643379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006328416168689728, AUC: 0.8648080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022892054319381713, AUC: 0.44301349999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006101222336292267, AUC: 0.8428100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006621908545494079, AUC: 0.8081229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006842907965183258, AUC: 0.811017\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00236634361743927, AUC: 0.6616869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006841962933540344, AUC: 0.680189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006189373433589935, AUC: 0.8534645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006633727252483368, AUC: 0.8301719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016742716431617736, AUC: 0.407713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006015156507492065, AUC: 0.8475600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005933499336242676, AUC: 0.852578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005933180451393128, AUC: 0.8445300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0027874279022216796, AUC: 0.47290750000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006142080128192902, AUC: 0.83404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005997861325740814, AUC: 0.8462160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006140103340148926, AUC: 0.8294385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010257512331008911, AUC: 0.581189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006511121988296509, AUC: 0.8056745000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005089136064052582, AUC: 0.877551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005625608861446381, AUC: 0.885593\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009869385659694672, AUC: 0.482831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000674735814332962, AUC: 0.6824695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008090960085391999, AUC: 0.8242935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007485774457454682, AUC: 0.8653040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0037818057537078857, AUC: 0.532743\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398706436157226, AUC: 0.8381375\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006351064443588256, AUC: 0.8354985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000674860805273056, AUC: 0.748929\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001506571888923645, AUC: 0.432117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005276901721954346, AUC: 0.860028\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000514761745929718, AUC: 0.879132\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007227966487407684, AUC: 0.861992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000873210459947586, AUC: 0.527602\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005246804654598236, AUC: 0.8665569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005317696034908294, AUC: 0.8744649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000523801475763321, AUC: 0.8842970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0028122329711914064, AUC: 0.3570695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006248667538166046, AUC: 0.8480555000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006563681662082673, AUC: 0.8025740000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688620388507843, AUC: 0.690154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011007780432701112, AUC: 0.348128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005186624526977539, AUC: 0.8591799999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007851009964942932, AUC: 0.8375300000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007995855808258056, AUC: 0.7688524999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013790134191513062, AUC: 0.513944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006125788986682892, AUC: 0.8423725000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006272584199905395, AUC: 0.838135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006388641893863678, AUC: 0.8417955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002428970456123352, AUC: 0.5392175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005847545862197876, AUC: 0.845156\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005844530761241913, AUC: 0.8773680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000608259916305542, AUC: 0.8854580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014144967794418335, AUC: 0.586534\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005974901020526886, AUC: 0.862759\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006765705943107605, AUC: 0.706087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006551863253116607, AUC: 0.8072325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019578813314437868, AUC: 0.48341500000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006261669993400574, AUC: 0.8235239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006124176383018494, AUC: 0.8423170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006511924266815185, AUC: 0.8206370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013067271709442138, AUC: 0.46551750000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004907991886138916, AUC: 0.8813669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005690548717975616, AUC: 0.887894\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005934380888938904, AUC: 0.8873959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001866813600063324, AUC: 0.3841675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005579014420509338, AUC: 0.848047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005370631515979766, AUC: 0.8646119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004596537947654724, AUC: 0.86873\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00777847170829773, AUC: 0.41192199999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006355049312114716, AUC: 0.8395849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006471508145332336, AUC: 0.8316269999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006847445070743561, AUC: 0.846053\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002777116537094116, AUC: 0.4394405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005711751878261566, AUC: 0.863619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005869375169277191, AUC: 0.8840480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00045956188440322875, AUC: 0.8874630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002098830819129944, AUC: 0.454946\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006628075242042542, AUC: 0.7315585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000607627421617508, AUC: 0.8539689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008337684571743012, AUC: 0.8316800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000864566057920456, AUC: 0.54452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007302896976470947, AUC: 0.841013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005391532778739929, AUC: 0.8600220000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005919980108737945, AUC: 0.8828950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0034024417400360107, AUC: 0.38822\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006567685008049011, AUC: 0.7775395000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006209209561347962, AUC: 0.8283019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006720066368579864, AUC: 0.7625700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017327115535736085, AUC: 0.618835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005071338266134263, AUC: 0.8806949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004923931658267975, AUC: 0.8950484999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004969876259565354, AUC: 0.897377\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.006256404399871826, AUC: 0.6461270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006778033375740051, AUC: 0.6908105000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005597448945045471, AUC: 0.845894\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005612990856170654, AUC: 0.853367\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003354053258895874, AUC: 0.47554799999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006186684072017669, AUC: 0.8345960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006096853017807007, AUC: 0.8530595000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000584184467792511, AUC: 0.8545179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013535410165786744, AUC: 0.521713\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006479749977588654, AUC: 0.8225169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005418091416358948, AUC: 0.8734280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005034784674644471, AUC: 0.8837100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036949244737625123, AUC: 0.466854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006138587892055512, AUC: 0.846241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344265341758728, AUC: 0.8482415000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006310907900333405, AUC: 0.8586834999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004121228933334351, AUC: 0.68397\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000503613367676735, AUC: 0.8847039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007339088618755341, AUC: 0.8709159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008803527355194092, AUC: 0.8022225000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000825583279132843, AUC: 0.44806100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006262606382369995, AUC: 0.8202400000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000648889034986496, AUC: 0.8011185000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006618787348270416, AUC: 0.8056585000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008303980827331543, AUC: 0.352232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006414982080459595, AUC: 0.7979119999999998\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.00066445854306221, AUC: 0.73513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006505716741085052, AUC: 0.7741719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012616962790489197, AUC: 0.408616\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000685448706150055, AUC: 0.6026729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006543847918510437, AUC: 0.7943145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006940144598484039, AUC: 0.643521\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022783639430999755, AUC: 0.5137825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00061380934715271, AUC: 0.866123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395249664783478, AUC: 0.8612830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451893150806427, AUC: 0.8523419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011639239192008971, AUC: 0.605835\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006406998336315155, AUC: 0.8042934999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005896612405776977, AUC: 0.8709985000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006484760344028473, AUC: 0.8094625\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008442066013813019, AUC: 0.557078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006709703207015991, AUC: 0.7647730000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000572922021150589, AUC: 0.8591520000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005016632527112961, AUC: 0.874503\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012410663366317748, AUC: 0.61726\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005457908213138581, AUC: 0.8670600000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004677779972553253, AUC: 0.8883755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006165055036544799, AUC: 0.879572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013405963778495788, AUC: 0.313927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006661110520362854, AUC: 0.7416375000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006309871077537537, AUC: 0.8258635000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006810003221035004, AUC: 0.6999844999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001137991964817047, AUC: 0.493815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000499697983264923, AUC: 0.854479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006049731969833374, AUC: 0.868394\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004934743642807007, AUC: 0.874991\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011863898038864135, AUC: 0.49671599999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006376435160636902, AUC: 0.8263630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006036295890808106, AUC: 0.851333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398028731346131, AUC: 0.813663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002961652994155884, AUC: 0.4618165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005333426296710967, AUC: 0.868224\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004946839064359665, AUC: 0.885541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005516469180583954, AUC: 0.888147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0044007494449615474, AUC: 0.368859\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004960661232471466, AUC: 0.875958\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000550115019083023, AUC: 0.861968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000699212521314621, AUC: 0.877967\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.011874160766601562, AUC: 0.44737000000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006492611169815063, AUC: 0.8039899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006496548652648926, AUC: 0.8018260000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006149434447288514, AUC: 0.847159\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001273837149143219, AUC: 0.576034\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005425065457820892, AUC: 0.859619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008276227712631226, AUC: 0.828629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011078814268112183, AUC: 0.816886\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031458616256714, AUC: 0.610266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006198377311229706, AUC: 0.8455820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006700587272644043, AUC: 0.849089\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00064273139834404, AUC: 0.865973\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004247836828231812, AUC: 0.5508575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006337027847766876, AUC: 0.8355079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132302582263947, AUC: 0.84414\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006132360696792603, AUC: 0.8543339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002555731654167175, AUC: 0.7032830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006408571302890778, AUC: 0.8460894999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006196078062057495, AUC: 0.868874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006219792068004608, AUC: 0.859063\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017747660279273988, AUC: 0.554449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006547065079212189, AUC: 0.800801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006387041211128235, AUC: 0.8516370000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006530337333679199, AUC: 0.848361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004325419187545776, AUC: 0.664548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005436498820781708, AUC: 0.8656809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005074474513530731, AUC: 0.882382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006171563863754272, AUC: 0.8732000000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009172396957874298, AUC: 0.506922\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006567936539649963, AUC: 0.7873855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006224538385868072, AUC: 0.827244\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001146400511264801, AUC: 0.808583\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003239582657814026, AUC: 0.622926\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000574897050857544, AUC: 0.8541720000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005895549058914185, AUC: 0.874652\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047615113854408263, AUC: 0.890563\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004002004861831665, AUC: 0.39249100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006638368666172028, AUC: 0.8052790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006286077499389649, AUC: 0.8472430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006791818737983704, AUC: 0.8121910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009913158416748047, AUC: 0.560173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005441051423549652, AUC: 0.8343710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007384428679943084, AUC: 0.8354759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004968568831682205, AUC: 0.888386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004691952466964721, AUC: 0.5234005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006466312110424042, AUC: 0.8161769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006267159581184388, AUC: 0.8388625000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006169022023677826, AUC: 0.8436920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014778287410736083, AUC: 0.358333\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006217733323574066, AUC: 0.8301220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006214540004730225, AUC: 0.8424010000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006195749342441559, AUC: 0.8443505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002436873197555542, AUC: 0.586218\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006357889771461487, AUC: 0.8115100000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006503265500068664, AUC: 0.8125095\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006591700315475464, AUC: 0.8201854999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012875214219093324, AUC: 0.325265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006022137999534607, AUC: 0.840729\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006470130085945129, AUC: 0.8328999999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006619875133037568, AUC: 0.8326079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009733951091766358, AUC: 0.314918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006652328670024871, AUC: 0.7856584999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006489710807800293, AUC: 0.831407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896189749240876, AUC: 0.7636685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001343103587627411, AUC: 0.401076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623954176902771, AUC: 0.8280604999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006115191578865051, AUC: 0.832221\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006117686927318573, AUC: 0.8352769999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007159433662891388, AUC: 0.707108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640302062034607, AUC: 0.831024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006325231194496155, AUC: 0.8418160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427593231201172, AUC: 0.858336\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004285976648330688, AUC: 0.286546\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004912228286266327, AUC: 0.8685839999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006241131126880645, AUC: 0.8692079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004772546738386154, AUC: 0.8843530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000701574295759201, AUC: 0.62507\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006505493819713592, AUC: 0.7930475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006641898155212402, AUC: 0.7837565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006530954539775848, AUC: 0.82228\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011403043270111085, AUC: 0.532626\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005216955840587616, AUC: 0.863009\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005125235319137574, AUC: 0.878531\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005967167019844056, AUC: 0.8730070000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010634523630142212, AUC: 0.592661\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005962080955505371, AUC: 0.8758800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010628481507301332, AUC: 0.8328880000000001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0012523153424263001, AUC: 0.814502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014955583214759827, AUC: 0.36607900000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005599774718284607, AUC: 0.8701439999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00043915772438049317, AUC: 0.88686\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009326309263706208, AUC: 0.86063\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0038191169500350953, AUC: 0.25574800000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006191120445728302, AUC: 0.840455\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006785275638103485, AUC: 0.703173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006376660168170929, AUC: 0.8411375000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001254319190979004, AUC: 0.39099950000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006377569735050202, AUC: 0.836465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006618456542491913, AUC: 0.7989205000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006024990379810333, AUC: 0.8675094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002687322735786438, AUC: 0.598554\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949887573719025, AUC: 0.8521830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000565591037273407, AUC: 0.870403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006767203807830811, AUC: 0.870911\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004042961955070495, AUC: 0.43826800000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006169524788856507, AUC: 0.8392640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004900486171245575, AUC: 0.893439\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00044291891157627105, AUC: 0.888725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004621175050735474, AUC: 0.542117\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006141937375068664, AUC: 0.853417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005118619501590729, AUC: 0.868349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006588236093521119, AUC: 0.876609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008287407159805298, AUC: 0.5834159999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006370896995067597, AUC: 0.8465900000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006538202166557312, AUC: 0.8375945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006767919361591339, AUC: 0.8291439999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022407302856445313, AUC: 0.40454900000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005937451720237732, AUC: 0.8609870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000592680811882019, AUC: 0.8566165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005911863148212432, AUC: 0.8579690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005357229232788086, AUC: 0.6364449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000630250632762909, AUC: 0.856477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005860344469547272, AUC: 0.8810019999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007921697497367859, AUC: 0.8666609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004360336303710938, AUC: 0.442102\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006618334949016571, AUC: 0.7907535000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006577974557876587, AUC: 0.8204130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883169114589691, AUC: 0.755675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005332017421722412, AUC: 0.594926\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006364358067512512, AUC: 0.8138169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006281273365020751, AUC: 0.8299770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366107761859894, AUC: 0.8316445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009557693302631378, AUC: 0.473186\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00047769390046596526, AUC: 0.8800204999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007535164356231689, AUC: 0.864937\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005158395171165467, AUC: 0.8847349999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002348461985588074, AUC: 0.538466\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006082327365875244, AUC: 0.8201240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006038172245025634, AUC: 0.8278750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005976865589618683, AUC: 0.835178\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005934151649475097, AUC: 0.2786505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006538701653480529, AUC: 0.7786765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000658840149641037, AUC: 0.7461055\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000805882602930069, AUC: 0.8777810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0043359074592590335, AUC: 0.385864\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006151879131793976, AUC: 0.8514889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006059941649436951, AUC: 0.844538\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006061725914478301, AUC: 0.8368100000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015659443736076356, AUC: 0.703086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005913686156272888, AUC: 0.8599049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005783863067626953, AUC: 0.8859689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005719656050205231, AUC: 0.890785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002506427764892578, AUC: 0.48638950000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006257885098457336, AUC: 0.854067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007565390765666962, AUC: 0.857934\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007484782040119171, AUC: 0.8705620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004225674867630005, AUC: 0.6881600000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006701560616493225, AUC: 0.7515695\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005365877151489258, AUC: 0.8600720000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006056832075119018, AUC: 0.864627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.009426028251647948, AUC: 0.7086210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006203167736530304, AUC: 0.8375140000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006046193242073059, AUC: 0.8515820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006455918848514557, AUC: 0.8126020000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005052299499511719, AUC: 0.40213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000680808573961258, AUC: 0.6574334999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006337172091007233, AUC: 0.8011870000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006354606449604035, AUC: 0.7855365000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0052195687294006345, AUC: 0.32587\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006974664330482483, AUC: 0.504497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006772365868091583, AUC: 0.700526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006748861372470855, AUC: 0.742706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008752115666866302, AUC: 0.506403\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005877090692520141, AUC: 0.86571\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006457892954349518, AUC: 0.8355044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006740934848785401, AUC: 0.8011020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008829686343669891, AUC: 0.470302\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000644108384847641, AUC: 0.8156690000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006369166374206543, AUC: 0.8263480000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000631668746471405, AUC: 0.8393544999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0058558895587921145, AUC: 0.483769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006484177112579346, AUC: 0.8050515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006434639394283294, AUC: 0.827409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006483155488967896, AUC: 0.8382040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001149299144744873, AUC: 0.301971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926962435245514, AUC: 0.561572\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000654712051153183, AUC: 0.7750900000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006573466658592224, AUC: 0.7866155000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024540804624557495, AUC: 0.599528\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005305717289447784, AUC: 0.843741\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006247161626815796, AUC: 0.8536980000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008755415380001068, AUC: 0.855211\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010261299312114716, AUC: 0.7043550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006450145542621613, AUC: 0.8381330000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008315579295158387, AUC: 0.856706\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006073980927467347, AUC: 0.8768960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.002747654914855957, AUC: 0.7049699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006011591553688049, AUC: 0.829388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007644605934619904, AUC: 0.8694080000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00046514658629894255, AUC: 0.8861269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012596235275268554, AUC: 0.377997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006185989975929261, AUC: 0.8424050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006070300340652465, AUC: 0.8619430000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006188713312149048, AUC: 0.8645365000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009452844262123108, AUC: 0.43927\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004900210201740265, AUC: 0.873636\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000573208749294281, AUC: 0.8794939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005247798264026641, AUC: 0.884784\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001034870684146881, AUC: 0.42770899999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006506774127483368, AUC: 0.8097799999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006285885870456696, AUC: 0.804595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005903526544570923, AUC: 0.842169\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010283302068710327, AUC: 0.45239599999999996\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006242739856243133, AUC: 0.8442550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006362875998020172, AUC: 0.8505915\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006303291320800781, AUC: 0.8577115000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00284311842918396, AUC: 0.5474640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006009430587291717, AUC: 0.869788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005907527208328247, AUC: 0.878839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008851970732212067, AUC: 0.8664289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.010909755229949952, AUC: 0.4244395000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000616276741027832, AUC: 0.8608725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000679631233215332, AUC: 0.7200700000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395933926105499, AUC: 0.8316265000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016303920745849609, AUC: 0.6265099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005951545834541321, AUC: 0.8453910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006282015144824982, AUC: 0.8307685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006714610457420349, AUC: 0.7932209999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011628427505493164, AUC: 0.434372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006715289354324341, AUC: 0.7347895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006779305934906006, AUC: 0.6817409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006636807918548584, AUC: 0.748877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004581897735595703, AUC: 0.36681450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005067306011915207, AUC: 0.8641859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000494600921869278, AUC: 0.876877\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005797322988510132, AUC: 0.8868020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00202478814125061, AUC: 0.41139349999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005662064850330353, AUC: 0.8339449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004653812199831009, AUC: 0.8838130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005283179879188538, AUC: 0.884129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007687472999095917, AUC: 0.495647\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000722708910703659, AUC: 0.853989\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004761858582496643, AUC: 0.87388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005158531665802002, AUC: 0.884495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0031012879610061647, AUC: 0.47502750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005672177374362946, AUC: 0.8062039999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006707825660705566, AUC: 0.824119\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005006458908319474, AUC: 0.882811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cosine distance + capped loss using whole class average tensor\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4]\n",
    "\n",
    "\n",
    "cap_aucs=[]\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_caps = [2]\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(100):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                loss_fn_args['avg_tensors'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False,loss_fn=loss_fns.CappedBCELossAvgDistance,loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                loss_fn_args['print_loss']= False\n",
    "                loss_fn_args['avg_tensors'] = []\n",
    "                for k in range(2):\n",
    "                    _, avg_tensor = network(avg_tensors_list[k])\n",
    "                    loss_fn_args['avg_tensors'].append(avg_tensor)\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELossAvgDistance, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote_avg\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, 'num_models=100']\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6f79075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6be2c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.002829554080963135, AUC: 0.5453365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006344849169254303, AUC: 0.7285795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006355868875980378, AUC: 0.7295389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000714277982711792, AUC: 0.645087\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012406718730926514, AUC: 0.48612500000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006413953006267547, AUC: 0.8049814999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007251677215099335, AUC: 0.79609\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000619562715291977, AUC: 0.8308659999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017167515754699708, AUC: 0.49402\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000710596114397049, AUC: 0.8133830000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010991505384445191, AUC: 0.7876580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005079457610845566, AUC: 0.874458\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012403169274330138, AUC: 0.6484135\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006479097008705139, AUC: 0.805772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006393750607967376, AUC: 0.821047\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006755166947841645, AUC: 0.775114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004949046850204468, AUC: 0.562738\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005993484556674957, AUC: 0.849537\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005410893559455872, AUC: 0.860447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005194709599018097, AUC: 0.8635809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.01089901113510132, AUC: 0.4895084999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006285934448242188, AUC: 0.8130220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006131206452846527, AUC: 0.8243124999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006175169348716736, AUC: 0.8338905000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012591203451156616, AUC: 0.39273199999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007001586258411407, AUC: 0.600365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005619890093803406, AUC: 0.843505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006385557055473328, AUC: 0.855353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0024405421018600464, AUC: 0.384619\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004953590333461762, AUC: 0.861898\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004903338700532914, AUC: 0.8654750000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008190570473670959, AUC: 0.83994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004177627086639405, AUC: 0.6354789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007651813626289367, AUC: 0.7795880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005815426409244537, AUC: 0.8684459999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004717462360858917, AUC: 0.8784519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036474497318267824, AUC: 0.7124280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005356723964214325, AUC: 0.83145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005251560211181641, AUC: 0.8528350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009840574562549591, AUC: 0.650412\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0026831005811691283, AUC: 0.49017299999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006416706144809723, AUC: 0.7737879999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006481346189975739, AUC: 0.7925724999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007445996403694152, AUC: 0.6059164999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008776916861534119, AUC: 0.5742590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006801342368125916, AUC: 0.7469589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006423808634281158, AUC: 0.8321694999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007653397619724274, AUC: 0.6295840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011589698791503907, AUC: 0.499863\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007208704352378846, AUC: 0.781718\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006554763317108154, AUC: 0.850171\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001320421040058136, AUC: 0.7525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007064866423606873, AUC: 0.634903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005779922306537628, AUC: 0.8473885000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007513488233089447, AUC: 0.847173\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007431649267673492, AUC: 0.844786\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009028397798538209, AUC: 0.496194\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005595059990882873, AUC: 0.8573049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004818124324083328, AUC: 0.865405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007778981029987335, AUC: 0.856287\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0021117732524871826, AUC: 0.482858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005125536769628524, AUC: 0.855401\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006758202016353607, AUC: 0.865858\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009052623808383941, AUC: 0.799526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.007654309272766113, AUC: 0.586404\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010818291306495665, AUC: 0.799662\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007405320405960083, AUC: 0.839348\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006496667861938477, AUC: 0.854903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013112865090370179, AUC: 0.514462\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006349096298217773, AUC: 0.8202685000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006649299561977387, AUC: 0.813462\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006499722599983216, AUC: 0.834532\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003148359775543213, AUC: 0.3840115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010422536730766297, AUC: 0.7701565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000647133618593216, AUC: 0.86712\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007921359539031982, AUC: 0.866072\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004497889280319214, AUC: 0.6244620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006678021848201752, AUC: 0.780161\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000704984575510025, AUC: 0.7308785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007247519791126251, AUC: 0.7735765\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0025632842779159544, AUC: 0.537447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005828485190868378, AUC: 0.8003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005502667725086212, AUC: 0.823008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005372520983219146, AUC: 0.83014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035398812294006347, AUC: 0.661874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005971636772155762, AUC: 0.8012030000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007926940023899079, AUC: 0.7901929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006528653502464295, AUC: 0.7899120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009828465282917024, AUC: 0.6350469999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006427830159664154, AUC: 0.8076684999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006345461308956146, AUC: 0.8291649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006195044219493866, AUC: 0.8435610000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015288962125778198, AUC: 0.524389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006576606333255768, AUC: 0.81854\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006482686102390289, AUC: 0.82836\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000595355749130249, AUC: 0.839407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007615720331668853, AUC: 0.574935\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006770481765270233, AUC: 0.6933595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006550976037979126, AUC: 0.7725295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000677433580160141, AUC: 0.6928194999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023440412282943724, AUC: 0.42169300000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006322152018547058, AUC: 0.8094325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006268071234226227, AUC: 0.8172219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006237948834896087, AUC: 0.8219365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010862239003181456, AUC: 0.448184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005460285246372223, AUC: 0.8255880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005318101644515991, AUC: 0.8500620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006762182414531707, AUC: 0.8477889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.005236338376998901, AUC: 0.45715849999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005851194858551025, AUC: 0.812634\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005436033308506012, AUC: 0.837241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005540345609188079, AUC: 0.8432390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0040945556163787845, AUC: 0.3604485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000626286119222641, AUC: 0.7412989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006604903936386109, AUC: 0.787928\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005480704605579376, AUC: 0.8110899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.004352675199508667, AUC: 0.5948775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000597592830657959, AUC: 0.7691439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005772238075733185, AUC: 0.8032360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005506550669670105, AUC: 0.818681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cosine distance + capped loss using whole class average embedding (not image)\n",
    "momentum=0\n",
    "learning_rates = [5e-4, 1e-3, 1e-4]\n",
    "\n",
    "\n",
    "cap_aucs=[]\n",
    "\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_caps = [0.5]\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                loss_fn_args['avg_tensors'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False,loss_fn=loss_fns.CappedBCELossAvgDistance,loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                loss_fn_args['print_loss']= False\n",
    "                loss_fn_args['avg_tensors'] = []\n",
    "                for k in range(2):\n",
    "                    _, class_tensor = network(class_img_list[k].float())\n",
    "                    avg_tensor = torch.mean(class_tensor, 0) \n",
    "                    loss_fn_args['avg_tensors'].append(avg_tensor)\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELossAvgDistance, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote_avg_embed\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, None]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32aa9999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "639f921d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008514704704284668, AUC: 0.568794\n",
      "\n",
      "Number capped: 6\n",
      "Average cap: 4.715336322784424\n",
      "Number capped: 22\n",
      "Average cap: 1.4334052801132202\n",
      "Number capped: 19\n",
      "Average cap: 2.146430253982544\n",
      "Number capped: 24\n",
      "Average cap: 1.757418155670166\n",
      "Number capped: 17\n",
      "Average cap: 2.1666197776794434\n",
      "Number capped: 15\n",
      "Average cap: 3.7003581523895264\n",
      "Number capped: 19\n",
      "Average cap: 3.400217056274414\n",
      "Number capped: 18\n",
      "Average cap: 2.61631441116333\n",
      "Number capped: 19\n",
      "Average cap: 2.364832878112793\n",
      "Number capped: 18\n",
      "Average cap: 2.6223745346069336\n",
      "Number capped: 21\n",
      "Average cap: 2.396857500076294\n",
      "Number capped: 18\n",
      "Average cap: 2.3579938411712646\n",
      "Number capped: 16\n",
      "Average cap: 2.3487868309020996\n",
      "Number capped: 17\n",
      "Average cap: 2.5006766319274902\n",
      "Number capped: 19\n",
      "Average cap: 2.2046806812286377\n",
      "Number capped: 17\n",
      "Average cap: 2.9541823863983154\n",
      "Number capped: 18\n",
      "Average cap: 1.3267803192138672\n",
      "Number capped: 11\n",
      "Average cap: 4.210212230682373\n",
      "Number capped: 12\n",
      "Average cap: 2.6164281368255615\n",
      "Number capped: 7\n",
      "Average cap: 5.3376240730285645\n",
      "Number capped: 24\n",
      "Average cap: 1.959067940711975\n",
      "Number capped: 21\n",
      "Average cap: 1.8828145265579224\n",
      "Number capped: 16\n",
      "Average cap: 3.1323394775390625\n",
      "Number capped: 20\n",
      "Average cap: 2.0959765911102295\n",
      "Number capped: 19\n",
      "Average cap: 3.191474199295044\n",
      "Number capped: 18\n",
      "Average cap: 2.073922872543335\n",
      "Number capped: 24\n",
      "Average cap: 1.773211121559143\n",
      "Number capped: 12\n",
      "Average cap: 3.815920114517212\n",
      "Number capped: 19\n",
      "Average cap: 2.2017674446105957\n",
      "Number capped: 12\n",
      "Average cap: 6.636145114898682\n",
      "Number capped: 23\n",
      "Average cap: 2.007545232772827\n",
      "Number capped: 17\n",
      "Average cap: 2.439218044281006\n",
      "Number capped: 17\n",
      "Average cap: 2.3845691680908203\n",
      "Number capped: 7\n",
      "Average cap: 5.661396503448486\n",
      "Number capped: 15\n",
      "Average cap: 4.037581920623779\n",
      "Number capped: 18\n",
      "Average cap: 0.5603836178779602\n",
      "Number capped: 13\n",
      "Average cap: 2.79152774810791\n",
      "Number capped: 15\n",
      "Average cap: 0.888297438621521\n",
      "Number capped: 10\n",
      "Average cap: 1.666174292564392\n",
      "Number capped: 17\n",
      "Average cap: 11.303014755249023\n",
      "Number capped: 16\n",
      "Average cap: 2.7209787368774414\n",
      "Number capped: 19\n",
      "Average cap: 2.29293155670166\n",
      "Number capped: 15\n",
      "Average cap: 1.478337049484253\n",
      "Number capped: 15\n",
      "Average cap: 2.8587567806243896\n",
      "Number capped: 18\n",
      "Average cap: 2.500164031982422\n",
      "Number capped: 18\n",
      "Average cap: 2.4000353813171387\n",
      "Number capped: 20\n",
      "Average cap: 2.279146671295166\n",
      "Number capped: 13\n",
      "Average cap: 3.244873046875\n",
      "Number capped: 20\n",
      "Average cap: 1.7539411783218384\n",
      "Number capped: 15\n",
      "Average cap: 2.334641218185425\n",
      "Number capped: 12\n",
      "Average cap: 2.6974918842315674\n",
      "Number capped: 19\n",
      "Average cap: 2.6156113147735596\n",
      "Number capped: 15\n",
      "Average cap: 7.120849609375\n",
      "Number capped: 12\n",
      "Average cap: 3.240861177444458\n",
      "Number capped: 6\n",
      "Average cap: 7.502542018890381\n",
      "Number capped: 6\n",
      "Average cap: 4.574895858764648\n",
      "Number capped: 9\n",
      "Average cap: 4.5227203369140625\n",
      "Number capped: 5\n",
      "Average cap: 9.776338577270508\n",
      "Number capped: 16\n",
      "Average cap: 2.958550453186035\n",
      "Number capped: 27\n",
      "Average cap: 1.0421700477600098\n",
      "Number capped: 20\n",
      "Average cap: 1.675140380859375\n",
      "Number capped: 20\n",
      "Average cap: 2.451796531677246\n",
      "Number capped: 25\n",
      "Average cap: 1.8006658554077148\n",
      "Number capped: 17\n",
      "Average cap: 2.54384446144104\n",
      "Number capped: 16\n",
      "Average cap: 0.7226827144622803\n",
      "Number capped: 15\n",
      "Average cap: 3.2057619094848633\n",
      "Number capped: 18\n",
      "Average cap: 2.4071686267852783\n",
      "Number capped: 10\n",
      "Average cap: 3.951281785964966\n",
      "Number capped: 13\n",
      "Average cap: 2.8657069206237793\n",
      "Number capped: 16\n",
      "Average cap: 2.46968936920166\n",
      "Number capped: 20\n",
      "Average cap: 2.313533306121826\n",
      "Number capped: 10\n",
      "Average cap: 4.222248077392578\n",
      "Number capped: 25\n",
      "Average cap: 1.8374927043914795\n",
      "Number capped: 13\n",
      "Average cap: 1.5761983394622803\n",
      "Number capped: 22\n",
      "Average cap: 1.745504379272461\n",
      "Number capped: 13\n",
      "Average cap: 5.00754451751709\n",
      "Number capped: 10\n",
      "Average cap: 3.327174425125122\n",
      "Number capped: 16\n",
      "Average cap: 2.1710712909698486\n",
      "Number capped: 18\n",
      "Average cap: 2.2122395038604736\n",
      "Number capped: 18\n",
      "Average cap: 2.8624820709228516\n",
      "Number capped: 19\n",
      "Average cap: 1.814214825630188\n",
      "Number capped: 15\n",
      "Average cap: 3.102687358856201\n",
      "Number capped: 17\n",
      "Average cap: 1.7807286977767944\n",
      "Number capped: 16\n",
      "Average cap: 2.2958245277404785\n",
      "Number capped: 22\n",
      "Average cap: 1.14052152633667\n",
      "Number capped: 18\n",
      "Average cap: 2.3460137844085693\n",
      "Number capped: 11\n",
      "Average cap: 3.992056369781494\n",
      "Number capped: 16\n",
      "Average cap: 2.9139366149902344\n",
      "Number capped: 13\n",
      "Average cap: 3.3902640342712402\n",
      "Number capped: 14\n",
      "Average cap: 3.0545713901519775\n",
      "Number capped: 19\n",
      "Average cap: 2.1602134704589844\n",
      "Number capped: 22\n",
      "Average cap: 1.9770251512527466\n",
      "Number capped: 20\n",
      "Average cap: 2.423428773880005\n",
      "Number capped: 25\n",
      "Average cap: 1.857977032661438\n",
      "Number capped: 15\n",
      "Average cap: 1.0188260078430176\n",
      "Number capped: 15\n",
      "Average cap: 3.0733635425567627\n",
      "Number capped: 12\n",
      "Average cap: 3.6452176570892334\n",
      "Number capped: 15\n",
      "Average cap: 3.0806429386138916\n",
      "Number capped: 20\n",
      "Average cap: 0.7720105648040771\n",
      "Number capped: 8\n",
      "Average cap: 4.929309368133545\n",
      "Number capped: 12\n",
      "Average cap: 1.4574331045150757\n",
      "Number capped: 12\n",
      "Average cap: 2.783367395401001\n",
      "Number capped: 23\n",
      "Average cap: 1.9288432598114014\n",
      "Number capped: 20\n",
      "Average cap: 2.3275160789489746\n",
      "Number capped: 18\n",
      "Average cap: 2.6637814044952393\n",
      "Number capped: 21\n",
      "Average cap: 2.225221872329712\n",
      "Number capped: 19\n",
      "Average cap: 0.7875173687934875\n",
      "Number capped: 16\n",
      "Average cap: 2.552135705947876\n",
      "Number capped: 20\n",
      "Average cap: 2.2024502754211426\n",
      "Number capped: 15\n",
      "Average cap: 3.479158639907837\n",
      "Number capped: 15\n",
      "Average cap: 2.8158648014068604\n",
      "Number capped: 19\n",
      "Average cap: 0.8310911059379578\n",
      "Number capped: 19\n",
      "Average cap: 2.1850779056549072\n",
      "Number capped: 18\n",
      "Average cap: 2.500674247741699\n",
      "Number capped: 15\n",
      "Average cap: 3.112748622894287\n",
      "Number capped: 19\n",
      "Average cap: 2.1038737297058105\n",
      "Number capped: 20\n",
      "Average cap: 1.8821302652359009\n",
      "Number capped: 16\n",
      "Average cap: 3.1269631385803223\n",
      "Number capped: 12\n",
      "Average cap: 3.9341518878936768\n",
      "Number capped: 19\n",
      "Average cap: 2.1810271739959717\n",
      "Number capped: 16\n",
      "Average cap: 2.5133206844329834\n",
      "Number capped: 14\n",
      "Average cap: 2.54417085647583\n",
      "Number capped: 12\n",
      "Average cap: 3.523258924484253\n",
      "Number capped: 18\n",
      "Average cap: 2.618112802505493\n",
      "Number capped: 18\n",
      "Average cap: 2.5364601612091064\n",
      "Number capped: 18\n",
      "Average cap: 2.1785402297973633\n",
      "Number capped: 16\n",
      "Average cap: 2.3932788372039795\n",
      "Number capped: 7\n",
      "Average cap: 5.684166431427002\n",
      "Number capped: 4\n",
      "Average cap: 0.19036860764026642\n",
      "Number capped: 23\n",
      "Average cap: 1.1212753057479858\n",
      "Number capped: 23\n",
      "Average cap: 1.987658977508545\n",
      "Number capped: 15\n",
      "Average cap: 2.889791250228882\n",
      "Number capped: 22\n",
      "Average cap: 1.8296939134597778\n",
      "Number capped: 17\n",
      "Average cap: 2.49611234664917\n",
      "Number capped: 24\n",
      "Average cap: 2.5309603214263916\n",
      "Number capped: 15\n",
      "Average cap: 3.0127017498016357\n",
      "Number capped: 21\n",
      "Average cap: 1.5631364583969116\n",
      "Number capped: 20\n",
      "Average cap: 0.6874469518661499\n",
      "Number capped: 21\n",
      "Average cap: 0.40501368045806885\n",
      "Number capped: 13\n",
      "Average cap: 2.620543956756592\n",
      "Number capped: 18\n",
      "Average cap: 3.166642427444458\n",
      "Number capped: 17\n",
      "Average cap: 2.0593509674072266\n",
      "Number capped: 14\n",
      "Average cap: 3.2145092487335205\n",
      "Number capped: 22\n",
      "Average cap: 2.084355115890503\n",
      "Number capped: 18\n",
      "Average cap: 3.7465341091156006\n",
      "Number capped: 17\n",
      "Average cap: 2.6647303104400635\n",
      "Number capped: 17\n",
      "Average cap: 2.513749122619629\n",
      "Number capped: 14\n",
      "Average cap: 3.3663361072540283\n",
      "Number capped: 12\n",
      "Average cap: 3.4901952743530273\n",
      "Number capped: 17\n",
      "Average cap: 1.5189180374145508\n",
      "Number capped: 17\n",
      "Average cap: 2.5075647830963135\n",
      "Number capped: 17\n",
      "Average cap: 2.3492679595947266\n",
      "Number capped: 20\n",
      "Average cap: 2.295314311981201\n",
      "Number capped: 13\n",
      "Average cap: 0.45455554127693176\n",
      "Number capped: 22\n",
      "Average cap: 1.7265368700027466\n",
      "Number capped: 21\n",
      "Average cap: 2.137301206588745\n",
      "Number capped: 3\n",
      "Average cap: 3.181353807449341\n",
      "Number capped: 16\n",
      "Average cap: 4.039509296417236\n",
      "Number capped: 18\n",
      "Average cap: 1.5949326753616333\n",
      "Number capped: 26\n",
      "Average cap: 1.6156038045883179\n",
      "Number capped: 15\n",
      "Average cap: 2.8113489151000977\n",
      "Number capped: 17\n",
      "Average cap: 2.837789297103882\n",
      "Number capped: 17\n",
      "Average cap: 2.4574387073516846\n",
      "Number capped: 18\n",
      "Average cap: 2.737002372741699\n",
      "Number capped: 24\n",
      "Average cap: 1.947830319404602\n",
      "Number capped: 17\n",
      "Average cap: 2.3735220432281494\n",
      "Number capped: 17\n",
      "Average cap: 2.4378113746643066\n",
      "Number capped: 22\n",
      "Average cap: 0.9953410625457764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 13\n",
      "Average cap: 3.514599084854126\n",
      "Number capped: 15\n",
      "Average cap: 2.800839424133301\n",
      "Number capped: 21\n",
      "Average cap: 0.277582049369812\n",
      "Number capped: 16\n",
      "Average cap: 2.8392889499664307\n",
      "Number capped: 18\n",
      "Average cap: 2.323920249938965\n",
      "Number capped: 19\n",
      "Average cap: 2.31716251373291\n",
      "Number capped: 13\n",
      "Average cap: 4.206882953643799\n",
      "Number capped: 12\n",
      "Average cap: 4.252286434173584\n",
      "Number capped: 21\n",
      "Average cap: 2.286985397338867\n",
      "Number capped: 23\n",
      "Average cap: 1.9151296615600586\n",
      "Number capped: 19\n",
      "Average cap: 2.0492944717407227\n",
      "Number capped: 9\n",
      "Average cap: 6.1090240478515625\n",
      "Number capped: 13\n",
      "Average cap: 3.0137109756469727\n",
      "Number capped: 18\n",
      "Average cap: 2.6573050022125244\n",
      "Number capped: 15\n",
      "Average cap: 3.064845323562622\n",
      "Number capped: 19\n",
      "Average cap: 2.287706136703491\n",
      "Number capped: 22\n",
      "Average cap: 1.7647722959518433\n",
      "Number capped: 20\n",
      "Average cap: 2.360644578933716\n",
      "Number capped: 12\n",
      "Average cap: 1.2185999155044556\n",
      "Number capped: 19\n",
      "Average cap: 2.0450687408447266\n",
      "Number capped: 12\n",
      "Average cap: 3.718838930130005\n",
      "Number capped: 12\n",
      "Average cap: 3.2121245861053467\n",
      "Number capped: 8\n",
      "Average cap: 10.290724754333496\n",
      "Number capped: 14\n",
      "Average cap: 2.53108286857605\n",
      "Number capped: 10\n",
      "Average cap: 3.101257562637329\n",
      "Number capped: 13\n",
      "Average cap: 4.2663655281066895\n",
      "Number capped: 12\n",
      "Average cap: 3.320483446121216\n",
      "Number capped: 11\n",
      "Average cap: 3.4672272205352783\n",
      "Number capped: 3\n",
      "Average cap: 7.902825832366943\n",
      "Number capped: 14\n",
      "Average cap: 2.579867124557495\n",
      "Number capped: 16\n",
      "Average cap: 2.3354787826538086\n",
      "Number capped: 18\n",
      "Average cap: 1.2668383121490479\n",
      "Number capped: 21\n",
      "Average cap: 1.8984202146530151\n",
      "Number capped: 16\n",
      "Average cap: 0.7305521368980408\n",
      "Number capped: 3\n",
      "Average cap: 2.7863171100616455\n",
      "Number capped: 22\n",
      "Average cap: 2.8439042568206787\n",
      "Number capped: 14\n",
      "Average cap: 2.990867853164673\n",
      "Number capped: 19\n",
      "Average cap: 2.418541431427002\n",
      "Number capped: 17\n",
      "Average cap: 2.054246425628662\n",
      "Number capped: 11\n",
      "Average cap: 3.6745362281799316\n",
      "Number capped: 24\n",
      "Average cap: 1.7743829488754272\n",
      "Number capped: 17\n",
      "Average cap: 2.50129771232605\n",
      "Number capped: 24\n",
      "Average cap: 1.7647854089736938\n",
      "Number capped: 20\n",
      "Average cap: 1.7845985889434814\n",
      "Number capped: 20\n",
      "Average cap: 2.1901564598083496\n",
      "Number capped: 10\n",
      "Average cap: 4.223986625671387\n",
      "Number capped: 13\n",
      "Average cap: 0.360208660364151\n",
      "Number capped: 23\n",
      "Average cap: 1.2333037853240967\n",
      "Number capped: 13\n",
      "Average cap: 4.357958793640137\n",
      "Number capped: 19\n",
      "Average cap: 1.5415502786636353\n",
      "Number capped: 23\n",
      "Average cap: 1.1197335720062256\n",
      "Number capped: 7\n",
      "Average cap: 3.05550217628479\n",
      "Number capped: 9\n",
      "Average cap: 5.236572265625\n",
      "Number capped: 16\n",
      "Average cap: 1.7077958583831787\n",
      "Number capped: 8\n",
      "Average cap: 5.2162065505981445\n",
      "Number capped: 15\n",
      "Average cap: 2.1063413619995117\n",
      "Number capped: 19\n",
      "Average cap: 2.4521842002868652\n",
      "Number capped: 7\n",
      "Average cap: 4.177753448486328\n",
      "Number capped: 11\n",
      "Average cap: 3.979304313659668\n",
      "Number capped: 17\n",
      "Average cap: 1.9604723453521729\n",
      "Number capped: 19\n",
      "Average cap: 2.1808664798736572\n",
      "Number capped: 17\n",
      "Average cap: 2.752476930618286\n",
      "Number capped: 15\n",
      "Average cap: 2.195965051651001\n",
      "Number capped: 12\n",
      "Average cap: 3.7078113555908203\n",
      "Number capped: 9\n",
      "Average cap: 4.398186683654785\n",
      "Number capped: 10\n",
      "Average cap: 4.203692436218262\n",
      "Number capped: 14\n",
      "Average cap: 2.5862600803375244\n",
      "Number capped: 14\n",
      "Average cap: 3.001147985458374\n",
      "Number capped: 11\n",
      "Average cap: 1.4886401891708374\n",
      "Number capped: 8\n",
      "Average cap: 0.8304838538169861\n",
      "Number capped: 16\n",
      "Average cap: 2.3514022827148438\n",
      "Number capped: 8\n",
      "Average cap: 5.930342197418213\n",
      "Number capped: 11\n",
      "Average cap: 3.761669874191284\n",
      "Number capped: 9\n",
      "Average cap: 4.7033586502075195\n",
      "Number capped: 14\n",
      "Average cap: 2.822718381881714\n",
      "Number capped: 7\n",
      "Average cap: 6.489527225494385\n",
      "Number capped: 15\n",
      "Average cap: 2.7647624015808105\n",
      "Number capped: 13\n",
      "Average cap: 4.023625373840332\n",
      "Number capped: 21\n",
      "Average cap: 1.5518053770065308\n",
      "Number capped: 15\n",
      "Average cap: 2.8807451725006104\n",
      "Number capped: 16\n",
      "Average cap: 0.6656309366226196\n",
      "Number capped: 7\n",
      "Average cap: 6.820866584777832\n",
      "Number capped: 13\n",
      "Average cap: 3.4895286560058594\n",
      "Number capped: 13\n",
      "Average cap: 2.6817030906677246\n",
      "Number capped: 8\n",
      "Average cap: 2.180752992630005\n",
      "Number capped: 18\n",
      "Average cap: 2.091864585876465\n",
      "Number capped: 15\n",
      "Average cap: 3.126671552658081\n",
      "Number capped: 13\n",
      "Average cap: 3.1644158363342285\n",
      "Number capped: 16\n",
      "Average cap: 1.7305028438568115\n",
      "Number capped: 16\n",
      "Average cap: 2.639669418334961\n",
      "Number capped: 8\n",
      "Average cap: 5.9853386878967285\n",
      "Number capped: 15\n",
      "Average cap: 1.6082215309143066\n",
      "Number capped: 12\n",
      "Average cap: 3.462844133377075\n",
      "Number capped: 5\n",
      "Average cap: 13.957246780395508\n",
      "Number capped: 16\n",
      "Average cap: 1.987325668334961\n",
      "Number capped: 12\n",
      "Average cap: 3.50700306892395\n",
      "Number capped: 18\n",
      "Average cap: 1.7059998512268066\n",
      "Number capped: 11\n",
      "Average cap: 3.4441988468170166\n",
      "Number capped: 5\n",
      "Average cap: 18.539440155029297\n",
      "Number capped: 11\n",
      "Average cap: 4.199319839477539\n",
      "Number capped: 21\n",
      "Average cap: 2.0336904525756836\n",
      "Number capped: 14\n",
      "Average cap: 3.380744695663452\n",
      "Number capped: 17\n",
      "Average cap: 1.6976803541183472\n",
      "Number capped: 13\n",
      "Average cap: 2.733038902282715\n",
      "Number capped: 10\n",
      "Average cap: 5.811116695404053\n",
      "Number capped: 18\n",
      "Average cap: 2.777419090270996\n",
      "Number capped: 12\n",
      "Average cap: 2.998316526412964\n",
      "Number capped: 13\n",
      "Average cap: 3.3856608867645264\n",
      "Number capped: 9\n",
      "Average cap: 4.743973731994629\n",
      "Number capped: 9\n",
      "Average cap: 2.301269054412842\n",
      "Number capped: 15\n",
      "Average cap: 2.3074443340301514\n",
      "Number capped: 13\n",
      "Average cap: 3.3434793949127197\n",
      "Number capped: 4\n",
      "Average cap: 10.375316619873047\n",
      "Number capped: 14\n",
      "Average cap: 2.376514434814453\n",
      "Number capped: 21\n",
      "Average cap: 2.1563916206359863\n",
      "Number capped: 12\n",
      "Average cap: 3.4386274814605713\n",
      "Number capped: 16\n",
      "Average cap: 2.02432918548584\n",
      "Number capped: 6\n",
      "Average cap: 6.901651382446289\n",
      "Number capped: 17\n",
      "Average cap: 2.646193027496338\n",
      "Number capped: 16\n",
      "Average cap: 0.8782835006713867\n",
      "Number capped: 21\n",
      "Average cap: 0.7717798352241516\n",
      "Number capped: 13\n",
      "Average cap: 2.644531011581421\n",
      "Number capped: 16\n",
      "Average cap: 2.26747989654541\n",
      "Number capped: 11\n",
      "Average cap: 3.822807550430298\n",
      "Number capped: 14\n",
      "Average cap: 4.789816379547119\n",
      "Number capped: 12\n",
      "Average cap: 6.810924530029297\n",
      "Number capped: 11\n",
      "Average cap: 4.218173027038574\n",
      "Number capped: 11\n",
      "Average cap: 0.4154250919818878\n",
      "Number capped: 10\n",
      "Average cap: 4.393592834472656\n",
      "Number capped: 9\n",
      "Average cap: 8.75627326965332\n",
      "Number capped: 12\n",
      "Average cap: 0.7782284617424011\n",
      "Number capped: 14\n",
      "Average cap: 2.4323275089263916\n",
      "Number capped: 13\n",
      "Average cap: 3.107977867126465\n",
      "Number capped: 14\n",
      "Average cap: 3.0287039279937744\n",
      "Number capped: 10\n",
      "Average cap: 2.965549945831299\n",
      "Number capped: 11\n",
      "Average cap: 4.032289981842041\n",
      "Number capped: 7\n",
      "Average cap: 5.184883117675781\n",
      "Number capped: 12\n",
      "Average cap: 1.1771639585494995\n",
      "Number capped: 15\n",
      "Average cap: 2.8197824954986572\n",
      "Number capped: 7\n",
      "Average cap: 6.950680255889893\n",
      "Number capped: 10\n",
      "Average cap: 3.9683234691619873\n",
      "Number capped: 15\n",
      "Average cap: 2.928680658340454\n",
      "Number capped: 9\n",
      "Average cap: 4.001728534698486\n",
      "Number capped: 15\n",
      "Average cap: 2.7712624073028564\n",
      "Number capped: 12\n",
      "Average cap: 0.7359551787376404\n",
      "Number capped: 7\n",
      "Average cap: 1.5860904455184937\n",
      "Number capped: 8\n",
      "Average cap: 7.805344581604004\n",
      "Number capped: 13\n",
      "Average cap: 1.1891145706176758\n",
      "Number capped: 4\n",
      "Average cap: 10.374835968017578\n",
      "Number capped: 16\n",
      "Average cap: 0.31973662972450256\n",
      "Number capped: 9\n",
      "Average cap: 1.0602778196334839\n",
      "Number capped: 11\n",
      "Average cap: 5.897244453430176\n",
      "Number capped: 8\n",
      "Average cap: 5.38163423538208\n",
      "Number capped: 10\n",
      "Average cap: 4.43557071685791\n",
      "Number capped: 5\n",
      "Average cap: 7.701254844665527\n",
      "Number capped: 10\n",
      "Average cap: 2.3844282627105713\n",
      "Number capped: 9\n",
      "Average cap: 1.5187312364578247\n",
      "Number capped: 13\n",
      "Average cap: 0.9061320424079895\n",
      "Number capped: 15\n",
      "Average cap: 2.2230865955352783\n",
      "Number capped: 4\n",
      "Average cap: 1.1133326292037964\n",
      "Number capped: 15\n",
      "Average cap: 2.7019941806793213\n",
      "Number capped: 22\n",
      "Average cap: 1.8272080421447754\n",
      "Number capped: 18\n",
      "Average cap: 1.760031819343567\n",
      "Number capped: 7\n",
      "Average cap: 6.2227067947387695\n",
      "Number capped: 9\n",
      "Average cap: 3.9175944328308105\n",
      "Number capped: 18\n",
      "Average cap: 2.3426475524902344\n",
      "Number capped: 9\n",
      "Average cap: 0.42320770025253296\n",
      "Number capped: 14\n",
      "Average cap: 1.7338008880615234\n",
      "Number capped: 15\n",
      "Average cap: 3.280884265899658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 4\n",
      "Average cap: 10.627801895141602\n",
      "Number capped: 17\n",
      "Average cap: 1.8094958066940308\n",
      "Number capped: 14\n",
      "Average cap: 2.3648104667663574\n",
      "Number capped: 17\n",
      "Average cap: 2.0455141067504883\n",
      "Number capped: 14\n",
      "Average cap: 2.9383175373077393\n",
      "Number capped: 8\n",
      "Average cap: 4.052430629730225\n",
      "Number capped: 9\n",
      "Average cap: 4.62959098815918\n",
      "Number capped: 8\n",
      "Average cap: 0.8909064531326294\n",
      "Number capped: 9\n",
      "Average cap: 8.09699821472168\n",
      "Number capped: 13\n",
      "Average cap: 3.1203813552856445\n",
      "Number capped: 12\n",
      "Average cap: 2.2060623168945312\n",
      "Number capped: 11\n",
      "Average cap: 3.7589635848999023\n",
      "Number capped: 11\n",
      "Average cap: 3.8464722633361816\n",
      "Number capped: 8\n",
      "Average cap: 1.4698829650878906\n",
      "Number capped: 10\n",
      "Average cap: 3.9581799507141113\n",
      "Number capped: 6\n",
      "Average cap: 13.483820915222168\n",
      "Number capped: 21\n",
      "Average cap: 1.5309159755706787\n",
      "Number capped: 13\n",
      "Average cap: 3.250629186630249\n",
      "Number capped: 26\n",
      "Average cap: 1.5295194387435913\n",
      "Number capped: 22\n",
      "Average cap: 1.3637592792510986\n",
      "Number capped: 23\n",
      "Average cap: 1.1688907146453857\n",
      "Number capped: 21\n",
      "Average cap: 2.222674608230591\n",
      "Number capped: 20\n",
      "Average cap: 2.005896806716919\n",
      "Number capped: 20\n",
      "Average cap: 0.75174480676651\n",
      "Number capped: 18\n",
      "Average cap: 1.978261113166809\n",
      "Number capped: 22\n",
      "Average cap: 1.9766738414764404\n",
      "Number capped: 22\n",
      "Average cap: 1.6837188005447388\n",
      "Number capped: 17\n",
      "Average cap: 1.6064282655715942\n",
      "Number capped: 14\n",
      "Average cap: 1.9638079404830933\n",
      "Number capped: 20\n",
      "Average cap: 1.4154666662216187\n",
      "Number capped: 17\n",
      "Average cap: 1.5920950174331665\n",
      "Number capped: 19\n",
      "Average cap: 2.206254243850708\n",
      "Number capped: 21\n",
      "Average cap: 2.0450994968414307\n",
      "Number capped: 19\n",
      "Average cap: 2.1899092197418213\n",
      "Number capped: 20\n",
      "Average cap: 2.0499024391174316\n",
      "Number capped: 19\n",
      "Average cap: 0.48502328991889954\n",
      "Number capped: 22\n",
      "Average cap: 2.085334539413452\n",
      "Number capped: 8\n",
      "Average cap: 5.186948299407959\n",
      "Number capped: 14\n",
      "Average cap: 0.8271945118904114\n",
      "Number capped: 8\n",
      "Average cap: 5.690799713134766\n",
      "Number capped: 5\n",
      "Average cap: 1.834193468093872\n",
      "Number capped: 7\n",
      "Average cap: 0.841573178768158\n",
      "Number capped: 4\n",
      "Average cap: 8.976869583129883\n",
      "Number capped: 13\n",
      "Average cap: 3.356160879135132\n",
      "Number capped: 17\n",
      "Average cap: 2.43503475189209\n",
      "Number capped: 16\n",
      "Average cap: 1.3045463562011719\n",
      "Number capped: 11\n",
      "Average cap: 0.9938302636146545\n",
      "Number capped: 7\n",
      "Average cap: 6.831255912780762\n",
      "Number capped: 5\n",
      "Average cap: 10.021712303161621\n",
      "Number capped: 10\n",
      "Average cap: 4.180675983428955\n",
      "Number capped: 15\n",
      "Average cap: 1.992875337600708\n",
      "Number capped: 14\n",
      "Average cap: 0.9135248064994812\n",
      "Number capped: 21\n",
      "Average cap: 2.0988876819610596\n",
      "Number capped: 9\n",
      "Average cap: 5.194858074188232\n",
      "Number capped: 8\n",
      "Average cap: 6.92349910736084\n",
      "Number capped: 8\n",
      "Average cap: 8.266153335571289\n",
      "Number capped: 7\n",
      "Average cap: 4.639944553375244\n",
      "Number capped: 16\n",
      "Average cap: 2.244765281677246\n",
      "Number capped: 16\n",
      "Average cap: 2.1701130867004395\n",
      "Number capped: 16\n",
      "Average cap: 3.0067026615142822\n",
      "Number capped: 17\n",
      "Average cap: 1.2116503715515137\n",
      "Number capped: 6\n",
      "Average cap: 9.601288795471191\n",
      "Number capped: 14\n",
      "Average cap: 1.0080653429031372\n",
      "Number capped: 4\n",
      "Average cap: 1.9434492588043213\n",
      "Number capped: 6\n",
      "Average cap: 6.317760944366455\n",
      "Number capped: 7\n",
      "Average cap: 9.68067455291748\n",
      "Number capped: 12\n",
      "Average cap: 9.139592170715332\n",
      "Number capped: 10\n",
      "Average cap: 0.6960482001304626\n",
      "Number capped: 12\n",
      "Average cap: 1.6767820119857788\n",
      "Number capped: 6\n",
      "Average cap: 8.305575370788574\n",
      "Number capped: 10\n",
      "Average cap: 4.444185256958008\n",
      "Number capped: 14\n",
      "Average cap: 2.231783151626587\n",
      "Number capped: 9\n",
      "Average cap: 1.4900164604187012\n",
      "Number capped: 12\n",
      "Average cap: 3.8269741535186768\n",
      "Number capped: 7\n",
      "Average cap: 7.048005104064941\n",
      "Number capped: 12\n",
      "Average cap: 3.502450704574585\n",
      "Number capped: 8\n",
      "Average cap: 1.4086496829986572\n",
      "Number capped: 8\n",
      "Average cap: 6.032248497009277\n",
      "Number capped: 9\n",
      "Average cap: 4.358695030212402\n",
      "Number capped: 12\n",
      "Average cap: 2.588528633117676\n",
      "Number capped: 4\n",
      "Average cap: 1.7481095790863037\n",
      "Number capped: 5\n",
      "Average cap: 9.098966598510742\n",
      "Number capped: 9\n",
      "Average cap: 3.9115986824035645\n",
      "Number capped: 9\n",
      "Average cap: 4.710721969604492\n",
      "Number capped: 16\n",
      "Average cap: 0.02962363138794899\n",
      "Number capped: 5\n",
      "Average cap: 3.996452808380127\n",
      "Number capped: 11\n",
      "Average cap: 3.4292335510253906\n",
      "Number capped: 15\n",
      "Average cap: 2.291557550430298\n",
      "Number capped: 6\n",
      "Average cap: 7.974819183349609\n",
      "Number capped: 3\n",
      "Average cap: 2.172382116317749\n",
      "Number capped: 12\n",
      "Average cap: 3.480745553970337\n",
      "Number capped: 8\n",
      "Average cap: 4.700404644012451\n",
      "Number capped: 6\n",
      "Average cap: 0.17663757503032684\n",
      "Number capped: 4\n",
      "Average cap: 14.346504211425781\n",
      "Number capped: 5\n",
      "Average cap: 8.38847541809082\n",
      "Number capped: 4\n",
      "Average cap: 1.0044347047805786\n",
      "Number capped: 4\n",
      "Average cap: 13.182004928588867\n",
      "Number capped: 14\n",
      "Average cap: 2.8849940299987793\n",
      "Number capped: 10\n",
      "Average cap: 3.59273099899292\n",
      "Number capped: 9\n",
      "Average cap: 4.904199123382568\n",
      "Number capped: 6\n",
      "Average cap: 4.0036091804504395\n",
      "Number capped: 11\n",
      "Average cap: 1.326267957687378\n",
      "Number capped: 12\n",
      "Average cap: 3.1964714527130127\n",
      "Number capped: 8\n",
      "Average cap: 3.627164840698242\n",
      "Number capped: 11\n",
      "Average cap: 3.7720389366149902\n",
      "Number capped: 13\n",
      "Average cap: 3.3318686485290527\n",
      "Number capped: 8\n",
      "Average cap: 1.6454800367355347\n",
      "Number capped: 9\n",
      "Average cap: 1.5586388111114502\n",
      "Number capped: 3\n",
      "Average cap: 12.821877479553223\n",
      "Number capped: 6\n",
      "Average cap: 7.499229907989502\n",
      "Number capped: 9\n",
      "Average cap: 4.558568000793457\n",
      "Number capped: 11\n",
      "Average cap: 4.124473571777344\n",
      "Number capped: 9\n",
      "Average cap: 4.2423095703125\n",
      "Number capped: 10\n",
      "Average cap: 4.089421272277832\n",
      "Number capped: 11\n",
      "Average cap: 5.100645065307617\n",
      "Number capped: 1\n",
      "Average cap: 21.406681060791016\n",
      "Number capped: 30\n",
      "Average cap: 0.7788575291633606\n",
      "Number capped: 19\n",
      "Average cap: 2.3166940212249756\n",
      "Number capped: 25\n",
      "Average cap: 1.8815116882324219\n",
      "Number capped: 21\n",
      "Average cap: 2.362769365310669\n",
      "Number capped: 21\n",
      "Average cap: 1.4716796875\n",
      "Number capped: 21\n",
      "Average cap: 2.2863452434539795\n",
      "Number capped: 21\n",
      "Average cap: 1.0062237977981567\n",
      "Number capped: 20\n",
      "Average cap: 2.1788411140441895\n",
      "Number capped: 24\n",
      "Average cap: 2.0097663402557373\n",
      "Number capped: 13\n",
      "Average cap: 3.2422077655792236\n",
      "Number capped: 20\n",
      "Average cap: 1.7902793884277344\n",
      "Number capped: 17\n",
      "Average cap: 2.2750282287597656\n",
      "Number capped: 15\n",
      "Average cap: 3.0177505016326904\n",
      "Number capped: 24\n",
      "Average cap: 1.7162394523620605\n",
      "Number capped: 20\n",
      "Average cap: 2.0736875534057617\n",
      "Number capped: 19\n",
      "Average cap: 2.5582122802734375\n",
      "Number capped: 16\n",
      "Average cap: 1.6317194700241089\n",
      "Number capped: 3\n",
      "Average cap: 2.9882612228393555\n",
      "Number capped: 19\n",
      "Average cap: 2.3156139850616455\n",
      "Number capped: 16\n",
      "Average cap: 2.23478102684021\n",
      "Number capped: 18\n",
      "Average cap: 2.6788623332977295\n",
      "Number capped: 19\n",
      "Average cap: 2.4726593494415283\n",
      "Number capped: 14\n",
      "Average cap: 3.238969326019287\n",
      "Number capped: 14\n",
      "Average cap: 2.9098896980285645\n",
      "Number capped: 19\n",
      "Average cap: 2.168537139892578\n",
      "Number capped: 17\n",
      "Average cap: 2.9552383422851562\n",
      "Number capped: 21\n",
      "Average cap: 2.0610337257385254\n",
      "Number capped: 21\n",
      "Average cap: 1.6561375856399536\n",
      "Number capped: 16\n",
      "Average cap: 2.553835391998291\n",
      "Number capped: 20\n",
      "Average cap: 2.0108718872070312\n",
      "Number capped: 10\n",
      "Average cap: 4.135101795196533\n",
      "Number capped: 16\n",
      "Average cap: 3.0484731197357178\n",
      "Number capped: 17\n",
      "Average cap: 2.1326305866241455\n",
      "Number capped: 11\n",
      "Average cap: 4.110110282897949\n",
      "Number capped: 12\n",
      "Average cap: 3.9040374755859375\n",
      "Number capped: 17\n",
      "Average cap: 1.8125094175338745\n",
      "Number capped: 12\n",
      "Average cap: 3.903164863586426\n",
      "Number capped: 23\n",
      "Average cap: 1.9212912321090698\n",
      "Number capped: 17\n",
      "Average cap: 1.1345239877700806\n",
      "Number capped: 14\n",
      "Average cap: 4.4320502281188965\n",
      "Number capped: 20\n",
      "Average cap: 2.1089320182800293\n",
      "Number capped: 15\n",
      "Average cap: 2.9810025691986084\n",
      "Number capped: 13\n",
      "Average cap: 3.2097837924957275\n",
      "Number capped: 13\n",
      "Average cap: 3.4837305545806885\n",
      "Number capped: 14\n",
      "Average cap: 3.0207951068878174\n",
      "Number capped: 10\n",
      "Average cap: 4.481136322021484\n",
      "Number capped: 7\n",
      "Average cap: 6.099431037902832\n",
      "Number capped: 10\n",
      "Average cap: 3.8358712196350098\n",
      "Number capped: 9\n",
      "Average cap: 4.320443153381348\n",
      "Number capped: 14\n",
      "Average cap: 3.098193645477295\n",
      "Number capped: 8\n",
      "Average cap: 5.615063190460205\n",
      "Number capped: 6\n",
      "Average cap: 6.7436065673828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 5\n",
      "Average cap: 7.68356990814209\n",
      "Number capped: 9\n",
      "Average cap: 3.703672170639038\n",
      "Number capped: 3\n",
      "Average cap: 13.190632820129395\n",
      "Number capped: 20\n",
      "Average cap: 1.3739259243011475\n",
      "Number capped: 11\n",
      "Average cap: 3.9685120582580566\n",
      "Number capped: 15\n",
      "Average cap: 2.8572144508361816\n",
      "Number capped: 12\n",
      "Average cap: 4.111420154571533\n",
      "Number capped: 9\n",
      "Average cap: 8.056411743164062\n",
      "Number capped: 13\n",
      "Average cap: 3.1799166202545166\n",
      "Number capped: 9\n",
      "Average cap: 5.364994525909424\n",
      "Number capped: 5\n",
      "Average cap: 4.524228572845459\n",
      "Number capped: 15\n",
      "Average cap: 0.6311108469963074\n",
      "Number capped: 8\n",
      "Average cap: 6.463203430175781\n",
      "Number capped: 8\n",
      "Average cap: 5.155890941619873\n",
      "Number capped: 21\n",
      "Average cap: 0.9138804078102112\n",
      "Number capped: 8\n",
      "Average cap: 4.868995666503906\n",
      "Number capped: 7\n",
      "Average cap: 11.48698902130127\n",
      "Number capped: 4\n",
      "Average cap: 0.09561275690793991\n",
      "Number capped: 11\n",
      "Average cap: 4.251951694488525\n",
      "Number capped: 9\n",
      "Average cap: 3.4170820713043213\n",
      "Number capped: 5\n",
      "Average cap: 4.945854187011719\n",
      "Number capped: 16\n",
      "Average cap: 1.5633437633514404\n",
      "Number capped: 16\n",
      "Average cap: 0.8887401819229126\n",
      "Number capped: 8\n",
      "Average cap: 3.1643259525299072\n",
      "Number capped: 6\n",
      "Average cap: 0.6259844303131104\n",
      "Number capped: 5\n",
      "Average cap: 8.859330177307129\n",
      "Number capped: 7\n",
      "Average cap: 0.7257217764854431\n",
      "Number capped: 8\n",
      "Average cap: 2.299339532852173\n",
      "Number capped: 5\n",
      "Average cap: 2.3856852054595947\n",
      "Number capped: 4\n",
      "Average cap: 10.617557525634766\n",
      "Number capped: 7\n",
      "Average cap: 5.781322956085205\n",
      "Number capped: 9\n",
      "Average cap: 3.9169819355010986\n",
      "Number capped: 8\n",
      "Average cap: 0.2639796733856201\n",
      "Number capped: 8\n",
      "Average cap: 4.848227500915527\n",
      "Number capped: 12\n",
      "Average cap: 3.5500662326812744\n",
      "Number capped: 7\n",
      "Average cap: 1.4446028470993042\n",
      "Number capped: 16\n",
      "Average cap: 2.2286579608917236\n",
      "Number capped: 11\n",
      "Average cap: 3.789301872253418\n",
      "Number capped: 9\n",
      "Average cap: 4.477662086486816\n",
      "Number capped: 3\n",
      "Average cap: 13.324969291687012\n",
      "Number capped: 2\n",
      "Average cap: 20.03132438659668\n",
      "Number capped: 6\n",
      "Average cap: 6.738962650299072\n",
      "Number capped: 16\n",
      "Average cap: 2.2970998287200928\n",
      "Number capped: 13\n",
      "Average cap: 3.116809844970703\n",
      "Number capped: 7\n",
      "Average cap: 6.7597527503967285\n",
      "Number capped: 8\n",
      "Average cap: 0.8775967955589294\n",
      "Number capped: 7\n",
      "Average cap: 1.1504172086715698\n",
      "Number capped: 7\n",
      "Average cap: 8.026222229003906\n",
      "Number capped: 14\n",
      "Average cap: 3.2728614807128906\n",
      "Number capped: 16\n",
      "Average cap: 2.9550156593322754\n",
      "Number capped: 10\n",
      "Average cap: 3.5832791328430176\n",
      "Number capped: 6\n",
      "Average cap: 6.776922225952148\n",
      "Number capped: 18\n",
      "Average cap: 2.155956983566284\n",
      "Number capped: 9\n",
      "Average cap: 3.837721109390259\n",
      "Number capped: 5\n",
      "Average cap: 2.2864022254943848\n",
      "Number capped: 7\n",
      "Average cap: 5.189239501953125\n",
      "Number capped: 12\n",
      "Average cap: 3.788140058517456\n",
      "Number capped: 7\n",
      "Average cap: 5.824837684631348\n",
      "Number capped: 11\n",
      "Average cap: 6.959419250488281\n",
      "Number capped: 11\n",
      "Average cap: 3.649822950363159\n",
      "Number capped: 8\n",
      "Average cap: 5.716333389282227\n",
      "Number capped: 8\n",
      "Average cap: 5.192611217498779\n",
      "Number capped: 13\n",
      "Average cap: 1.9000223875045776\n",
      "Number capped: 5\n",
      "Average cap: 7.5325140953063965\n",
      "Number capped: 6\n",
      "Average cap: 6.4362640380859375\n",
      "Number capped: 9\n",
      "Average cap: 4.549779891967773\n",
      "Number capped: 15\n",
      "Average cap: 2.783526659011841\n",
      "Number capped: 16\n",
      "Average cap: 1.2638881206512451\n",
      "Number capped: 12\n",
      "Average cap: 1.1475697755813599\n",
      "Number capped: 11\n",
      "Average cap: 2.9363913536071777\n",
      "Number capped: 7\n",
      "Average cap: 8.653154373168945\n",
      "Number capped: 13\n",
      "Average cap: 2.0034737586975098\n",
      "Number capped: 11\n",
      "Average cap: 3.8725168704986572\n",
      "Number capped: 14\n",
      "Average cap: 3.004944086074829\n",
      "Number capped: 7\n",
      "Average cap: 0.27145764231681824\n",
      "Number capped: 12\n",
      "Average cap: 3.9662628173828125\n",
      "Number capped: 7\n",
      "Average cap: 3.275529384613037\n",
      "Number capped: 3\n",
      "Average cap: 14.07665729522705\n",
      "Number capped: 10\n",
      "Average cap: 4.3788957595825195\n",
      "Number capped: 12\n",
      "Average cap: 3.3277626037597656\n",
      "Number capped: 7\n",
      "Average cap: 2.64858078956604\n",
      "Number capped: 5\n",
      "Average cap: 3.0520613193511963\n",
      "Number capped: 4\n",
      "Average cap: 0.8612107634544373\n",
      "Number capped: 19\n",
      "Average cap: 2.1192519664764404\n",
      "Number capped: 13\n",
      "Average cap: 2.3832523822784424\n",
      "Number capped: 6\n",
      "Average cap: 6.198121547698975\n",
      "Number capped: 8\n",
      "Average cap: 1.5025501251220703\n",
      "Number capped: 2\n",
      "Average cap: 1.3636325597763062\n",
      "Number capped: 14\n",
      "Average cap: 3.4861090183258057\n",
      "Number capped: 14\n",
      "Average cap: 5.700047016143799\n",
      "Number capped: 19\n",
      "Average cap: 1.7232879400253296\n",
      "Number capped: 15\n",
      "Average cap: 0.7544475197792053\n",
      "Number capped: 12\n",
      "Average cap: 3.6649129390716553\n",
      "Number capped: 16\n",
      "Average cap: 0.5080214142799377\n",
      "Number capped: 11\n",
      "Average cap: 2.143115758895874\n",
      "Number capped: 13\n",
      "Average cap: 2.0801374912261963\n",
      "Number capped: 13\n",
      "Average cap: 4.113494396209717\n",
      "Number capped: 14\n",
      "Average cap: 1.5588408708572388\n",
      "Number capped: 10\n",
      "Average cap: 2.7181663513183594\n",
      "Number capped: 14\n",
      "Average cap: 3.732644557952881\n",
      "Number capped: 13\n",
      "Average cap: 1.563196063041687\n",
      "Number capped: 9\n",
      "Average cap: 1.3789616823196411\n",
      "Number capped: 7\n",
      "Average cap: 5.699742794036865\n",
      "Number capped: 9\n",
      "Average cap: 5.6743974685668945\n",
      "Number capped: 5\n",
      "Average cap: 7.8826141357421875\n",
      "Number capped: 4\n",
      "Average cap: 7.35037088394165\n",
      "Number capped: 3\n",
      "Average cap: 13.051399230957031\n",
      "Number capped: 7\n",
      "Average cap: 0.4322013556957245\n",
      "Number capped: 5\n",
      "Average cap: 3.4931094646453857\n",
      "Number capped: 13\n",
      "Average cap: 0.7635105848312378\n",
      "Number capped: 7\n",
      "Average cap: 5.246971130371094\n",
      "Number capped: 8\n",
      "Average cap: 10.56512451171875\n",
      "Number capped: 15\n",
      "Average cap: 1.8008310794830322\n",
      "Number capped: 5\n",
      "Average cap: 8.86776065826416\n",
      "Number capped: 3\n",
      "Average cap: 7.756923198699951\n",
      "Number capped: 16\n",
      "Average cap: 2.303966522216797\n",
      "Number capped: 11\n",
      "Average cap: 5.2027668952941895\n",
      "Number capped: 7\n",
      "Average cap: 0.3786875307559967\n",
      "Number capped: 9\n",
      "Average cap: 4.15211296081543\n",
      "Number capped: 9\n",
      "Average cap: 4.584300994873047\n",
      "Number capped: 12\n",
      "Average cap: 2.7559125423431396\n",
      "Number capped: 9\n",
      "Average cap: 1.1524643898010254\n",
      "Number capped: 1\n",
      "Average cap: 7.838133811950684\n",
      "Number capped: 2\n",
      "Average cap: 24.353790283203125\n",
      "Number capped: 6\n",
      "Average cap: 4.427520751953125\n",
      "Number capped: 9\n",
      "Average cap: 1.0236256122589111\n",
      "Number capped: 5\n",
      "Average cap: 1.2990810871124268\n",
      "Number capped: 8\n",
      "Average cap: 1.3803619146347046\n",
      "Number capped: 5\n",
      "Average cap: 31.148279190063477\n",
      "Number capped: 8\n",
      "Average cap: 3.6510367393493652\n",
      "Number capped: 6\n",
      "Average cap: 1.3480777740478516\n",
      "Number capped: 5\n",
      "Average cap: 7.509644985198975\n",
      "Number capped: 5\n",
      "Average cap: 1.2755742073059082\n",
      "Number capped: 8\n",
      "Average cap: 2.55535888671875\n",
      "Number capped: 5\n",
      "Average cap: 10.654694557189941\n",
      "Number capped: 8\n",
      "Average cap: 4.99668550491333\n",
      "Number capped: 8\n",
      "Average cap: 6.839841842651367\n",
      "Number capped: 4\n",
      "Average cap: 1.8031208515167236\n",
      "Number capped: 5\n",
      "Average cap: 1.6809200048446655\n",
      "Number capped: 4\n",
      "Average cap: 11.200674057006836\n",
      "Number capped: 5\n",
      "Average cap: 8.061291694641113\n",
      "Number capped: 4\n",
      "Average cap: 0.913879930973053\n",
      "Number capped: 5\n",
      "Average cap: 11.483525276184082\n",
      "Number capped: 6\n",
      "Average cap: 4.212899684906006\n",
      "Number capped: 6\n",
      "Average cap: 3.6535518169403076\n",
      "Number capped: 8\n",
      "Average cap: 5.389475345611572\n",
      "Number capped: 2\n",
      "Average cap: 2.695683479309082\n",
      "Number capped: 7\n",
      "Average cap: 4.503686904907227\n",
      "Number capped: 11\n",
      "Average cap: 4.362870693206787\n",
      "Number capped: 10\n",
      "Average cap: 2.2932729721069336\n",
      "Number capped: 11\n",
      "Average cap: 0.7526543140411377\n",
      "Number capped: 6\n",
      "Average cap: 6.8982768058776855\n",
      "Number capped: 10\n",
      "Average cap: 0.9631586074829102\n",
      "Number capped: 4\n",
      "Average cap: 12.255097389221191\n",
      "Number capped: 8\n",
      "Average cap: 4.431124687194824\n",
      "Number capped: 11\n",
      "Average cap: 0.9732309579849243\n",
      "Number capped: 9\n",
      "Average cap: 4.544656753540039\n",
      "Number capped: 6\n",
      "Average cap: 7.704905986785889\n",
      "Number capped: 4\n",
      "Average cap: 9.979040145874023\n",
      "Number capped: 6\n",
      "Average cap: 1.6287389993667603\n",
      "Number capped: 4\n",
      "Average cap: 0.043971143662929535\n",
      "Number capped: 7\n",
      "Average cap: 5.65536642074585\n",
      "Number capped: 8\n",
      "Average cap: 5.908170223236084\n",
      "Number capped: 9\n",
      "Average cap: 4.355140686035156\n",
      "Number capped: 6\n",
      "Average cap: 7.999617099761963\n",
      "Number capped: 12\n",
      "Average cap: 3.3485629558563232\n",
      "Number capped: 8\n",
      "Average cap: 8.963980674743652\n",
      "Number capped: 4\n",
      "Average cap: 4.9794745445251465\n",
      "Number capped: 8\n",
      "Average cap: 1.9219950437545776\n",
      "Number capped: 6\n",
      "Average cap: 5.9550251960754395\n",
      "Number capped: 5\n",
      "Average cap: 0.6655316948890686\n",
      "Number capped: 9\n",
      "Average cap: 3.80416202545166\n",
      "Number capped: 6\n",
      "Average cap: 0.8133604526519775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 10\n",
      "Average cap: 3.95503306388855\n",
      "Number capped: 9\n",
      "Average cap: 3.8355441093444824\n",
      "Number capped: 8\n",
      "Average cap: 0.5312440395355225\n",
      "Number capped: 3\n",
      "Average cap: 44.769161224365234\n",
      "Number capped: 7\n",
      "Average cap: 5.581712245941162\n",
      "Number capped: 7\n",
      "Average cap: 7.8283562660217285\n",
      "Number capped: 9\n",
      "Average cap: 3.7892160415649414\n",
      "Number capped: 6\n",
      "Average cap: 5.1856770515441895\n",
      "Number capped: 5\n",
      "Average cap: 4.201229095458984\n",
      "Number capped: 8\n",
      "Average cap: 0.2601824402809143\n",
      "Number capped: 5\n",
      "Average cap: 7.995112419128418\n",
      "Number capped: 3\n",
      "Average cap: 9.201055526733398\n",
      "Number capped: 3\n",
      "Average cap: 0.37269020080566406\n",
      "Number capped: 10\n",
      "Average cap: 0.6820558905601501\n",
      "Number capped: 2\n",
      "Average cap: 5.2878851890563965\n",
      "Number capped: 9\n",
      "Average cap: 4.427680969238281\n",
      "Number capped: 9\n",
      "Average cap: 0.9054810404777527\n",
      "Number capped: 6\n",
      "Average cap: 6.114488124847412\n",
      "Number capped: 10\n",
      "Average cap: 3.975579023361206\n",
      "Number capped: 3\n",
      "Average cap: 2.671720504760742\n",
      "Number capped: 9\n",
      "Average cap: 0.9177453517913818\n",
      "Number capped: 2\n",
      "Average cap: 24.993247985839844\n",
      "Number capped: 7\n",
      "Average cap: 5.584537982940674\n",
      "Number capped: 13\n",
      "Average cap: 2.8285255432128906\n",
      "Number capped: 3\n",
      "Average cap: 13.64774227142334\n",
      "Number capped: 6\n",
      "Average cap: 12.537528991699219\n",
      "Number capped: 4\n",
      "Average cap: 1.1351325511932373\n",
      "Number capped: 3\n",
      "Average cap: 14.855351448059082\n",
      "Number capped: 8\n",
      "Average cap: 4.358092784881592\n",
      "Number capped: 8\n",
      "Average cap: 6.661716938018799\n",
      "Number capped: 13\n",
      "Average cap: 0.7648734450340271\n",
      "Number capped: 8\n",
      "Average cap: 4.943147659301758\n",
      "Number capped: 7\n",
      "Average cap: 6.803015232086182\n",
      "Number capped: 5\n",
      "Average cap: 7.648426055908203\n",
      "Number capped: 5\n",
      "Average cap: 7.838233947753906\n",
      "Number capped: 2\n",
      "Average cap: 23.372257232666016\n",
      "Number capped: 7\n",
      "Average cap: 5.151791095733643\n",
      "Number capped: 5\n",
      "Average cap: 1.032200813293457\n",
      "Number capped: 10\n",
      "Average cap: 0.6977657079696655\n",
      "Number capped: 8\n",
      "Average cap: 5.008260250091553\n",
      "Number capped: 6\n",
      "Average cap: 6.274570465087891\n",
      "Number capped: 6\n",
      "Average cap: 1.411024570465088\n",
      "Number capped: 8\n",
      "Average cap: 2.2006475925445557\n",
      "Number capped: 9\n",
      "Average cap: 3.7269668579101562\n",
      "Number capped: 4\n",
      "Average cap: 9.85234260559082\n",
      "Number capped: 5\n",
      "Average cap: 0.6318630576133728\n",
      "Number capped: 2\n",
      "Average cap: 19.60159683227539\n",
      "Number capped: 10\n",
      "Average cap: 4.200779914855957\n",
      "Number capped: 8\n",
      "Average cap: 4.71952486038208\n",
      "Number capped: 4\n",
      "Average cap: 1.5440990924835205\n",
      "Number capped: 7\n",
      "Average cap: 5.096154689788818\n",
      "Number capped: 8\n",
      "Average cap: 3.3062593936920166\n",
      "Number capped: 6\n",
      "Average cap: 6.548570156097412\n",
      "Number capped: 7\n",
      "Average cap: 3.095643997192383\n",
      "Number capped: 6\n",
      "Average cap: 4.403526782989502\n",
      "Number capped: 4\n",
      "Average cap: 8.198348999023438\n",
      "Number capped: 2\n",
      "Average cap: 8.718905448913574\n",
      "Number capped: 3\n",
      "Average cap: 16.6581974029541\n",
      "Number capped: 3\n",
      "Average cap: 13.47061824798584\n",
      "Number capped: 5\n",
      "Average cap: 0.9106966853141785\n",
      "Number capped: 5\n",
      "Average cap: 12.7044095993042\n",
      "Number capped: 2\n",
      "Average cap: 8.250931739807129\n",
      "Number capped: 5\n",
      "Average cap: 7.9412407875061035\n",
      "Number capped: 9\n",
      "Average cap: 4.725763320922852\n",
      "Number capped: 8\n",
      "Average cap: 6.635483264923096\n",
      "Number capped: 9\n",
      "Average cap: 0.44407278299331665\n",
      "Number capped: 10\n",
      "Average cap: 4.063025951385498\n",
      "Number capped: 7\n",
      "Average cap: 10.693193435668945\n",
      "Number capped: 4\n",
      "Average cap: 1.9674196243286133\n",
      "Number capped: 8\n",
      "Average cap: 4.868256092071533\n",
      "Number capped: 4\n",
      "Average cap: 12.108586311340332\n",
      "Number capped: 8\n",
      "Average cap: 4.312352657318115\n",
      "Number capped: 3\n",
      "Average cap: 13.123578071594238\n",
      "Number capped: 2\n",
      "Average cap: 18.011930465698242\n",
      "Number capped: 5\n",
      "Average cap: 2.9287285804748535\n",
      "Number capped: 4\n",
      "Average cap: 12.799823760986328\n",
      "Number capped: 6\n",
      "Average cap: 10.059952735900879\n",
      "Number capped: 6\n",
      "Average cap: 6.314128875732422\n",
      "Number capped: 9\n",
      "Average cap: 4.739413738250732\n",
      "Number capped: 2\n",
      "Average cap: 1.0131584405899048\n",
      "Number capped: 11\n",
      "Average cap: 3.861769914627075\n",
      "Number capped: 8\n",
      "Average cap: 4.899116039276123\n",
      "Number capped: 6\n",
      "Average cap: 6.005703449249268\n",
      "Number capped: 2\n",
      "Average cap: 21.23481559753418\n",
      "Number capped: 8\n",
      "Average cap: 2.7515628337860107\n",
      "Number capped: 5\n",
      "Average cap: 7.8825273513793945\n",
      "Number capped: 5\n",
      "Average cap: 8.437910079956055\n",
      "Number capped: 6\n",
      "Average cap: 15.049450874328613\n",
      "Number capped: 10\n",
      "Average cap: 4.120724678039551\n",
      "Number capped: 7\n",
      "Average cap: 5.547786235809326\n",
      "Number capped: 5\n",
      "Average cap: 7.534613609313965\n",
      "Number capped: 4\n",
      "Average cap: 0.6603608727455139\n",
      "Number capped: 6\n",
      "Average cap: 7.190650939941406\n",
      "Number capped: 5\n",
      "Average cap: 7.285527229309082\n",
      "Number capped: 5\n",
      "Average cap: 9.37496566772461\n",
      "Number capped: 7\n",
      "Average cap: 5.005584716796875\n",
      "Number capped: 5\n",
      "Average cap: 7.162798881530762\n",
      "Number capped: 9\n",
      "Average cap: 0.5015226006507874\n",
      "Number capped: 4\n",
      "Average cap: 9.258417129516602\n",
      "Number capped: 7\n",
      "Average cap: 8.054518699645996\n",
      "Number capped: 6\n",
      "Average cap: 2.340043783187866\n",
      "Number capped: 2\n",
      "Average cap: 14.360610961914062\n",
      "Number capped: 5\n",
      "Average cap: 4.652369976043701\n",
      "Number capped: 8\n",
      "Average cap: 4.893815517425537\n",
      "Number capped: 6\n",
      "Average cap: 2.743743896484375\n",
      "Number capped: 4\n",
      "Average cap: 9.417593002319336\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 5\n",
      "Average cap: 8.307143211364746\n",
      "Number capped: 4\n",
      "Average cap: 7.077448844909668\n",
      "Number capped: 4\n",
      "Average cap: 2.7531497478485107\n",
      "Number capped: 14\n",
      "Average cap: 0.8612357378005981\n",
      "Number capped: 4\n",
      "Average cap: 11.541781425476074\n",
      "Number capped: 8\n",
      "Average cap: 3.601951837539673\n",
      "Number capped: 5\n",
      "Average cap: 4.7660956382751465\n",
      "Number capped: 2\n",
      "Average cap: 60.07788848876953\n",
      "Number capped: 4\n",
      "Average cap: 9.036561965942383\n",
      "Number capped: 1\n",
      "Average cap: 7.860911846160889\n",
      "Number capped: 10\n",
      "Average cap: 4.512838363647461\n",
      "Number capped: 2\n",
      "Average cap: 0.4277834892272949\n",
      "Number capped: 5\n",
      "Average cap: 1.245013952255249\n",
      "Number capped: 3\n",
      "Average cap: 0.7180015444755554\n",
      "Number capped: 7\n",
      "Average cap: 5.168894290924072\n",
      "Number capped: 4\n",
      "Average cap: 1.6431818008422852\n",
      "Number capped: 3\n",
      "Average cap: 12.75546932220459\n",
      "Number capped: 5\n",
      "Average cap: 7.8795881271362305\n",
      "Number capped: 6\n",
      "Average cap: 6.787158489227295\n",
      "Number capped: 3\n",
      "Average cap: 3.769667625427246\n",
      "Number capped: 5\n",
      "Average cap: 0.6029955148696899\n",
      "Number capped: 6\n",
      "Average cap: 7.855881214141846\n",
      "Number capped: 6\n",
      "Average cap: 10.803048133850098\n",
      "Number capped: 12\n",
      "Average cap: 2.644216775894165\n",
      "Number capped: 3\n",
      "Average cap: 15.094502449035645\n",
      "Number capped: 6\n",
      "Average cap: 5.837564945220947\n",
      "Number capped: 3\n",
      "Average cap: 13.061482429504395\n",
      "Number capped: 7\n",
      "Average cap: 2.611891269683838\n",
      "Number capped: 6\n",
      "Average cap: 3.0261030197143555\n",
      "Number capped: 2\n",
      "Average cap: 24.952470779418945\n",
      "Number capped: 17\n",
      "Average cap: 1.0613564252853394\n",
      "Number capped: 5\n",
      "Average cap: 2.427219867706299\n",
      "Number capped: 13\n",
      "Average cap: 2.9789090156555176\n",
      "Number capped: 6\n",
      "Average cap: 2.9038407802581787\n",
      "Number capped: 8\n",
      "Average cap: 4.905002593994141\n",
      "Number capped: 6\n",
      "Average cap: 1.017194390296936\n",
      "Number capped: 6\n",
      "Average cap: 0.8634315133094788\n",
      "Number capped: 5\n",
      "Average cap: 15.985525131225586\n",
      "Number capped: 5\n",
      "Average cap: 1.6149005889892578\n",
      "Number capped: 7\n",
      "Average cap: 5.635224342346191\n",
      "Number capped: 5\n",
      "Average cap: 7.114523887634277\n",
      "Number capped: 4\n",
      "Average cap: 17.808856964111328\n",
      "Number capped: 7\n",
      "Average cap: 5.580740451812744\n",
      "Number capped: 4\n",
      "Average cap: 0.5747522711753845\n",
      "Number capped: 10\n",
      "Average cap: 0.724286675453186\n",
      "Number capped: 5\n",
      "Average cap: 0.6742356419563293\n",
      "Number capped: 11\n",
      "Average cap: 3.917593479156494\n",
      "Number capped: 3\n",
      "Average cap: 7.24029016494751\n",
      "Number capped: 10\n",
      "Average cap: 3.8842101097106934\n",
      "Number capped: 4\n",
      "Average cap: 9.532480239868164\n",
      "Number capped: 6\n",
      "Average cap: 3.8235416412353516\n",
      "Number capped: 9\n",
      "Average cap: 0.6015866994857788\n",
      "Number capped: 4\n",
      "Average cap: 9.943161010742188\n",
      "Number capped: 4\n",
      "Average cap: 9.655248641967773\n",
      "Number capped: 5\n",
      "Average cap: 1.880286455154419\n",
      "Number capped: 5\n",
      "Average cap: 3.173753023147583\n",
      "Number capped: 4\n",
      "Average cap: 0.8641167879104614\n",
      "Number capped: 5\n",
      "Average cap: 7.355991363525391\n",
      "Number capped: 7\n",
      "Average cap: 5.865768909454346\n",
      "Number capped: 5\n",
      "Average cap: 9.861757278442383\n",
      "Number capped: 9\n",
      "Average cap: 0.7246692776679993\n",
      "Number capped: 5\n",
      "Average cap: 7.688563346862793\n",
      "Number capped: 5\n",
      "Average cap: 7.9041748046875\n",
      "Number capped: 4\n",
      "Average cap: 0.747826337814331\n",
      "Number capped: 8\n",
      "Average cap: 4.8672308921813965\n",
      "Number capped: 5\n",
      "Average cap: 0.7164233922958374\n",
      "Number capped: 16\n",
      "Average cap: 2.4363744258880615\n",
      "Number capped: 9\n",
      "Average cap: 2.0410563945770264\n",
      "Number capped: 5\n",
      "Average cap: 5.842929840087891\n",
      "Number capped: 2\n",
      "Average cap: 19.48520278930664\n",
      "Number capped: 8\n",
      "Average cap: 4.433121681213379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 2\n",
      "Average cap: 19.305885314941406\n",
      "Number capped: 8\n",
      "Average cap: 1.332735538482666\n",
      "Number capped: 7\n",
      "Average cap: 0.8767052888870239\n",
      "Number capped: 5\n",
      "Average cap: 9.151310920715332\n",
      "Number capped: 7\n",
      "Average cap: 0.3239912986755371\n",
      "Number capped: 4\n",
      "Average cap: 16.727346420288086\n",
      "Number capped: 10\n",
      "Average cap: 4.517697811126709\n",
      "Number capped: 8\n",
      "Average cap: 4.861210823059082\n",
      "Number capped: 8\n",
      "Average cap: 6.072116374969482\n",
      "Number capped: 10\n",
      "Average cap: 4.913789749145508\n",
      "Number capped: 4\n",
      "Average cap: 13.058260917663574\n",
      "Number capped: 2\n",
      "Average cap: 20.31639289855957\n",
      "Number capped: 5\n",
      "Average cap: 4.9985575675964355\n",
      "Number capped: 5\n",
      "Average cap: 1.4421665668487549\n",
      "Number capped: 2\n",
      "Average cap: 19.31671142578125\n",
      "Number capped: 2\n",
      "Average cap: 5.082912445068359\n",
      "Number capped: 3\n",
      "Average cap: 12.230391502380371\n",
      "Number capped: 9\n",
      "Average cap: 5.668125629425049\n",
      "Number capped: 6\n",
      "Average cap: 6.58305025100708\n",
      "Number capped: 5\n",
      "Average cap: 7.0895094871521\n",
      "Number capped: 1\n",
      "Average cap: 1.872563123703003\n",
      "Number capped: 12\n",
      "Average cap: 2.081953763961792\n",
      "Number capped: 11\n",
      "Average cap: 3.337287664413452\n",
      "Number capped: 11\n",
      "Average cap: 3.9688761234283447\n",
      "Number capped: 6\n",
      "Average cap: 5.784029006958008\n",
      "Number capped: 7\n",
      "Average cap: 1.2731939554214478\n",
      "Number capped: 7\n",
      "Average cap: 7.732222080230713\n",
      "Number capped: 3\n",
      "Average cap: 12.756889343261719\n",
      "Number capped: 9\n",
      "Average cap: 3.6783947944641113\n",
      "Number capped: 4\n",
      "Average cap: 9.583090782165527\n",
      "Number capped: 1\n",
      "Average cap: 38.341400146484375\n",
      "Number capped: 9\n",
      "Average cap: 4.405855178833008\n",
      "Number capped: 7\n",
      "Average cap: 5.7309441566467285\n",
      "Number capped: 6\n",
      "Average cap: 5.03553581237793\n",
      "Number capped: 6\n",
      "Average cap: 7.758276462554932\n",
      "Number capped: 7\n",
      "Average cap: 5.065963268280029\n",
      "Number capped: 6\n",
      "Average cap: 9.385836601257324\n",
      "Number capped: 5\n",
      "Average cap: 7.782467842102051\n",
      "Number capped: 5\n",
      "Average cap: 0.6233096718788147\n",
      "Number capped: 5\n",
      "Average cap: 8.301373481750488\n",
      "Number capped: 3\n",
      "Average cap: 12.886454582214355\n",
      "Number capped: 8\n",
      "Average cap: 4.614452362060547\n",
      "Number capped: 4\n",
      "Average cap: 6.6545538902282715\n",
      "Number capped: 5\n",
      "Average cap: 7.425478458404541\n",
      "Number capped: 6\n",
      "Average cap: 6.332552433013916\n",
      "Number capped: 5\n",
      "Average cap: 8.595141410827637\n",
      "Number capped: 8\n",
      "Average cap: 4.786037921905518\n",
      "Number capped: 3\n",
      "Average cap: 12.956631660461426\n",
      "Number capped: 6\n",
      "Average cap: 8.205597877502441\n",
      "Number capped: 10\n",
      "Average cap: 2.6692655086517334\n",
      "Number capped: 10\n",
      "Average cap: 3.959054946899414\n",
      "Number capped: 4\n",
      "Average cap: 10.108455657958984\n",
      "Number capped: 6\n",
      "Average cap: 6.528411865234375\n",
      "Number capped: 4\n",
      "Average cap: 1.60811185836792\n",
      "Number capped: 5\n",
      "Average cap: 5.454056739807129\n",
      "Number capped: 7\n",
      "Average cap: 5.631980895996094\n",
      "Number capped: 9\n",
      "Average cap: 3.453294515609741\n",
      "Number capped: 4\n",
      "Average cap: 13.733177185058594\n",
      "Number capped: 6\n",
      "Average cap: 5.334065914154053\n",
      "Number capped: 3\n",
      "Average cap: 8.568095207214355\n",
      "Number capped: 5\n",
      "Average cap: 8.477063179016113\n",
      "Number capped: 7\n",
      "Average cap: 1.5747026205062866\n",
      "Number capped: 7\n",
      "Average cap: 3.2573935985565186\n",
      "Number capped: 2\n",
      "Average cap: 29.90862464904785\n",
      "Number capped: 5\n",
      "Average cap: 8.417343139648438\n",
      "Number capped: 4\n",
      "Average cap: 3.6923322677612305\n",
      "Number capped: 4\n",
      "Average cap: 1.1187158823013306\n",
      "Number capped: 4\n",
      "Average cap: 9.760201454162598\n",
      "Number capped: 4\n",
      "Average cap: 9.974444389343262\n",
      "Number capped: 6\n",
      "Average cap: 1.2062697410583496\n",
      "Number capped: 3\n",
      "Average cap: 14.297134399414062\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 6\n",
      "Average cap: 15.411049842834473\n",
      "Number capped: 5\n",
      "Average cap: 0.7407999038696289\n",
      "Number capped: 7\n",
      "Average cap: 4.888909816741943\n",
      "Number capped: 12\n",
      "Average cap: 2.8332908153533936\n",
      "Number capped: 12\n",
      "Average cap: 2.947343587875366\n",
      "Number capped: 5\n",
      "Average cap: 1.3963178396224976\n",
      "Number capped: 5\n",
      "Average cap: 11.015719413757324\n",
      "Number capped: 5\n",
      "Average cap: 7.267792701721191\n",
      "Number capped: 10\n",
      "Average cap: 6.01207160949707\n",
      "Number capped: 9\n",
      "Average cap: 0.46951356530189514\n",
      "Number capped: 8\n",
      "Average cap: 4.172747611999512\n",
      "Number capped: 1\n",
      "Average cap: 51.84574508666992\n",
      "Number capped: 11\n",
      "Average cap: 3.7877275943756104\n",
      "Number capped: 2\n",
      "Average cap: 0.7118314504623413\n",
      "Number capped: 5\n",
      "Average cap: 2.0228257179260254\n",
      "Number capped: 6\n",
      "Average cap: 6.619164943695068\n",
      "Number capped: 2\n",
      "Average cap: 35.3641357421875\n",
      "Number capped: 10\n",
      "Average cap: 1.2908025979995728\n",
      "Number capped: 5\n",
      "Average cap: 0.08488745987415314\n",
      "Number capped: 4\n",
      "Average cap: 3.1225807666778564\n",
      "Number capped: 8\n",
      "Average cap: 5.172569274902344\n",
      "Number capped: 4\n",
      "Average cap: 0.02956932783126831\n",
      "Number capped: 2\n",
      "Average cap: 19.925071716308594\n",
      "Number capped: 6\n",
      "Average cap: 6.266123294830322\n",
      "Number capped: 7\n",
      "Average cap: 5.550206661224365\n",
      "Number capped: 3\n",
      "Average cap: 9.082015037536621\n",
      "Number capped: 5\n",
      "Average cap: 9.172128677368164\n",
      "Number capped: 4\n",
      "Average cap: 9.837713241577148\n",
      "Number capped: 6\n",
      "Average cap: 6.352505207061768\n",
      "Number capped: 5\n",
      "Average cap: 0.10622911155223846\n",
      "Number capped: 3\n",
      "Average cap: 20.655614852905273\n",
      "Number capped: 10\n",
      "Average cap: 2.321402072906494\n",
      "Number capped: 3\n",
      "Average cap: 4.084531784057617\n",
      "Number capped: 6\n",
      "Average cap: 1.0854599475860596\n",
      "Number capped: 5\n",
      "Average cap: 7.748924255371094\n",
      "Number capped: 7\n",
      "Average cap: 4.40327787399292\n",
      "Number capped: 8\n",
      "Average cap: 5.409097194671631\n",
      "Number capped: 6\n",
      "Average cap: 6.455109119415283\n",
      "Number capped: 8\n",
      "Average cap: 5.623051166534424\n",
      "Number capped: 8\n",
      "Average cap: 5.763862609863281\n",
      "Number capped: 6\n",
      "Average cap: 0.10327857732772827\n",
      "Number capped: 5\n",
      "Average cap: 7.653378486633301\n",
      "Number capped: 11\n",
      "Average cap: 0.6874311566352844\n",
      "Number capped: 7\n",
      "Average cap: 4.008224010467529\n",
      "Number capped: 12\n",
      "Average cap: 1.8111599683761597\n",
      "Number capped: 7\n",
      "Average cap: 1.2968292236328125\n",
      "Number capped: 5\n",
      "Average cap: 8.505475997924805\n",
      "Number capped: 3\n",
      "Average cap: 13.031250953674316\n",
      "Number capped: 7\n",
      "Average cap: 1.5456782579421997\n",
      "Number capped: 5\n",
      "Average cap: 8.71420669555664\n",
      "Number capped: 9\n",
      "Average cap: 0.07789541780948639\n",
      "Number capped: 4\n",
      "Average cap: 9.69311237335205\n",
      "Number capped: 5\n",
      "Average cap: 7.780709266662598\n",
      "Number capped: 6\n",
      "Average cap: 1.3726106882095337\n",
      "Number capped: 6\n",
      "Average cap: 7.004811763763428\n",
      "Number capped: 5\n",
      "Average cap: 7.6912431716918945\n",
      "Number capped: 5\n",
      "Average cap: 0.0015738203655928373\n",
      "Number capped: 3\n",
      "Average cap: 12.103439331054688\n",
      "Number capped: 2\n",
      "Average cap: 18.80693244934082\n",
      "Number capped: 4\n",
      "Average cap: 9.329811096191406\n",
      "Number capped: 5\n",
      "Average cap: 7.57372522354126\n",
      "Number capped: 8\n",
      "Average cap: 4.9754557609558105\n",
      "Number capped: 4\n",
      "Average cap: 10.044283866882324\n",
      "Number capped: 6\n",
      "Average cap: 6.871927738189697\n",
      "Number capped: 8\n",
      "Average cap: 4.755410671234131\n",
      "Number capped: 3\n",
      "Average cap: 1.2807772159576416\n",
      "Number capped: 8\n",
      "Average cap: 4.7279510498046875\n",
      "Number capped: 7\n",
      "Average cap: 5.72333288192749\n",
      "Number capped: 3\n",
      "Average cap: 15.176529884338379\n",
      "Number capped: 8\n",
      "Average cap: 4.779830455780029\n",
      "Number capped: 2\n",
      "Average cap: 18.70071792602539\n",
      "Number capped: 5\n",
      "Average cap: 1.8242385387420654\n",
      "Number capped: 6\n",
      "Average cap: 1.9234784841537476\n",
      "Number capped: 6\n",
      "Average cap: 5.9638519287109375\n",
      "Number capped: 3\n",
      "Average cap: 3.893383026123047\n",
      "Number capped: 4\n",
      "Average cap: 3.8563759326934814\n",
      "Number capped: 8\n",
      "Average cap: 3.3359827995300293\n",
      "Number capped: 3\n",
      "Average cap: 11.23812198638916\n",
      "Number capped: 3\n",
      "Average cap: 9.73083209991455\n",
      "Number capped: 11\n",
      "Average cap: 0.5891332626342773\n",
      "Number capped: 9\n",
      "Average cap: 0.6867789626121521\n",
      "Number capped: 7\n",
      "Average cap: 5.363499641418457\n",
      "Number capped: 8\n",
      "Average cap: 4.755439281463623\n",
      "Number capped: 7\n",
      "Average cap: 0.35094770789146423\n",
      "Number capped: 6\n",
      "Average cap: 5.912113666534424\n",
      "Number capped: 8\n",
      "Average cap: 1.1260251998901367\n",
      "Number capped: 5\n",
      "Average cap: 0.5860950946807861\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 8\n",
      "Average cap: 0.9526773691177368\n",
      "Number capped: 6\n",
      "Average cap: 2.789165496826172\n",
      "Number capped: 6\n",
      "Average cap: 6.20941162109375\n",
      "Number capped: 4\n",
      "Average cap: 6.79625129699707\n",
      "Number capped: 5\n",
      "Average cap: 7.875995635986328\n",
      "Number capped: 5\n",
      "Average cap: 0.8061906695365906\n",
      "Number capped: 4\n",
      "Average cap: 6.1317291259765625\n",
      "Number capped: 7\n",
      "Average cap: 4.686919212341309\n",
      "Number capped: 2\n",
      "Average cap: 0.7142198085784912\n",
      "Number capped: 4\n",
      "Average cap: 8.991876602172852\n",
      "Number capped: 4\n",
      "Average cap: 0.08632690459489822\n",
      "Number capped: 6\n",
      "Average cap: 6.407724857330322\n",
      "Number capped: 5\n",
      "Average cap: 15.392306327819824\n",
      "Number capped: 6\n",
      "Average cap: 6.528097629547119\n",
      "Number capped: 6\n",
      "Average cap: 0.6608455777168274\n",
      "Number capped: 6\n",
      "Average cap: 2.477908134460449\n",
      "Number capped: 4\n",
      "Average cap: 0.7226251363754272\n",
      "Number capped: 8\n",
      "Average cap: 4.6285295486450195\n",
      "Number capped: 2\n",
      "Average cap: 0.863885223865509\n",
      "Number capped: 6\n",
      "Average cap: 0.703233003616333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 4\n",
      "Average cap: 9.421307563781738\n",
      "Number capped: 7\n",
      "Average cap: 4.8935956954956055\n",
      "Number capped: 3\n",
      "Average cap: 0.16126935184001923\n",
      "Number capped: 6\n",
      "Average cap: 2.023008346557617\n",
      "Number capped: 8\n",
      "Average cap: 0.2428039014339447\n",
      "Number capped: 5\n",
      "Average cap: 4.592976093292236\n",
      "Number capped: 2\n",
      "Average cap: 25.24976348876953\n",
      "Number capped: 9\n",
      "Average cap: 4.158068656921387\n",
      "Number capped: 5\n",
      "Average cap: 7.121281623840332\n",
      "Number capped: 6\n",
      "Average cap: 2.626572847366333\n",
      "Number capped: 8\n",
      "Average cap: 5.385320663452148\n",
      "Number capped: 6\n",
      "Average cap: 6.580637454986572\n",
      "Number capped: 3\n",
      "Average cap: 9.228236198425293\n",
      "Number capped: 8\n",
      "Average cap: 1.5441635847091675\n",
      "Number capped: 2\n",
      "Average cap: 18.781442642211914\n",
      "Number capped: 5\n",
      "Average cap: 3.9496302604675293\n",
      "Number capped: 6\n",
      "Average cap: 1.7904820442199707\n",
      "Number capped: 7\n",
      "Average cap: 5.399494647979736\n",
      "Number capped: 5\n",
      "Average cap: 7.084173679351807\n",
      "Number capped: 3\n",
      "Average cap: 14.80107593536377\n",
      "Number capped: 6\n",
      "Average cap: 7.249871730804443\n",
      "Number capped: 7\n",
      "Average cap: 0.9100333452224731\n",
      "Number capped: 5\n",
      "Average cap: 1.0345005989074707\n",
      "Number capped: 3\n",
      "Average cap: 13.37133502960205\n",
      "Number capped: 11\n",
      "Average cap: 3.134524345397949\n",
      "Number capped: 4\n",
      "Average cap: 0.83843594789505\n",
      "Number capped: 9\n",
      "Average cap: 0.6769892573356628\n",
      "Number capped: 6\n",
      "Average cap: 3.2430531978607178\n",
      "Number capped: 3\n",
      "Average cap: 1.578377366065979\n",
      "Number capped: 5\n",
      "Average cap: 5.1753129959106445\n",
      "Number capped: 4\n",
      "Average cap: 10.745851516723633\n",
      "Number capped: 8\n",
      "Average cap: 4.887573719024658\n",
      "Number capped: 5\n",
      "Average cap: 7.606254577636719\n",
      "Number capped: 5\n",
      "Average cap: 8.875112533569336\n",
      "Number capped: 11\n",
      "Average cap: 2.803997039794922\n",
      "Number capped: 4\n",
      "Average cap: 16.769329071044922\n",
      "Number capped: 6\n",
      "Average cap: 6.308967113494873\n",
      "Number capped: 5\n",
      "Average cap: 0.5939756631851196\n",
      "Number capped: 4\n",
      "Average cap: 9.452698707580566\n",
      "Number capped: 5\n",
      "Average cap: 8.497079849243164\n",
      "Number capped: 2\n",
      "Average cap: 21.16097640991211\n",
      "Number capped: 7\n",
      "Average cap: 5.553853511810303\n",
      "Number capped: 12\n",
      "Average cap: 4.601219177246094\n",
      "Number capped: 8\n",
      "Average cap: 5.791842937469482\n",
      "Number capped: 3\n",
      "Average cap: 7.70753812789917\n",
      "Number capped: 7\n",
      "Average cap: 0.4135429859161377\n",
      "Number capped: 5\n",
      "Average cap: 7.4800519943237305\n",
      "Number capped: 3\n",
      "Average cap: 12.791522979736328\n",
      "Number capped: 3\n",
      "Average cap: 10.317115783691406\n",
      "Number capped: 5\n",
      "Average cap: 7.1066765785217285\n",
      "Number capped: 8\n",
      "Average cap: 0.46259334683418274\n",
      "Number capped: 7\n",
      "Average cap: 10.781962394714355\n",
      "Number capped: 3\n",
      "Average cap: 0.13961021602153778\n",
      "Number capped: 7\n",
      "Average cap: 3.297852039337158\n",
      "Number capped: 6\n",
      "Average cap: 7.116123199462891\n",
      "Number capped: 7\n",
      "Average cap: 5.715401649475098\n",
      "Number capped: 5\n",
      "Average cap: 7.865048408508301\n",
      "Number capped: 3\n",
      "Average cap: 2.7702484130859375\n",
      "Number capped: 4\n",
      "Average cap: 8.827264785766602\n",
      "Number capped: 6\n",
      "Average cap: 6.642063140869141\n",
      "Number capped: 3\n",
      "Average cap: 12.751452445983887\n",
      "Number capped: 5\n",
      "Average cap: 11.139666557312012\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 14\n",
      "Average cap: 0.599971354007721\n",
      "Number capped: 5\n",
      "Average cap: 7.867613792419434\n",
      "Number capped: 10\n",
      "Average cap: 4.972033977508545\n",
      "Number capped: 4\n",
      "Average cap: 1.922850489616394\n",
      "Number capped: 5\n",
      "Average cap: 7.267779350280762\n",
      "Number capped: 7\n",
      "Average cap: 5.853993892669678\n",
      "Number capped: 2\n",
      "Average cap: 1.4900678396224976\n",
      "Number capped: 3\n",
      "Average cap: 5.166966915130615\n",
      "Number capped: 3\n",
      "Average cap: 2.886061668395996\n",
      "Number capped: 9\n",
      "Average cap: 16.36432647705078\n",
      "Number capped: 8\n",
      "Average cap: 1.14638352394104\n",
      "Number capped: 7\n",
      "Average cap: 5.5321574211120605\n",
      "Number capped: 4\n",
      "Average cap: 9.620060920715332\n",
      "Number capped: 7\n",
      "Average cap: 2.713258743286133\n",
      "Number capped: 5\n",
      "Average cap: 7.58810567855835\n",
      "Number capped: 7\n",
      "Average cap: 6.63544225692749\n",
      "Number capped: 6\n",
      "Average cap: 12.291558265686035\n",
      "Number capped: 13\n",
      "Average cap: 2.5326995849609375\n",
      "Number capped: 4\n",
      "Average cap: 9.836596488952637\n",
      "Number capped: 6\n",
      "Average cap: 0.9775480628013611\n",
      "Number capped: 5\n",
      "Average cap: 7.604567050933838\n",
      "Number capped: 6\n",
      "Average cap: 0.9068307876586914\n",
      "Number capped: 4\n",
      "Average cap: 2.844118118286133\n",
      "Number capped: 4\n",
      "Average cap: 8.69636058807373\n",
      "Number capped: 1\n",
      "Average cap: 38.75106430053711\n",
      "Number capped: 11\n",
      "Average cap: 3.604027509689331\n",
      "Number capped: 5\n",
      "Average cap: 8.277631759643555\n",
      "Number capped: 4\n",
      "Average cap: 0.594636857509613\n",
      "Number capped: 7\n",
      "Average cap: 0.5973495244979858\n",
      "Number capped: 4\n",
      "Average cap: 0.8280866742134094\n",
      "Number capped: 7\n",
      "Average cap: 0.579760730266571\n",
      "Number capped: 4\n",
      "Average cap: 10.153159141540527\n",
      "Number capped: 4\n",
      "Average cap: 9.33674430847168\n",
      "Number capped: 3\n",
      "Average cap: 12.985068321228027\n",
      "Number capped: 5\n",
      "Average cap: 5.779782295227051\n",
      "Number capped: 12\n",
      "Average cap: 3.1464312076568604\n",
      "Number capped: 3\n",
      "Average cap: 1.8464411497116089\n",
      "Number capped: 8\n",
      "Average cap: 4.572698593139648\n",
      "Number capped: 4\n",
      "Average cap: 1.9427093267440796\n",
      "Number capped: 4\n",
      "Average cap: 3.4530487060546875\n",
      "Number capped: 3\n",
      "Average cap: 0.6559421420097351\n",
      "Number capped: 5\n",
      "Average cap: 9.242818832397461\n",
      "Number capped: 3\n",
      "Average cap: 1.120422124862671\n",
      "Number capped: 3\n",
      "Average cap: 12.295899391174316\n",
      "Number capped: 5\n",
      "Average cap: 7.452770233154297\n",
      "Number capped: 6\n",
      "Average cap: 6.529298305511475\n",
      "Number capped: 4\n",
      "Average cap: 3.963681697845459\n",
      "Number capped: 4\n",
      "Average cap: 6.995274066925049\n",
      "Number capped: 4\n",
      "Average cap: 10.132657051086426\n",
      "Number capped: 7\n",
      "Average cap: 4.713187217712402\n",
      "Number capped: 4\n",
      "Average cap: 8.807636260986328\n",
      "Number capped: 8\n",
      "Average cap: 2.02066707611084\n",
      "Number capped: 3\n",
      "Average cap: 3.0917093753814697\n",
      "Number capped: 5\n",
      "Average cap: 7.547043800354004\n",
      "Number capped: 8\n",
      "Average cap: 1.1777262687683105\n",
      "Number capped: 4\n",
      "Average cap: 0.9461562037467957\n",
      "Number capped: 6\n",
      "Average cap: 0.7042704224586487\n",
      "Number capped: 8\n",
      "Average cap: 4.999896049499512\n",
      "Number capped: 6\n",
      "Average cap: 6.142154216766357\n",
      "Number capped: 3\n",
      "Average cap: 12.36859130859375\n",
      "Number capped: 3\n",
      "Average cap: 15.847183227539062\n",
      "Number capped: 9\n",
      "Average cap: 3.75783109664917\n",
      "Number capped: 6\n",
      "Average cap: 1.1249552965164185\n",
      "Number capped: 2\n",
      "Average cap: 4.886077880859375\n",
      "Number capped: 8\n",
      "Average cap: 4.138655185699463\n",
      "Number capped: 7\n",
      "Average cap: 5.309228420257568\n",
      "Number capped: 5\n",
      "Average cap: 7.657200813293457\n",
      "Number capped: 3\n",
      "Average cap: 12.322976112365723\n",
      "Number capped: 2\n",
      "Average cap: 10.415634155273438\n",
      "Number capped: 3\n",
      "Average cap: 12.82626724243164\n",
      "Number capped: 4\n",
      "Average cap: 1.9165010452270508\n",
      "Number capped: 3\n",
      "Average cap: 12.095779418945312\n",
      "Number capped: 5\n",
      "Average cap: 7.654444694519043\n",
      "Number capped: 3\n",
      "Average cap: 10.516318321228027\n",
      "Number capped: 9\n",
      "Average cap: 0.4298437237739563\n",
      "Number capped: 3\n",
      "Average cap: 0.43247196078300476\n",
      "Number capped: 4\n",
      "Average cap: 10.359333992004395\n",
      "Number capped: 5\n",
      "Average cap: 7.502986907958984\n",
      "Number capped: 6\n",
      "Average cap: 0.5786409974098206\n",
      "Number capped: 9\n",
      "Average cap: 4.550544261932373\n",
      "Number capped: 3\n",
      "Average cap: 2.1972570419311523\n",
      "Number capped: 6\n",
      "Average cap: 0.6231871247291565\n",
      "Number capped: 6\n",
      "Average cap: 1.7945369482040405\n",
      "Number capped: 1\n",
      "Average cap: 36.69941329956055\n",
      "Number capped: 8\n",
      "Average cap: 6.416938781738281\n",
      "Number capped: 6\n",
      "Average cap: 5.6688456535339355\n",
      "Number capped: 2\n",
      "Average cap: 0.6315559148788452\n",
      "Number capped: 7\n",
      "Average cap: 3.7886908054351807\n",
      "Number capped: 4\n",
      "Average cap: 1.6564998626708984\n",
      "Number capped: 3\n",
      "Average cap: 12.039748191833496\n",
      "Number capped: 3\n",
      "Average cap: 0.7476231455802917\n",
      "Number capped: 6\n",
      "Average cap: 14.200945854187012\n",
      "Number capped: 3\n",
      "Average cap: 14.371611595153809\n",
      "Number capped: 1\n",
      "Average cap: 30.212947845458984\n",
      "Number capped: 6\n",
      "Average cap: 7.429349422454834\n",
      "Number capped: 6\n",
      "Average cap: 0.6710970997810364\n",
      "Number capped: 5\n",
      "Average cap: 0.5883911848068237\n",
      "Number capped: 6\n",
      "Average cap: 2.5805981159210205\n",
      "Number capped: 5\n",
      "Average cap: 1.3099806308746338\n",
      "Number capped: 4\n",
      "Average cap: 10.014154434204102\n",
      "Number capped: 6\n",
      "Average cap: 6.260269641876221\n",
      "Number capped: 8\n",
      "Average cap: 5.4449968338012695\n",
      "Number capped: 4\n",
      "Average cap: 0.024852976202964783\n",
      "Number capped: 6\n",
      "Average cap: 0.06333356350660324\n",
      "Number capped: 4\n",
      "Average cap: 9.376428604125977\n",
      "Number capped: 2\n",
      "Average cap: 22.738798141479492\n",
      "Number capped: 5\n",
      "Average cap: 13.196542739868164\n",
      "Number capped: 7\n",
      "Average cap: 5.608850002288818\n",
      "Number capped: 5\n",
      "Average cap: 2.468844175338745\n",
      "Number capped: 3\n",
      "Average cap: 23.309478759765625\n",
      "Number capped: 5\n",
      "Average cap: 1.5648267269134521\n",
      "Number capped: 9\n",
      "Average cap: 4.085791110992432\n",
      "Number capped: 9\n",
      "Average cap: 0.1925026774406433\n",
      "Number capped: 2\n",
      "Average cap: 17.933622360229492\n",
      "Number capped: 1\n",
      "Average cap: 3.7727174758911133\n",
      "Number capped: 9\n",
      "Average cap: 3.588582754135132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 7\n",
      "Average cap: 8.152981758117676\n",
      "Number capped: 6\n",
      "Average cap: 5.66575813293457\n",
      "Number capped: 2\n",
      "Average cap: 18.447559356689453\n",
      "Number capped: 5\n",
      "Average cap: 5.892594337463379\n",
      "Number capped: 4\n",
      "Average cap: 8.462247848510742\n",
      "Number capped: 3\n",
      "Average cap: 12.201653480529785\n",
      "Number capped: 6\n",
      "Average cap: 0.998772919178009\n",
      "Number capped: 6\n",
      "Average cap: 0.6913126111030579\n",
      "Number capped: 7\n",
      "Average cap: 1.0980135202407837\n",
      "Number capped: 6\n",
      "Average cap: 6.033149719238281\n",
      "Number capped: 5\n",
      "Average cap: 9.425240516662598\n",
      "Number capped: 3\n",
      "Average cap: 4.24251127243042\n",
      "Number capped: 7\n",
      "Average cap: 0.32804393768310547\n",
      "Number capped: 6\n",
      "Average cap: 3.0298588275909424\n",
      "Number capped: 3\n",
      "Average cap: 12.103866577148438\n",
      "Number capped: 5\n",
      "Average cap: 7.502677917480469\n",
      "Number capped: 6\n",
      "Average cap: 8.558431625366211\n",
      "Number capped: 6\n",
      "Average cap: 6.350425720214844\n",
      "Number capped: 7\n",
      "Average cap: 1.2908594608306885\n",
      "Number capped: 4\n",
      "Average cap: 29.16094398498535\n",
      "Number capped: 7\n",
      "Average cap: 20.237071990966797\n",
      "Number capped: 6\n",
      "Average cap: 0.7452040314674377\n",
      "Number capped: 5\n",
      "Average cap: 7.561690330505371\n",
      "Number capped: 4\n",
      "Average cap: 2.0666956901550293\n",
      "Number capped: 6\n",
      "Average cap: 8.249669075012207\n",
      "Number capped: 1\n",
      "Average cap: 5.575870513916016\n",
      "Number capped: 3\n",
      "Average cap: 0.8110666275024414\n",
      "Number capped: 2\n",
      "Average cap: 17.944311141967773\n",
      "Number capped: 3\n",
      "Average cap: 12.292157173156738\n",
      "Number capped: 7\n",
      "Average cap: 5.498442649841309\n",
      "Number capped: 6\n",
      "Average cap: 4.258870601654053\n",
      "Number capped: 4\n",
      "Average cap: 5.239553451538086\n",
      "Number capped: 7\n",
      "Average cap: 1.0499346256256104\n",
      "Number capped: 5\n",
      "Average cap: 0.8025109171867371\n",
      "Number capped: 3\n",
      "Average cap: 12.545494079589844\n",
      "Number capped: 3\n",
      "Average cap: 3.4547271728515625\n",
      "Number capped: 6\n",
      "Average cap: 4.710928916931152\n",
      "Number capped: 2\n",
      "Average cap: 1.6193795204162598\n",
      "Number capped: 5\n",
      "Average cap: 0.15045860409736633\n",
      "Number capped: 2\n",
      "Average cap: 18.63058853149414\n",
      "Number capped: 1\n",
      "Average cap: 0.687493085861206\n",
      "\n",
      "Test set: Avg. loss: 0.0005868429839611054, AUC: 0.855867\n",
      "\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 4\n",
      "Average cap: 8.922800064086914\n",
      "Number capped: 9\n",
      "Average cap: 0.21556325256824493\n",
      "Number capped: 8\n",
      "Average cap: 1.2817845344543457\n",
      "Number capped: 5\n",
      "Average cap: 7.272631645202637\n",
      "Number capped: 6\n",
      "Average cap: 0.012987366877496243\n",
      "Number capped: 6\n",
      "Average cap: 6.1357197761535645\n",
      "Number capped: 8\n",
      "Average cap: 4.509408950805664\n",
      "Number capped: 6\n",
      "Average cap: 6.0450520515441895\n",
      "Number capped: 6\n",
      "Average cap: 6.142751216888428\n",
      "Number capped: 4\n",
      "Average cap: 9.29542350769043\n",
      "Number capped: 4\n",
      "Average cap: 7.992995738983154\n",
      "Number capped: 2\n",
      "Average cap: 0.5987954139709473\n",
      "Number capped: 8\n",
      "Average cap: 13.548707008361816\n",
      "Number capped: 1\n",
      "Average cap: 3.252591609954834\n",
      "Number capped: 1\n",
      "Average cap: 93.44461059570312\n",
      "Number capped: 5\n",
      "Average cap: 7.5216522216796875\n",
      "Number capped: 3\n",
      "Average cap: 12.104957580566406\n",
      "Number capped: 6\n",
      "Average cap: 3.936248540878296\n",
      "Number capped: 6\n",
      "Average cap: 8.000327110290527\n",
      "Number capped: 5\n",
      "Average cap: 5.764976501464844\n",
      "Number capped: 5\n",
      "Average cap: 6.053487777709961\n",
      "Number capped: 5\n",
      "Average cap: 7.258177280426025\n",
      "Number capped: 2\n",
      "Average cap: 18.575429916381836\n",
      "Number capped: 5\n",
      "Average cap: 0.07868332415819168\n",
      "Number capped: 11\n",
      "Average cap: 0.0011886085849255323\n",
      "Number capped: 4\n",
      "Average cap: 10.576909065246582\n",
      "Number capped: 3\n",
      "Average cap: 17.47814178466797\n",
      "Number capped: 5\n",
      "Average cap: 7.363774299621582\n",
      "Number capped: 12\n",
      "Average cap: 3.2195842266082764\n",
      "Number capped: 5\n",
      "Average cap: 8.630887031555176\n",
      "Number capped: 4\n",
      "Average cap: 9.047150611877441\n",
      "Number capped: 5\n",
      "Average cap: 7.9075422286987305\n",
      "Number capped: 3\n",
      "Average cap: 11.50062084197998\n",
      "Number capped: 4\n",
      "Average cap: 10.033638954162598\n",
      "Number capped: 7\n",
      "Average cap: 4.727207660675049\n",
      "Number capped: 5\n",
      "Average cap: 7.225551605224609\n",
      "Number capped: 5\n",
      "Average cap: 7.253063201904297\n",
      "Number capped: 6\n",
      "Average cap: 0.8503573536872864\n",
      "Number capped: 3\n",
      "Average cap: 11.555270195007324\n",
      "Number capped: 4\n",
      "Average cap: 0.7779817581176758\n",
      "Number capped: 6\n",
      "Average cap: 6.0443596839904785\n",
      "Number capped: 4\n",
      "Average cap: 10.911308288574219\n",
      "Number capped: 2\n",
      "Average cap: 0.5630253553390503\n",
      "Number capped: 6\n",
      "Average cap: 6.001183032989502\n",
      "Number capped: 12\n",
      "Average cap: 3.1572799682617188\n",
      "Number capped: 5\n",
      "Average cap: 5.72421932220459\n",
      "Number capped: 8\n",
      "Average cap: 4.087141036987305\n",
      "Number capped: 6\n",
      "Average cap: 0.6268753409385681\n",
      "Number capped: 6\n",
      "Average cap: 6.334298610687256\n",
      "Number capped: 8\n",
      "Average cap: 0.5713441371917725\n",
      "Number capped: 6\n",
      "Average cap: 7.919065952301025\n",
      "Number capped: 3\n",
      "Average cap: 12.248017311096191\n",
      "Number capped: 6\n",
      "Average cap: 6.858117580413818\n",
      "Number capped: 2\n",
      "Average cap: 22.064502716064453\n",
      "Number capped: 8\n",
      "Average cap: 2.1437435150146484\n",
      "Number capped: 5\n",
      "Average cap: 6.920233249664307\n",
      "Number capped: 3\n",
      "Average cap: 11.50527286529541\n",
      "Number capped: 3\n",
      "Average cap: 29.028366088867188\n",
      "Number capped: 2\n",
      "Average cap: 22.653722763061523\n",
      "Number capped: 3\n",
      "Average cap: 13.08487606048584\n",
      "Number capped: 8\n",
      "Average cap: 6.015827178955078\n",
      "Number capped: 5\n",
      "Average cap: 21.23118782043457\n",
      "Number capped: 7\n",
      "Average cap: 9.113062858581543\n",
      "Number capped: 7\n",
      "Average cap: 6.721546649932861\n",
      "Number capped: 1\n",
      "Average cap: 35.94799041748047\n",
      "Number capped: 5\n",
      "Average cap: 7.533896446228027\n",
      "Number capped: 9\n",
      "Average cap: 8.223540306091309\n",
      "Number capped: 4\n",
      "Average cap: 9.060623168945312\n",
      "Number capped: 5\n",
      "Average cap: 0.3983461558818817\n",
      "Number capped: 2\n",
      "Average cap: 0.18790268898010254\n",
      "Number capped: 5\n",
      "Average cap: 13.184684753417969\n",
      "Number capped: 5\n",
      "Average cap: 6.8036370277404785\n",
      "Number capped: 4\n",
      "Average cap: 1.5609010457992554\n",
      "Number capped: 7\n",
      "Average cap: 4.705612659454346\n",
      "Number capped: 6\n",
      "Average cap: 6.2456889152526855\n",
      "Number capped: 7\n",
      "Average cap: 3.5244553089141846\n",
      "Number capped: 3\n",
      "Average cap: 11.485651969909668\n",
      "Number capped: 6\n",
      "Average cap: 6.158621311187744\n",
      "Number capped: 5\n",
      "Average cap: 0.0001409045362379402\n",
      "Number capped: 2\n",
      "Average cap: 7.0292510986328125\n",
      "Number capped: 7\n",
      "Average cap: 5.183567523956299\n",
      "Number capped: 3\n",
      "Average cap: 13.651058197021484\n",
      "Number capped: 10\n",
      "Average cap: 3.8682987689971924\n",
      "Number capped: 7\n",
      "Average cap: 6.491352558135986\n",
      "Number capped: 4\n",
      "Average cap: 6.313703536987305\n",
      "Number capped: 10\n",
      "Average cap: 3.978269577026367\n",
      "Number capped: 8\n",
      "Average cap: 1.935923457145691\n",
      "Number capped: 3\n",
      "Average cap: 2.3859689235687256\n",
      "Number capped: 10\n",
      "Average cap: 4.5412797927856445\n",
      "Number capped: 5\n",
      "Average cap: 6.940428256988525\n",
      "Number capped: 5\n",
      "Average cap: 9.116869926452637\n",
      "Number capped: 7\n",
      "Average cap: 5.12970495223999\n",
      "Number capped: 5\n",
      "Average cap: 8.93321704864502\n",
      "Number capped: 6\n",
      "Average cap: 3.9368371963500977\n",
      "Number capped: 2\n",
      "Average cap: 61.216609954833984\n",
      "Number capped: 8\n",
      "Average cap: 3.9908812046051025\n",
      "Number capped: 6\n",
      "Average cap: 6.940879821777344\n",
      "Number capped: 4\n",
      "Average cap: 10.497078895568848\n",
      "Number capped: 9\n",
      "Average cap: 5.6505208015441895\n",
      "Number capped: 2\n",
      "Average cap: 18.343177795410156\n",
      "Number capped: 5\n",
      "Average cap: 6.674450874328613\n",
      "Number capped: 5\n",
      "Average cap: 0.007515758275985718\n",
      "Number capped: 1\n",
      "Average cap: 132.98297119140625\n",
      "Number capped: 5\n",
      "Average cap: 0.4700734615325928\n",
      "Number capped: 5\n",
      "Average cap: 1.752755880355835\n",
      "Number capped: 4\n",
      "Average cap: 10.849495887756348\n",
      "Number capped: 10\n",
      "Average cap: 2.786163568496704\n",
      "Number capped: 4\n",
      "Average cap: 0.2469552606344223\n",
      "Number capped: 14\n",
      "Average cap: 2.0105340480804443\n",
      "Number capped: 6\n",
      "Average cap: 2.5046699047088623\n",
      "Number capped: 5\n",
      "Average cap: 0.10233211517333984\n",
      "Number capped: 3\n",
      "Average cap: 5.625598907470703\n",
      "Number capped: 7\n",
      "Average cap: 0.28995180130004883\n",
      "Number capped: 4\n",
      "Average cap: 9.195354461669922\n",
      "Number capped: 4\n",
      "Average cap: 8.653936386108398\n",
      "Number capped: 4\n",
      "Average cap: 9.260068893432617\n",
      "Number capped: 3\n",
      "Average cap: 12.353596687316895\n",
      "Number capped: 9\n",
      "Average cap: 0.6105819940567017\n",
      "Number capped: 6\n",
      "Average cap: 1.4133366346359253\n",
      "Number capped: 5\n",
      "Average cap: 7.330430507659912\n",
      "Number capped: 2\n",
      "Average cap: 21.35294532775879\n",
      "Number capped: 6\n",
      "Average cap: 1.8943229913711548\n",
      "Number capped: 7\n",
      "Average cap: 4.895100116729736\n",
      "Number capped: 5\n",
      "Average cap: 8.486231803894043\n",
      "Number capped: 3\n",
      "Average cap: 12.344149589538574\n",
      "Number capped: 3\n",
      "Average cap: 1.1953433752059937\n",
      "Number capped: 4\n",
      "Average cap: 1.9409167766571045\n",
      "Number capped: 4\n",
      "Average cap: 8.786434173583984\n",
      "Number capped: 4\n",
      "Average cap: 1.4342424869537354\n",
      "Number capped: 2\n",
      "Average cap: 17.36898422241211\n",
      "Number capped: 6\n",
      "Average cap: 6.105727672576904\n",
      "Number capped: 2\n",
      "Average cap: 18.536100387573242\n",
      "Number capped: 6\n",
      "Average cap: 0.685764729976654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 9\n",
      "Average cap: 8.51706314086914\n",
      "Number capped: 2\n",
      "Average cap: 17.420942306518555\n",
      "Number capped: 5\n",
      "Average cap: 8.076733589172363\n",
      "Number capped: 3\n",
      "Average cap: 12.43546199798584\n",
      "Number capped: 7\n",
      "Average cap: 21.426454544067383\n",
      "Number capped: 9\n",
      "Average cap: 3.7007572650909424\n",
      "Number capped: 2\n",
      "Average cap: 60.79058074951172\n",
      "Number capped: 9\n",
      "Average cap: 0.6036336421966553\n",
      "Number capped: 2\n",
      "Average cap: 17.96044921875\n",
      "Number capped: 1\n",
      "Average cap: 1.8936817646026611\n",
      "Number capped: 8\n",
      "Average cap: 1.7585715055465698\n",
      "Number capped: 5\n",
      "Average cap: 6.653563022613525\n",
      "Number capped: 7\n",
      "Average cap: 4.5335774421691895\n",
      "Number capped: 6\n",
      "Average cap: 3.3581724166870117\n",
      "Number capped: 4\n",
      "Average cap: 14.722047805786133\n",
      "Number capped: 6\n",
      "Average cap: 0.049289729446172714\n",
      "Number capped: 6\n",
      "Average cap: 8.355987548828125\n",
      "Number capped: 6\n",
      "Average cap: 1.8398655652999878\n",
      "Number capped: 1\n",
      "Average cap: 75.48904418945312\n",
      "Number capped: 5\n",
      "Average cap: 0.7464081645011902\n",
      "Number capped: 5\n",
      "Average cap: 2.1216177940368652\n",
      "Number capped: 3\n",
      "Average cap: 4.106180667877197\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 11\n",
      "Average cap: 4.652340412139893\n",
      "Number capped: 9\n",
      "Average cap: 0.07823903113603592\n",
      "Number capped: 4\n",
      "Average cap: 0.3411922752857208\n",
      "Number capped: 13\n",
      "Average cap: 2.4766175746917725\n",
      "Number capped: 5\n",
      "Average cap: 6.880718231201172\n",
      "Number capped: 4\n",
      "Average cap: 8.839278221130371\n",
      "Number capped: 5\n",
      "Average cap: 1.9080528020858765\n",
      "Number capped: 7\n",
      "Average cap: 1.5789669752120972\n",
      "Number capped: 2\n",
      "Average cap: 18.678157806396484\n",
      "Number capped: 5\n",
      "Average cap: 7.478761196136475\n",
      "Number capped: 2\n",
      "Average cap: 18.600238800048828\n",
      "Number capped: 8\n",
      "Average cap: 4.467686176300049\n",
      "Number capped: 14\n",
      "Average cap: 2.8358051776885986\n",
      "Number capped: 6\n",
      "Average cap: 5.8985209465026855\n",
      "Number capped: 7\n",
      "Average cap: 6.360409736633301\n",
      "Number capped: 7\n",
      "Average cap: 0.3924092650413513\n",
      "Number capped: 5\n",
      "Average cap: 1.5113999843597412\n",
      "Number capped: 6\n",
      "Average cap: 6.055412292480469\n",
      "Number capped: 5\n",
      "Average cap: 5.258549690246582\n",
      "Number capped: 1\n",
      "Average cap: 57.982208251953125\n",
      "Number capped: 2\n",
      "Average cap: 6.616084098815918\n",
      "Number capped: 3\n",
      "Average cap: 0.1405021846294403\n",
      "Number capped: 2\n",
      "Average cap: 26.129039764404297\n",
      "Number capped: 9\n",
      "Average cap: 5.133318901062012\n",
      "Number capped: 1\n",
      "Average cap: 51.89466857910156\n",
      "Number capped: 6\n",
      "Average cap: 2.6142637729644775\n",
      "Number capped: 11\n",
      "Average cap: 4.2676005363464355\n",
      "Number capped: 10\n",
      "Average cap: 0.9373010396957397\n",
      "Number capped: 5\n",
      "Average cap: 7.1646294593811035\n",
      "Number capped: 4\n",
      "Average cap: 9.04275894165039\n",
      "Number capped: 4\n",
      "Average cap: 9.115643501281738\n",
      "Number capped: 7\n",
      "Average cap: 2.1726841926574707\n",
      "Number capped: 4\n",
      "Average cap: 0.39612358808517456\n",
      "Number capped: 5\n",
      "Average cap: 0.047704923897981644\n",
      "Number capped: 7\n",
      "Average cap: 4.954463958740234\n",
      "Number capped: 3\n",
      "Average cap: 11.43375301361084\n",
      "Number capped: 5\n",
      "Average cap: 7.253506660461426\n",
      "Number capped: 2\n",
      "Average cap: 0.5544357895851135\n",
      "Number capped: 6\n",
      "Average cap: 0.06401480734348297\n",
      "Number capped: 2\n",
      "Average cap: 34.975067138671875\n",
      "Number capped: 4\n",
      "Average cap: 0.03569941222667694\n",
      "Number capped: 8\n",
      "Average cap: 4.542385101318359\n",
      "Number capped: 6\n",
      "Average cap: 1.4853156805038452\n",
      "Number capped: 5\n",
      "Average cap: 0.6317880749702454\n",
      "Number capped: 8\n",
      "Average cap: 4.090446472167969\n",
      "Number capped: 3\n",
      "Average cap: 11.304560661315918\n",
      "Number capped: 3\n",
      "Average cap: 11.197216033935547\n",
      "Number capped: 3\n",
      "Average cap: 0.3900105655193329\n",
      "Number capped: 3\n",
      "Average cap: 0.5693392753601074\n",
      "Number capped: 5\n",
      "Average cap: 7.416876792907715\n",
      "Number capped: 10\n",
      "Average cap: 3.8557446002960205\n",
      "Number capped: 2\n",
      "Average cap: 17.805892944335938\n",
      "Number capped: 1\n",
      "Average cap: 35.22541809082031\n",
      "Number capped: 4\n",
      "Average cap: 8.913418769836426\n",
      "Number capped: 3\n",
      "Average cap: 2.174748659133911\n",
      "Number capped: 6\n",
      "Average cap: 7.394056797027588\n",
      "Number capped: 3\n",
      "Average cap: 3.2833669185638428\n",
      "Number capped: 3\n",
      "Average cap: 1.221396803855896\n",
      "Number capped: 4\n",
      "Average cap: 9.165250778198242\n",
      "Number capped: 4\n",
      "Average cap: 13.45319938659668\n",
      "Number capped: 2\n",
      "Average cap: 29.47977066040039\n",
      "Number capped: 8\n",
      "Average cap: 4.452071666717529\n",
      "Number capped: 6\n",
      "Average cap: 0.07663463801145554\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 5\n",
      "Average cap: 7.006242275238037\n",
      "Number capped: 5\n",
      "Average cap: 0.4251161217689514\n",
      "Number capped: 1\n",
      "Average cap: 37.520896911621094\n",
      "Number capped: 3\n",
      "Average cap: 6.752632141113281\n",
      "Number capped: 7\n",
      "Average cap: 3.736128330230713\n",
      "Number capped: 9\n",
      "Average cap: 2.8869845867156982\n",
      "Number capped: 1\n",
      "Average cap: 36.61201095581055\n",
      "Number capped: 5\n",
      "Average cap: 0.7858492136001587\n",
      "Number capped: 6\n",
      "Average cap: 5.579015731811523\n",
      "Number capped: 2\n",
      "Average cap: 18.449180603027344\n",
      "Number capped: 5\n",
      "Average cap: 4.209271430969238\n",
      "Number capped: 4\n",
      "Average cap: 1.0593860149383545\n",
      "Number capped: 9\n",
      "Average cap: 0.8313243389129639\n",
      "Number capped: 7\n",
      "Average cap: 5.366035461425781\n",
      "Number capped: 8\n",
      "Average cap: 6.037127494812012\n",
      "Number capped: 4\n",
      "Average cap: 0.6009942889213562\n",
      "Number capped: 2\n",
      "Average cap: 17.765045166015625\n",
      "Number capped: 6\n",
      "Average cap: 5.518543720245361\n",
      "Number capped: 3\n",
      "Average cap: 2.1700618267059326\n",
      "Number capped: 5\n",
      "Average cap: 0.005666019860655069\n",
      "Number capped: 8\n",
      "Average cap: 1.718923807144165\n",
      "Number capped: 5\n",
      "Average cap: 6.973825931549072\n",
      "Number capped: 3\n",
      "Average cap: 17.350282669067383\n",
      "Number capped: 3\n",
      "Average cap: 12.268951416015625\n",
      "Number capped: 3\n",
      "Average cap: 2.106355667114258\n",
      "Number capped: 2\n",
      "Average cap: 17.73772430419922\n",
      "Number capped: 6\n",
      "Average cap: 5.309959888458252\n",
      "Number capped: 5\n",
      "Average cap: 0.8270303010940552\n",
      "Number capped: 10\n",
      "Average cap: 2.163074016571045\n",
      "Number capped: 8\n",
      "Average cap: 3.7527081966400146\n",
      "Number capped: 8\n",
      "Average cap: 3.8609488010406494\n",
      "Number capped: 8\n",
      "Average cap: 3.9148919582366943\n",
      "Number capped: 3\n",
      "Average cap: 0.5441085696220398\n",
      "Number capped: 4\n",
      "Average cap: 1.2960498332977295\n",
      "Number capped: 6\n",
      "Average cap: 5.436516284942627\n",
      "Number capped: 4\n",
      "Average cap: 2.1629819869995117\n",
      "Number capped: 5\n",
      "Average cap: 6.9084625244140625\n",
      "Number capped: 3\n",
      "Average cap: 4.300898551940918\n",
      "Number capped: 1\n",
      "Average cap: 36.236812591552734\n",
      "Number capped: 5\n",
      "Average cap: 0.12409527599811554\n",
      "Number capped: 1\n",
      "Average cap: 35.96497344970703\n",
      "Number capped: 3\n",
      "Average cap: 65.16395568847656\n",
      "Number capped: 5\n",
      "Average cap: 0.6165026426315308\n",
      "Number capped: 8\n",
      "Average cap: 5.244215488433838\n",
      "Number capped: 4\n",
      "Average cap: 10.146870613098145\n",
      "Number capped: 2\n",
      "Average cap: 3.0672829151153564\n",
      "Number capped: 6\n",
      "Average cap: 6.110072612762451\n",
      "Number capped: 8\n",
      "Average cap: 4.4335856437683105\n",
      "Number capped: 5\n",
      "Average cap: 2.748868465423584\n",
      "Number capped: 3\n",
      "Average cap: 9.296536445617676\n",
      "Number capped: 6\n",
      "Average cap: 5.84044075012207\n",
      "Number capped: 7\n",
      "Average cap: 5.209409236907959\n",
      "Number capped: 3\n",
      "Average cap: 0.12787878513336182\n",
      "Number capped: 6\n",
      "Average cap: 5.448791980743408\n",
      "Number capped: 4\n",
      "Average cap: 8.436924934387207\n",
      "Number capped: 5\n",
      "Average cap: 9.5442533493042\n",
      "Number capped: 5\n",
      "Average cap: 6.585515022277832\n",
      "Number capped: 8\n",
      "Average cap: 0.0009876256808638573\n",
      "Number capped: 4\n",
      "Average cap: 2.3574938774108887\n",
      "Number capped: 3\n",
      "Average cap: 16.4315242767334\n",
      "Number capped: 7\n",
      "Average cap: 4.468766689300537\n",
      "Number capped: 8\n",
      "Average cap: 0.46780988574028015\n",
      "Number capped: 2\n",
      "Average cap: 14.467260360717773\n",
      "Number capped: 2\n",
      "Average cap: 1.3607711791992188\n",
      "Number capped: 3\n",
      "Average cap: 12.169663429260254\n",
      "Number capped: 9\n",
      "Average cap: 5.216205596923828\n",
      "Number capped: 1\n",
      "Average cap: 36.75670623779297\n",
      "Number capped: 6\n",
      "Average cap: 5.809378147125244\n",
      "Number capped: 2\n",
      "Average cap: 15.023507118225098\n",
      "Number capped: 1\n",
      "Average cap: 30.469175338745117\n",
      "Number capped: 3\n",
      "Average cap: 0.07084137946367264\n",
      "Number capped: 1\n",
      "Average cap: 34.71969985961914\n",
      "Number capped: 4\n",
      "Average cap: 0.17377403378486633\n",
      "Number capped: 3\n",
      "Average cap: 11.226506233215332\n",
      "Number capped: 7\n",
      "Average cap: 0.7140299677848816\n",
      "Number capped: 7\n",
      "Average cap: 1.6376415491104126\n",
      "Number capped: 3\n",
      "Average cap: 12.091560363769531\n",
      "Number capped: 2\n",
      "Average cap: 22.226215362548828\n",
      "Number capped: 7\n",
      "Average cap: 5.466244697570801\n",
      "Number capped: 3\n",
      "Average cap: 13.391674995422363\n",
      "Number capped: 4\n",
      "Average cap: 8.146578788757324\n",
      "Number capped: 4\n",
      "Average cap: 4.598996639251709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 5\n",
      "Average cap: 7.4562859535217285\n",
      "Number capped: 2\n",
      "Average cap: 17.351154327392578\n",
      "Number capped: 5\n",
      "Average cap: 5.812108993530273\n",
      "Number capped: 5\n",
      "Average cap: 0.5494539141654968\n",
      "Number capped: 6\n",
      "Average cap: 6.633712291717529\n",
      "Number capped: 2\n",
      "Average cap: 17.42233657836914\n",
      "Number capped: 11\n",
      "Average cap: 1.4719605445861816\n",
      "Number capped: 5\n",
      "Average cap: 7.187975883483887\n",
      "Number capped: 2\n",
      "Average cap: 1.7017104625701904\n",
      "Number capped: 8\n",
      "Average cap: 6.8699951171875\n",
      "Number capped: 1\n",
      "Average cap: 8.144145965576172\n",
      "Number capped: 2\n",
      "Average cap: 2.098801374435425\n",
      "Number capped: 7\n",
      "Average cap: 4.96256685256958\n",
      "Number capped: 6\n",
      "Average cap: 5.763004779815674\n",
      "Number capped: 4\n",
      "Average cap: 7.544737339019775\n",
      "Number capped: 5\n",
      "Average cap: 7.588685512542725\n",
      "Number capped: 5\n",
      "Average cap: 9.448762893676758\n",
      "Number capped: 8\n",
      "Average cap: 4.7614521980285645\n",
      "Number capped: 13\n",
      "Average cap: 3.2797067165374756\n",
      "Number capped: 5\n",
      "Average cap: 7.296779632568359\n",
      "Number capped: 5\n",
      "Average cap: 10.283653259277344\n",
      "Number capped: 8\n",
      "Average cap: 0.8514987230300903\n",
      "Number capped: 6\n",
      "Average cap: 0.2603456377983093\n",
      "Number capped: 6\n",
      "Average cap: 0.5990447998046875\n",
      "Number capped: 5\n",
      "Average cap: 9.12780475616455\n",
      "Number capped: 5\n",
      "Average cap: 1.752480149269104\n",
      "Number capped: 9\n",
      "Average cap: 4.396708965301514\n",
      "Number capped: 4\n",
      "Average cap: 12.69025993347168\n",
      "Number capped: 6\n",
      "Average cap: 5.581980228424072\n",
      "Number capped: 3\n",
      "Average cap: 0.8583198189735413\n",
      "Number capped: 8\n",
      "Average cap: 3.384793996810913\n",
      "Number capped: 5\n",
      "Average cap: 7.040674686431885\n",
      "Number capped: 6\n",
      "Average cap: 5.942832946777344\n",
      "Number capped: 5\n",
      "Average cap: 0.9635326266288757\n",
      "Number capped: 5\n",
      "Average cap: 0.20697736740112305\n",
      "Number capped: 4\n",
      "Average cap: 0.1721116602420807\n",
      "Number capped: 1\n",
      "Average cap: 0.8564287424087524\n",
      "Number capped: 4\n",
      "Average cap: 8.80612564086914\n",
      "Number capped: 3\n",
      "Average cap: 10.905391693115234\n",
      "Number capped: 5\n",
      "Average cap: 8.052164077758789\n",
      "Number capped: 4\n",
      "Average cap: 7.275457382202148\n",
      "Number capped: 4\n",
      "Average cap: 8.373002052307129\n",
      "Number capped: 2\n",
      "Average cap: 2.8980889320373535\n",
      "Number capped: 3\n",
      "Average cap: 12.008872985839844\n",
      "Number capped: 3\n",
      "Average cap: 1.0736252069473267\n",
      "Number capped: 9\n",
      "Average cap: 4.517183303833008\n",
      "Number capped: 3\n",
      "Average cap: 0.5908761024475098\n",
      "Number capped: 4\n",
      "Average cap: 8.747203826904297\n",
      "Number capped: 2\n",
      "Average cap: 7.594931602478027\n",
      "Number capped: 4\n",
      "Average cap: 0.3040809631347656\n",
      "Number capped: 4\n",
      "Average cap: 9.327693939208984\n",
      "Number capped: 5\n",
      "Average cap: 7.1586594581604\n",
      "Number capped: 9\n",
      "Average cap: 4.214877128601074\n",
      "Number capped: 4\n",
      "Average cap: 8.680693626403809\n",
      "Number capped: 3\n",
      "Average cap: 0.912447988986969\n",
      "Number capped: 5\n",
      "Average cap: 6.047608375549316\n",
      "Number capped: 3\n",
      "Average cap: 0.4578278362751007\n",
      "Number capped: 2\n",
      "Average cap: 12.200764656066895\n",
      "Number capped: 5\n",
      "Average cap: 1.3332607746124268\n",
      "Number capped: 2\n",
      "Average cap: 17.59881591796875\n",
      "Number capped: 10\n",
      "Average cap: 1.3439525365829468\n",
      "Number capped: 6\n",
      "Average cap: 6.1939239501953125\n",
      "Number capped: 5\n",
      "Average cap: 6.492542266845703\n",
      "Number capped: 6\n",
      "Average cap: 5.777967929840088\n",
      "Number capped: 7\n",
      "Average cap: 4.987738609313965\n",
      "Number capped: 7\n",
      "Average cap: 5.141552925109863\n",
      "Number capped: 7\n",
      "Average cap: 0.008495943620800972\n",
      "Number capped: 3\n",
      "Average cap: 11.487715721130371\n",
      "Number capped: 2\n",
      "Average cap: 1.6453096866607666\n",
      "Number capped: 3\n",
      "Average cap: 11.454063415527344\n",
      "Number capped: 4\n",
      "Average cap: 1.2265187501907349\n",
      "Number capped: 2\n",
      "Average cap: 0.038232266902923584\n",
      "Number capped: 7\n",
      "Average cap: 4.926628112792969\n",
      "Number capped: 2\n",
      "Average cap: 17.344188690185547\n",
      "Number capped: 5\n",
      "Average cap: 7.044297218322754\n",
      "Number capped: 9\n",
      "Average cap: 3.8836870193481445\n",
      "Number capped: 4\n",
      "Average cap: 8.819099426269531\n",
      "Number capped: 7\n",
      "Average cap: 4.308365821838379\n",
      "Number capped: 5\n",
      "Average cap: 0.7434536218643188\n",
      "Number capped: 6\n",
      "Average cap: 0.05597851052880287\n",
      "Number capped: 6\n",
      "Average cap: 1.3850369453430176\n",
      "Number capped: 4\n",
      "Average cap: 8.59686279296875\n",
      "Number capped: 2\n",
      "Average cap: 10.444973945617676\n",
      "Number capped: 7\n",
      "Average cap: 4.477661609649658\n",
      "Number capped: 7\n",
      "Average cap: 7.144493579864502\n",
      "Number capped: 5\n",
      "Average cap: 5.400553226470947\n",
      "Number capped: 3\n",
      "Average cap: 7.43287992477417\n",
      "Number capped: 10\n",
      "Average cap: 3.98549222946167\n",
      "Number capped: 3\n",
      "Average cap: 11.806793212890625\n",
      "Number capped: 6\n",
      "Average cap: 2.5999693870544434\n",
      "Number capped: 4\n",
      "Average cap: 0.5669155120849609\n",
      "Number capped: 5\n",
      "Average cap: 6.586555480957031\n",
      "Number capped: 2\n",
      "Average cap: 70.91830444335938\n",
      "Number capped: 8\n",
      "Average cap: 1.7296069860458374\n",
      "Number capped: 9\n",
      "Average cap: 3.383925437927246\n",
      "Number capped: 3\n",
      "Average cap: 16.169910430908203\n",
      "Number capped: 7\n",
      "Average cap: 0.4789837896823883\n",
      "Number capped: 6\n",
      "Average cap: 0.5634754300117493\n",
      "Number capped: 7\n",
      "Average cap: 4.854610443115234\n",
      "Number capped: 5\n",
      "Average cap: 6.5187482833862305\n",
      "Number capped: 5\n",
      "Average cap: 1.8281500339508057\n",
      "Number capped: 5\n",
      "Average cap: 6.965151309967041\n",
      "Number capped: 6\n",
      "Average cap: 0.5521895885467529\n",
      "Number capped: 4\n",
      "Average cap: 0.7658482193946838\n",
      "Number capped: 6\n",
      "Average cap: 8.882960319519043\n",
      "Number capped: 9\n",
      "Average cap: 3.4568684101104736\n",
      "Number capped: 12\n",
      "Average cap: 0.5399878621101379\n",
      "Number capped: 2\n",
      "Average cap: 17.093238830566406\n",
      "Number capped: 4\n",
      "Average cap: 8.615768432617188\n",
      "Number capped: 5\n",
      "Average cap: 1.5659558773040771\n",
      "Number capped: 3\n",
      "Average cap: 23.389074325561523\n",
      "Number capped: 4\n",
      "Average cap: 9.54101276397705\n",
      "Number capped: 3\n",
      "Average cap: 11.72137451171875\n",
      "Number capped: 6\n",
      "Average cap: 6.691727161407471\n",
      "Number capped: 7\n",
      "Average cap: 6.647387504577637\n",
      "Number capped: 4\n",
      "Average cap: 0.0005836678319610655\n",
      "Number capped: 7\n",
      "Average cap: 0.9251876473426819\n",
      "Number capped: 4\n",
      "Average cap: 0.8728357553482056\n",
      "Number capped: 7\n",
      "Average cap: 4.944409370422363\n",
      "Number capped: 5\n",
      "Average cap: 0.10878632962703705\n",
      "Number capped: 2\n",
      "Average cap: 7.102859020233154\n",
      "Number capped: 3\n",
      "Average cap: 18.3061580657959\n",
      "Number capped: 3\n",
      "Average cap: 11.243943214416504\n",
      "Number capped: 8\n",
      "Average cap: 4.331935405731201\n",
      "Number capped: 5\n",
      "Average cap: 0.609765887260437\n",
      "Number capped: 5\n",
      "Average cap: 6.949830055236816\n",
      "Number capped: 5\n",
      "Average cap: 6.869540214538574\n",
      "Number capped: 7\n",
      "Average cap: 8.451742172241211\n",
      "Number capped: 5\n",
      "Average cap: 9.249353408813477\n",
      "Number capped: 3\n",
      "Average cap: 16.287960052490234\n",
      "Number capped: 5\n",
      "Average cap: 0.00457942858338356\n",
      "Number capped: 2\n",
      "Average cap: 0.7777780890464783\n",
      "Number capped: 6\n",
      "Average cap: 5.629167079925537\n",
      "Number capped: 4\n",
      "Average cap: 0.15291422605514526\n",
      "Number capped: 4\n",
      "Average cap: 1.2674331665039062\n",
      "Number capped: 6\n",
      "Average cap: 5.35739278793335\n",
      "Number capped: 6\n",
      "Average cap: 5.918336868286133\n",
      "Number capped: 8\n",
      "Average cap: 4.254322052001953\n",
      "Number capped: 8\n",
      "Average cap: 4.257615089416504\n",
      "Number capped: 7\n",
      "Average cap: 4.329745292663574\n",
      "Number capped: 5\n",
      "Average cap: 7.061836242675781\n",
      "Number capped: 5\n",
      "Average cap: 0.23246319591999054\n",
      "Number capped: 3\n",
      "Average cap: 12.293078422546387\n",
      "Number capped: 5\n",
      "Average cap: 0.5699219107627869\n",
      "Number capped: 2\n",
      "Average cap: 17.306161880493164\n",
      "Number capped: 6\n",
      "Average cap: 5.800251483917236\n",
      "Number capped: 6\n",
      "Average cap: 12.527817726135254\n",
      "Number capped: 2\n",
      "Average cap: 3.6573781967163086\n",
      "Number capped: 4\n",
      "Average cap: 0.9402233958244324\n",
      "Number capped: 7\n",
      "Average cap: 3.5582756996154785\n",
      "Number capped: 5\n",
      "Average cap: 8.75545883178711\n",
      "Number capped: 6\n",
      "Average cap: 5.787680149078369\n",
      "Number capped: 4\n",
      "Average cap: 7.991794586181641\n",
      "Number capped: 5\n",
      "Average cap: 0.011990523897111416\n",
      "Number capped: 4\n",
      "Average cap: 4.550423622131348\n",
      "Number capped: 3\n",
      "Average cap: 11.342679023742676\n",
      "Number capped: 1\n",
      "Average cap: 47.985294342041016\n",
      "Number capped: 3\n",
      "Average cap: 2.513082265853882\n",
      "Number capped: 2\n",
      "Average cap: 21.039535522460938\n",
      "Number capped: 5\n",
      "Average cap: 9.454960823059082\n",
      "Number capped: 8\n",
      "Average cap: 0.6778051257133484\n",
      "Number capped: 2\n",
      "Average cap: 20.593780517578125\n",
      "Number capped: 7\n",
      "Average cap: 0.14717291295528412\n",
      "Number capped: 4\n",
      "Average cap: 8.226786613464355\n",
      "Number capped: 5\n",
      "Average cap: 0.008353062905371189\n",
      "Number capped: 4\n",
      "Average cap: 13.50121021270752\n",
      "Number capped: 5\n",
      "Average cap: 0.8470617532730103\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 4\n",
      "Average cap: 8.792272567749023\n",
      "Number capped: 6\n",
      "Average cap: 5.180507659912109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 5\n",
      "Average cap: 6.9452619552612305\n",
      "Number capped: 3\n",
      "Average cap: 21.462587356567383\n",
      "Number capped: 3\n",
      "Average cap: 10.800211906433105\n",
      "Number capped: 4\n",
      "Average cap: 8.185935020446777\n",
      "Number capped: 4\n",
      "Average cap: 0.08016939461231232\n",
      "Number capped: 7\n",
      "Average cap: 0.08997809886932373\n",
      "Number capped: 5\n",
      "Average cap: 0.6169565320014954\n",
      "Number capped: 5\n",
      "Average cap: 7.309263706207275\n",
      "Number capped: 10\n",
      "Average cap: 4.938380241394043\n",
      "Number capped: 5\n",
      "Average cap: 12.307319641113281\n",
      "Number capped: 3\n",
      "Average cap: 11.542637825012207\n",
      "Number capped: 3\n",
      "Average cap: 8.62901782989502\n",
      "Number capped: 1\n",
      "Average cap: 50.47660827636719\n",
      "Number capped: 2\n",
      "Average cap: 28.07876205444336\n",
      "Number capped: 12\n",
      "Average cap: 2.3003687858581543\n",
      "Number capped: 5\n",
      "Average cap: 6.3827033042907715\n",
      "Number capped: 10\n",
      "Average cap: 0.5470672845840454\n",
      "Number capped: 7\n",
      "Average cap: 4.350461006164551\n",
      "Number capped: 5\n",
      "Average cap: 7.049358367919922\n",
      "Number capped: 3\n",
      "Average cap: 82.0837173461914\n",
      "Number capped: 4\n",
      "Average cap: 9.218406677246094\n",
      "Number capped: 10\n",
      "Average cap: 2.975841760635376\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 8\n",
      "Average cap: 4.407430648803711\n",
      "Number capped: 6\n",
      "Average cap: 1.276852011680603\n",
      "Number capped: 5\n",
      "Average cap: 0.1688576638698578\n",
      "Number capped: 7\n",
      "Average cap: 1.2621670961380005\n",
      "Number capped: 2\n",
      "Average cap: 53.896690368652344\n",
      "Number capped: 4\n",
      "Average cap: 8.452305793762207\n",
      "Number capped: 7\n",
      "Average cap: 0.7994675040245056\n",
      "Number capped: 8\n",
      "Average cap: 1.4548834562301636\n",
      "Number capped: 4\n",
      "Average cap: 11.441678047180176\n",
      "Number capped: 1\n",
      "Average cap: 2.0687289237976074\n",
      "Number capped: 4\n",
      "Average cap: 2.7913289070129395\n",
      "Number capped: 5\n",
      "Average cap: 6.824603080749512\n",
      "Number capped: 5\n",
      "Average cap: 0.5678364038467407\n",
      "Number capped: 8\n",
      "Average cap: 4.372567176818848\n",
      "Number capped: 5\n",
      "Average cap: 6.6714677810668945\n",
      "Number capped: 3\n",
      "Average cap: 7.212099075317383\n",
      "Number capped: 3\n",
      "Average cap: 0.5509976148605347\n",
      "Number capped: 3\n",
      "Average cap: 12.758186340332031\n",
      "Number capped: 6\n",
      "Average cap: 1.8888211250305176\n",
      "Number capped: 6\n",
      "Average cap: 0.5419425368309021\n",
      "Number capped: 4\n",
      "Average cap: 8.972590446472168\n",
      "Number capped: 4\n",
      "Average cap: 0.536696195602417\n",
      "Number capped: 4\n",
      "Average cap: 8.688817977905273\n",
      "Number capped: 4\n",
      "Average cap: 9.051008224487305\n",
      "Number capped: 1\n",
      "Average cap: 33.3089714050293\n",
      "Number capped: 4\n",
      "Average cap: 0.7942380309104919\n",
      "Number capped: 5\n",
      "Average cap: 7.001233100891113\n",
      "Number capped: 6\n",
      "Average cap: 6.055522441864014\n",
      "Number capped: 4\n",
      "Average cap: 8.186853408813477\n",
      "Number capped: 4\n",
      "Average cap: 12.15410327911377\n",
      "Number capped: 1\n",
      "Average cap: 0.444184809923172\n",
      "Number capped: 5\n",
      "Average cap: 0.003006107872352004\n",
      "Number capped: 4\n",
      "Average cap: 1.3421214818954468\n",
      "Number capped: 7\n",
      "Average cap: 3.7391133308410645\n",
      "Number capped: 4\n",
      "Average cap: 8.882891654968262\n",
      "Number capped: 8\n",
      "Average cap: 0.4323110580444336\n",
      "Number capped: 3\n",
      "Average cap: 11.426688194274902\n",
      "Number capped: 3\n",
      "Average cap: 26.779335021972656\n",
      "Number capped: 7\n",
      "Average cap: 6.5360026359558105\n",
      "Number capped: 7\n",
      "Average cap: 5.74248743057251\n",
      "Number capped: 1\n",
      "Average cap: 1.5822259187698364\n",
      "Number capped: 7\n",
      "Average cap: 0.1506655514240265\n",
      "Number capped: 6\n",
      "Average cap: 7.781128406524658\n",
      "Number capped: 1\n",
      "Average cap: 33.287471771240234\n",
      "Number capped: 1\n",
      "Average cap: 54.34528350830078\n",
      "Number capped: 6\n",
      "Average cap: 5.45362663269043\n",
      "Number capped: 6\n",
      "Average cap: 7.165637969970703\n",
      "Number capped: 4\n",
      "Average cap: 1.102468729019165\n",
      "Number capped: 8\n",
      "Average cap: 4.572772026062012\n",
      "Number capped: 2\n",
      "Average cap: 5.44394063949585\n",
      "Number capped: 8\n",
      "Average cap: 0.18924391269683838\n",
      "Number capped: 1\n",
      "Average cap: 1.3132492303848267\n",
      "Number capped: 7\n",
      "Average cap: 4.813307762145996\n",
      "Number capped: 5\n",
      "Average cap: 6.655821323394775\n",
      "Number capped: 3\n",
      "Average cap: 11.03304386138916\n",
      "Number capped: 7\n",
      "Average cap: 1.1725460290908813\n",
      "Number capped: 3\n",
      "Average cap: 13.905654907226562\n",
      "Number capped: 4\n",
      "Average cap: 4.893382549285889\n",
      "Number capped: 7\n",
      "Average cap: 4.827709674835205\n",
      "Number capped: 4\n",
      "Average cap: 9.883322715759277\n",
      "Number capped: 6\n",
      "Average cap: 5.728317737579346\n",
      "Number capped: 2\n",
      "Average cap: 5.414655685424805\n",
      "Number capped: 5\n",
      "Average cap: 0.5502318143844604\n",
      "Number capped: 4\n",
      "Average cap: 8.0949068069458\n",
      "Number capped: 1\n",
      "Average cap: 4.94195556640625\n",
      "Number capped: 5\n",
      "Average cap: 4.744657516479492\n",
      "Number capped: 6\n",
      "Average cap: 5.626648426055908\n",
      "Number capped: 10\n",
      "Average cap: 2.8335037231445312\n",
      "Number capped: 4\n",
      "Average cap: 0.5409972071647644\n",
      "Number capped: 11\n",
      "Average cap: 3.089298963546753\n",
      "Number capped: 5\n",
      "Average cap: 0.7390859723091125\n",
      "Number capped: 8\n",
      "Average cap: 4.172447204589844\n",
      "Number capped: 5\n",
      "Average cap: 6.967080593109131\n",
      "Number capped: 11\n",
      "Average cap: 1.0245105028152466\n",
      "Number capped: 6\n",
      "Average cap: 6.443049907684326\n",
      "Number capped: 5\n",
      "Average cap: 7.700858116149902\n",
      "Number capped: 7\n",
      "Average cap: 8.021401405334473\n",
      "Number capped: 5\n",
      "Average cap: 1.5297911167144775\n",
      "Number capped: 7\n",
      "Average cap: 5.2501220703125\n",
      "Number capped: 4\n",
      "Average cap: 0.5984524488449097\n",
      "Number capped: 4\n",
      "Average cap: 13.642087936401367\n",
      "Number capped: 5\n",
      "Average cap: 6.700631618499756\n",
      "Number capped: 4\n",
      "Average cap: 8.097081184387207\n",
      "Number capped: 6\n",
      "Average cap: 0.6262438297271729\n",
      "Number capped: 14\n",
      "Average cap: 1.8840001821517944\n",
      "Number capped: 3\n",
      "Average cap: 8.975497245788574\n",
      "Number capped: 4\n",
      "Average cap: 8.399005889892578\n",
      "Number capped: 6\n",
      "Average cap: 5.703921794891357\n",
      "Number capped: 5\n",
      "Average cap: 3.7596213817596436\n",
      "Number capped: 4\n",
      "Average cap: 2.883246421813965\n",
      "Number capped: 2\n",
      "Average cap: 17.826095581054688\n",
      "Number capped: 9\n",
      "Average cap: 3.4434101581573486\n",
      "Number capped: 3\n",
      "Average cap: 0.5206728577613831\n",
      "Number capped: 5\n",
      "Average cap: 0.605871856212616\n",
      "Number capped: 1\n",
      "Average cap: 0.9945261478424072\n",
      "Number capped: 5\n",
      "Average cap: 6.846067905426025\n",
      "Number capped: 3\n",
      "Average cap: 0.5752362608909607\n",
      "Number capped: 4\n",
      "Average cap: 1.3964923620224\n",
      "Number capped: 3\n",
      "Average cap: 1.0622267723083496\n",
      "Number capped: 4\n",
      "Average cap: 0.7107282876968384\n",
      "Number capped: 1\n",
      "Average cap: 1.5784728527069092\n",
      "Number capped: 10\n",
      "Average cap: 0.38397216796875\n",
      "Number capped: 7\n",
      "Average cap: 5.2399001121521\n",
      "Number capped: 6\n",
      "Average cap: 9.707271575927734\n",
      "Number capped: 6\n",
      "Average cap: 5.889109134674072\n",
      "Number capped: 3\n",
      "Average cap: 0.5821177363395691\n",
      "Number capped: 4\n",
      "Average cap: 7.946281909942627\n",
      "Number capped: 6\n",
      "Average cap: 0.7889131903648376\n",
      "Number capped: 3\n",
      "Average cap: 0.8090503811836243\n",
      "Number capped: 3\n",
      "Average cap: 11.661796569824219\n",
      "Number capped: 6\n",
      "Average cap: 5.3099517822265625\n",
      "Number capped: 6\n",
      "Average cap: 5.061553001403809\n",
      "Number capped: 3\n",
      "Average cap: 10.693089485168457\n",
      "Number capped: 4\n",
      "Average cap: 6.756383895874023\n",
      "Number capped: 6\n",
      "Average cap: 7.547820568084717\n",
      "Number capped: 6\n",
      "Average cap: 0.16239725053310394\n",
      "Number capped: 5\n",
      "Average cap: 0.11887357383966446\n",
      "Number capped: 6\n",
      "Average cap: 4.851704120635986\n",
      "Number capped: 4\n",
      "Average cap: 8.135831832885742\n",
      "Number capped: 7\n",
      "Average cap: 4.260016918182373\n",
      "Number capped: 2\n",
      "Average cap: 0.631553590297699\n",
      "Number capped: 3\n",
      "Average cap: 6.512569427490234\n",
      "Number capped: 2\n",
      "Average cap: 0.434609055519104\n",
      "Number capped: 4\n",
      "Average cap: 1.8704007863998413\n",
      "Number capped: 8\n",
      "Average cap: 3.6222102642059326\n",
      "Number capped: 8\n",
      "Average cap: 0.5431822538375854\n",
      "Number capped: 2\n",
      "Average cap: 0.2546328008174896\n",
      "Number capped: 7\n",
      "Average cap: 5.388072490692139\n",
      "Number capped: 3\n",
      "Average cap: 12.115519523620605\n",
      "Number capped: 3\n",
      "Average cap: 12.061858177185059\n",
      "Number capped: 5\n",
      "Average cap: 6.670263767242432\n",
      "Number capped: 2\n",
      "Average cap: 3.843517541885376\n",
      "Number capped: 5\n",
      "Average cap: 1.2553322315216064\n",
      "Number capped: 3\n",
      "Average cap: 1.4054356813430786\n",
      "Number capped: 8\n",
      "Average cap: 5.661643028259277\n",
      "Number capped: 4\n",
      "Average cap: 13.768329620361328\n",
      "Number capped: 8\n",
      "Average cap: 4.288536071777344\n",
      "Number capped: 6\n",
      "Average cap: 5.982865810394287\n",
      "Number capped: 6\n",
      "Average cap: 5.206182479858398\n",
      "Number capped: 4\n",
      "Average cap: 8.244097709655762\n",
      "Number capped: 3\n",
      "Average cap: 6.079435348510742\n",
      "Number capped: 5\n",
      "Average cap: 6.81227970123291\n",
      "Number capped: 10\n",
      "Average cap: 3.6685147285461426\n",
      "Number capped: 3\n",
      "Average cap: 18.533845901489258\n",
      "Number capped: 7\n",
      "Average cap: 4.537852764129639\n",
      "Number capped: 7\n",
      "Average cap: 3.43605637550354\n",
      "Number capped: 3\n",
      "Average cap: 15.15429401397705\n",
      "Number capped: 4\n",
      "Average cap: 11.728543281555176\n",
      "Number capped: 7\n",
      "Average cap: 7.490128040313721\n",
      "Number capped: 6\n",
      "Average cap: 0.09257423877716064\n",
      "Number capped: 1\n",
      "Average cap: 33.4737548828125\n",
      "Number capped: 9\n",
      "Average cap: 3.195345163345337\n",
      "Number capped: 3\n",
      "Average cap: 4.569178104400635\n",
      "Number capped: 2\n",
      "Average cap: 1.3736903667449951\n",
      "Number capped: 2\n",
      "Average cap: 16.11559295654297\n",
      "Number capped: 3\n",
      "Average cap: 12.494662284851074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 3\n",
      "Average cap: 18.558317184448242\n",
      "Number capped: 6\n",
      "Average cap: 1.6117091178894043\n",
      "Number capped: 3\n",
      "Average cap: 10.493721961975098\n",
      "Number capped: 2\n",
      "Average cap: 28.891193389892578\n",
      "Number capped: 4\n",
      "Average cap: 8.3716402053833\n",
      "Number capped: 6\n",
      "Average cap: 10.249260902404785\n",
      "Number capped: 4\n",
      "Average cap: 0.0010980418883264065\n",
      "Number capped: 8\n",
      "Average cap: 4.4520769119262695\n",
      "Number capped: 3\n",
      "Average cap: 11.503186225891113\n",
      "Number capped: 5\n",
      "Average cap: 2.6633152961730957\n",
      "Number capped: 3\n",
      "Average cap: 13.6821928024292\n",
      "Number capped: 8\n",
      "Average cap: 3.6942291259765625\n",
      "Number capped: 4\n",
      "Average cap: 9.3812255859375\n",
      "Number capped: 1\n",
      "Average cap: 30.218109130859375\n",
      "Number capped: 3\n",
      "Average cap: 0.3723374903202057\n",
      "Number capped: 2\n",
      "Average cap: 0.9640604257583618\n",
      "Number capped: 4\n",
      "Average cap: 8.55446720123291\n",
      "Number capped: 4\n",
      "Average cap: 0.8442102670669556\n",
      "Number capped: 2\n",
      "Average cap: 16.680021286010742\n",
      "Number capped: 5\n",
      "Average cap: 7.6960906982421875\n",
      "Number capped: 7\n",
      "Average cap: 2.3862648010253906\n",
      "Number capped: 2\n",
      "Average cap: 3.727217674255371\n",
      "Number capped: 3\n",
      "Average cap: 11.23843002319336\n",
      "Number capped: 8\n",
      "Average cap: 4.606174468994141\n",
      "Number capped: 2\n",
      "Average cap: 19.294095993041992\n",
      "Number capped: 5\n",
      "Average cap: 0.26980453729629517\n",
      "Number capped: 6\n",
      "Average cap: 5.546417713165283\n",
      "Number capped: 1\n",
      "Average cap: 33.08760070800781\n",
      "Number capped: 3\n",
      "Average cap: 0.10344886779785156\n",
      "Number capped: 8\n",
      "Average cap: 4.559372901916504\n",
      "Number capped: 2\n",
      "Average cap: 19.331331253051758\n",
      "Number capped: 9\n",
      "Average cap: 0.3935753107070923\n",
      "Number capped: 5\n",
      "Average cap: 0.7458540201187134\n",
      "Number capped: 5\n",
      "Average cap: 6.683835029602051\n",
      "Number capped: 5\n",
      "Average cap: 7.266531467437744\n",
      "Number capped: 3\n",
      "Average cap: 1.4356576204299927\n",
      "Number capped: 6\n",
      "Average cap: 7.776695728302002\n",
      "Number capped: 1\n",
      "Average cap: 3.534573554992676\n",
      "Number capped: 8\n",
      "Average cap: 0.6280589699745178\n",
      "Number capped: 8\n",
      "Average cap: 0.6258296370506287\n",
      "Number capped: 8\n",
      "Average cap: 0.4294476807117462\n",
      "Number capped: 3\n",
      "Average cap: 11.153038024902344\n",
      "Number capped: 9\n",
      "Average cap: 3.600017547607422\n",
      "Number capped: 3\n",
      "Average cap: 14.790488243103027\n",
      "Number capped: 1\n",
      "Average cap: 0.8954864144325256\n",
      "Number capped: 3\n",
      "Average cap: 11.055769920349121\n",
      "Number capped: 4\n",
      "Average cap: 7.864593982696533\n",
      "Number capped: 2\n",
      "Average cap: 18.199798583984375\n",
      "Number capped: 7\n",
      "Average cap: 2.1932871341705322\n",
      "Number capped: 5\n",
      "Average cap: 6.322904586791992\n",
      "Number capped: 2\n",
      "Average cap: 19.482521057128906\n",
      "Number capped: 3\n",
      "Average cap: 11.661361694335938\n",
      "Number capped: 3\n",
      "Average cap: 11.066483497619629\n",
      "Number capped: 3\n",
      "Average cap: 0.8202285766601562\n",
      "Number capped: 4\n",
      "Average cap: 8.307511329650879\n",
      "Number capped: 6\n",
      "Average cap: 4.918832302093506\n",
      "Number capped: 5\n",
      "Average cap: 6.805834770202637\n",
      "Number capped: 10\n",
      "Average cap: 0.008702847175300121\n",
      "Number capped: 6\n",
      "Average cap: 7.046698093414307\n",
      "Number capped: 10\n",
      "Average cap: 4.490672588348389\n",
      "Number capped: 3\n",
      "Average cap: 0.643523097038269\n",
      "Number capped: 3\n",
      "Average cap: 14.745648384094238\n",
      "Number capped: 3\n",
      "Average cap: 0.8217957615852356\n",
      "Number capped: 6\n",
      "Average cap: 5.073256969451904\n",
      "Number capped: 5\n",
      "Average cap: 6.233958721160889\n",
      "Number capped: 6\n",
      "Average cap: 0.0005024143611080945\n",
      "Number capped: 3\n",
      "Average cap: 3.799435615539551\n",
      "Number capped: 4\n",
      "Average cap: 8.365522384643555\n",
      "Number capped: 4\n",
      "Average cap: 2.98578143119812\n",
      "Number capped: 3\n",
      "Average cap: 2.103336811065674\n",
      "Number capped: 2\n",
      "Average cap: 0.8537557721138\n",
      "Number capped: 5\n",
      "Average cap: 0.5403541326522827\n",
      "Number capped: 8\n",
      "Average cap: 2.3519580364227295\n",
      "Number capped: 3\n",
      "Average cap: 10.41539192199707\n",
      "Number capped: 4\n",
      "Average cap: 7.655099868774414\n",
      "Number capped: 4\n",
      "Average cap: 0.03542060777544975\n",
      "Number capped: 6\n",
      "Average cap: 7.059878826141357\n",
      "Number capped: 7\n",
      "Average cap: 5.566642761230469\n",
      "Number capped: 4\n",
      "Average cap: 0.5496513843536377\n",
      "Number capped: 5\n",
      "Average cap: 7.889090061187744\n",
      "Number capped: 7\n",
      "Average cap: 5.04766321182251\n",
      "Number capped: 5\n",
      "Average cap: 0.3823942542076111\n",
      "Number capped: 11\n",
      "Average cap: 2.495608329772949\n",
      "Number capped: 1\n",
      "Average cap: 38.29434585571289\n",
      "Number capped: 7\n",
      "Average cap: 5.222412586212158\n",
      "Number capped: 7\n",
      "Average cap: 4.883505344390869\n",
      "Number capped: 3\n",
      "Average cap: 11.3272066116333\n",
      "Number capped: 3\n",
      "Average cap: 0.902656614780426\n",
      "Number capped: 1\n",
      "Average cap: 0.040632013231515884\n",
      "Number capped: 6\n",
      "Average cap: 0.6472925543785095\n",
      "Number capped: 6\n",
      "Average cap: 6.511316299438477\n",
      "Number capped: 5\n",
      "Average cap: 0.585820198059082\n",
      "Number capped: 10\n",
      "Average cap: 3.3360531330108643\n",
      "Number capped: 8\n",
      "Average cap: 3.660026788711548\n",
      "Number capped: 4\n",
      "Average cap: 0.4094802141189575\n",
      "Number capped: 3\n",
      "Average cap: 0.20053191483020782\n",
      "Number capped: 7\n",
      "Average cap: 5.175389766693115\n",
      "Number capped: 3\n",
      "Average cap: 0.009369496256113052\n",
      "Number capped: 4\n",
      "Average cap: 1.7339894771575928\n",
      "Number capped: 5\n",
      "Average cap: 1.053119421005249\n",
      "Number capped: 6\n",
      "Average cap: 9.233729362487793\n",
      "Number capped: 4\n",
      "Average cap: 0.7822690010070801\n",
      "Number capped: 2\n",
      "Average cap: 15.690840721130371\n",
      "Number capped: 6\n",
      "Average cap: 0.012703746557235718\n",
      "Number capped: 4\n",
      "Average cap: 0.5130061507225037\n",
      "Number capped: 1\n",
      "Average cap: 7.918890953063965\n",
      "Number capped: 2\n",
      "Average cap: 15.702424049377441\n",
      "Number capped: 4\n",
      "Average cap: 10.858901977539062\n",
      "Number capped: 4\n",
      "Average cap: 13.499626159667969\n",
      "Number capped: 7\n",
      "Average cap: 4.201783657073975\n",
      "Number capped: 4\n",
      "Average cap: 0.3167567849159241\n",
      "Number capped: 8\n",
      "Average cap: 0.5539178252220154\n",
      "Number capped: 3\n",
      "Average cap: 4.174959182739258\n",
      "Number capped: 7\n",
      "Average cap: 7.344645977020264\n",
      "Number capped: 2\n",
      "Average cap: 4.419687271118164\n",
      "Number capped: 7\n",
      "Average cap: 2.3291304111480713\n",
      "Number capped: 4\n",
      "Average cap: 8.315086364746094\n",
      "Number capped: 8\n",
      "Average cap: 1.2193584442138672\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 6\n",
      "Average cap: 8.352437973022461\n",
      "Number capped: 4\n",
      "Average cap: 8.104761123657227\n",
      "Number capped: 7\n",
      "Average cap: 1.053642988204956\n",
      "Number capped: 3\n",
      "Average cap: 0.5231626033782959\n",
      "Number capped: 1\n",
      "Average cap: 32.551025390625\n",
      "Number capped: 10\n",
      "Average cap: 0.5173055529594421\n",
      "Number capped: 7\n",
      "Average cap: 5.035595417022705\n",
      "Number capped: 10\n",
      "Average cap: 2.863668918609619\n",
      "Number capped: 3\n",
      "Average cap: 2.318732500076294\n",
      "Number capped: 4\n",
      "Average cap: 8.039684295654297\n",
      "Number capped: 3\n",
      "Average cap: 8.130497932434082\n",
      "Number capped: 3\n",
      "Average cap: 0.5767671465873718\n",
      "Number capped: 7\n",
      "Average cap: 5.0441999435424805\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 1\n",
      "Average cap: 32.856327056884766\n",
      "Number capped: 5\n",
      "Average cap: 1.6391617059707642\n",
      "Number capped: 5\n",
      "Average cap: 6.294650077819824\n",
      "Number capped: 2\n",
      "Average cap: 0.23698724806308746\n",
      "Number capped: 2\n",
      "Average cap: 16.81061553955078\n",
      "Number capped: 5\n",
      "Average cap: 1.650887131690979\n",
      "Number capped: 3\n",
      "Average cap: 3.5440587997436523\n",
      "Number capped: 3\n",
      "Average cap: 11.814139366149902\n",
      "Number capped: 5\n",
      "Average cap: 8.508230209350586\n",
      "Number capped: 6\n",
      "Average cap: 5.754171848297119\n",
      "Number capped: 8\n",
      "Average cap: 1.5793789625167847\n",
      "Number capped: 4\n",
      "Average cap: 7.901335716247559\n",
      "Number capped: 7\n",
      "Average cap: 1.0152355432510376\n",
      "Number capped: 4\n",
      "Average cap: 1.548902988433838\n",
      "Number capped: 2\n",
      "Average cap: 16.138525009155273\n",
      "Number capped: 11\n",
      "Average cap: 3.34555721282959\n",
      "Number capped: 2\n",
      "Average cap: 12.458261489868164\n",
      "Number capped: 3\n",
      "Average cap: 5.503397464752197\n",
      "Number capped: 6\n",
      "Average cap: 0.1575639247894287\n",
      "Number capped: 3\n",
      "Average cap: 4.248486042022705\n",
      "Number capped: 5\n",
      "Average cap: 0.548129141330719\n",
      "Number capped: 6\n",
      "Average cap: 0.6213411688804626\n",
      "Number capped: 9\n",
      "Average cap: 4.598108768463135\n",
      "Number capped: 2\n",
      "Average cap: 16.17845916748047\n",
      "Number capped: 3\n",
      "Average cap: 0.0127851702272892\n",
      "Number capped: 4\n",
      "Average cap: 8.362615585327148\n",
      "Number capped: 2\n",
      "Average cap: 0.8669474720954895\n",
      "Number capped: 4\n",
      "Average cap: 0.16821153461933136\n",
      "Number capped: 8\n",
      "Average cap: 5.179279804229736\n",
      "Number capped: 6\n",
      "Average cap: 0.5975509285926819\n",
      "Number capped: 6\n",
      "Average cap: 1.0814696550369263\n",
      "Number capped: 6\n",
      "Average cap: 5.294277667999268\n",
      "Number capped: 5\n",
      "Average cap: 2.5546319484710693\n",
      "Number capped: 6\n",
      "Average cap: 6.641981601715088\n",
      "Number capped: 4\n",
      "Average cap: 1.7108747959136963\n",
      "Number capped: 8\n",
      "Average cap: 0.0488235242664814\n",
      "Number capped: 6\n",
      "Average cap: 5.552196502685547\n",
      "Number capped: 10\n",
      "Average cap: 0.3487618565559387\n",
      "Number capped: 6\n",
      "Average cap: 5.8783721923828125\n",
      "Number capped: 4\n",
      "Average cap: 1.1248087882995605\n",
      "Number capped: 2\n",
      "Average cap: 1.4206490516662598\n",
      "Number capped: 4\n",
      "Average cap: 0.022329822182655334\n",
      "Number capped: 3\n",
      "Average cap: 11.02877140045166\n",
      "Number capped: 7\n",
      "Average cap: 4.670316219329834\n",
      "Number capped: 9\n",
      "Average cap: 0.49069929122924805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 6\n",
      "Average cap: 6.095015048980713\n",
      "Number capped: 5\n",
      "Average cap: 0.3131386935710907\n",
      "Number capped: 4\n",
      "Average cap: 13.200911521911621\n",
      "Number capped: 8\n",
      "Average cap: 3.7854881286621094\n",
      "Number capped: 1\n",
      "Average cap: 0.04966791719198227\n",
      "Number capped: 5\n",
      "Average cap: 7.396573066711426\n",
      "Number capped: 4\n",
      "Average cap: 0.7339433431625366\n",
      "Number capped: 6\n",
      "Average cap: 1.5837541818618774\n",
      "Number capped: 7\n",
      "Average cap: 0.4575585424900055\n",
      "Number capped: 3\n",
      "Average cap: 3.496603012084961\n",
      "Number capped: 5\n",
      "Average cap: 4.014744281768799\n",
      "Number capped: 5\n",
      "Average cap: 6.232168197631836\n",
      "Number capped: 3\n",
      "Average cap: 13.203932762145996\n",
      "Number capped: 4\n",
      "Average cap: 19.435382843017578\n",
      "Number capped: 3\n",
      "Average cap: 11.532317161560059\n",
      "Number capped: 5\n",
      "Average cap: 6.216880798339844\n",
      "Number capped: 4\n",
      "Average cap: 14.479626655578613\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 15.489592552185059\n",
      "Number capped: 7\n",
      "Average cap: 4.709282398223877\n",
      "Number capped: 2\n",
      "Average cap: 16.62773895263672\n",
      "Number capped: 5\n",
      "Average cap: 6.4830498695373535\n",
      "Number capped: 4\n",
      "Average cap: 9.563179969787598\n",
      "Number capped: 5\n",
      "Average cap: 6.778051853179932\n",
      "Number capped: 2\n",
      "Average cap: 22.59650993347168\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 5\n",
      "Average cap: 0.18674235045909882\n",
      "Number capped: 9\n",
      "Average cap: 3.5114431381225586\n",
      "Number capped: 7\n",
      "Average cap: 3.344428300857544\n",
      "Number capped: 2\n",
      "Average cap: 28.140344619750977\n",
      "Number capped: 3\n",
      "Average cap: 8.084155082702637\n",
      "Number capped: 3\n",
      "Average cap: 11.153510093688965\n",
      "Number capped: 4\n",
      "Average cap: 0.2066091150045395\n",
      "Number capped: 9\n",
      "Average cap: 3.3675761222839355\n",
      "Number capped: 7\n",
      "Average cap: 5.4449238777160645\n",
      "Number capped: 4\n",
      "Average cap: 3.2727832794189453\n",
      "Number capped: 10\n",
      "Average cap: 4.081536293029785\n",
      "Number capped: 7\n",
      "Average cap: 0.49502256512641907\n",
      "Number capped: 6\n",
      "Average cap: 0.5108148455619812\n",
      "Number capped: 3\n",
      "Average cap: 10.274321556091309\n",
      "Number capped: 9\n",
      "Average cap: 3.6371142864227295\n",
      "Number capped: 1\n",
      "Average cap: 31.874656677246094\n",
      "Number capped: 5\n",
      "Average cap: 2.833014726638794\n",
      "Number capped: 6\n",
      "Average cap: 4.9513959884643555\n",
      "Number capped: 1\n",
      "Average cap: 33.17995834350586\n",
      "Number capped: 4\n",
      "Average cap: 1.0153398513793945\n",
      "Number capped: 3\n",
      "Average cap: 10.83014965057373\n",
      "Number capped: 3\n",
      "Average cap: 0.6320792436599731\n",
      "Number capped: 3\n",
      "Average cap: 6.4067840576171875\n",
      "Number capped: 4\n",
      "Average cap: 0.5305687785148621\n",
      "Number capped: 7\n",
      "Average cap: 4.206161975860596\n",
      "Number capped: 5\n",
      "Average cap: 6.476110935211182\n",
      "Number capped: 5\n",
      "Average cap: 6.626049995422363\n",
      "Number capped: 7\n",
      "Average cap: 5.745954990386963\n",
      "Number capped: 3\n",
      "Average cap: 11.00706958770752\n",
      "Number capped: 4\n",
      "Average cap: 0.5511876344680786\n",
      "Number capped: 3\n",
      "Average cap: 4.560894966125488\n",
      "Number capped: 2\n",
      "Average cap: 15.57412338256836\n",
      "Number capped: 7\n",
      "Average cap: 0.23240426182746887\n",
      "Number capped: 3\n",
      "Average cap: 0.603809118270874\n",
      "Number capped: 2\n",
      "Average cap: 0.5200436115264893\n",
      "Number capped: 2\n",
      "Average cap: 0.0440719872713089\n",
      "Number capped: 4\n",
      "Average cap: 3.777553081512451\n",
      "Number capped: 3\n",
      "Average cap: 0.04780808091163635\n",
      "Number capped: 2\n",
      "Average cap: 0.49509671330451965\n",
      "Number capped: 11\n",
      "Average cap: 2.4915850162506104\n",
      "Number capped: 3\n",
      "Average cap: 11.428851127624512\n",
      "Number capped: 7\n",
      "Average cap: 4.872958660125732\n",
      "Number capped: 3\n",
      "Average cap: 1.3521758317947388\n",
      "Number capped: 8\n",
      "Average cap: 0.3308562934398651\n",
      "Number capped: 2\n",
      "Average cap: 16.080371856689453\n",
      "Number capped: 9\n",
      "Average cap: 3.617630958557129\n",
      "Number capped: 6\n",
      "Average cap: 5.2984442710876465\n",
      "Number capped: 6\n",
      "Average cap: 5.887287616729736\n",
      "Number capped: 7\n",
      "Average cap: 4.9047136306762695\n",
      "Number capped: 2\n",
      "Average cap: 0.006243329029530287\n",
      "Number capped: 4\n",
      "Average cap: 7.893226623535156\n",
      "Number capped: 4\n",
      "Average cap: 7.429344177246094\n",
      "Number capped: 8\n",
      "Average cap: 0.410270094871521\n",
      "Number capped: 6\n",
      "Average cap: 0.01675381511449814\n",
      "Number capped: 7\n",
      "Average cap: 7.482986927032471\n",
      "Number capped: 6\n",
      "Average cap: 0.5620279908180237\n",
      "Number capped: 3\n",
      "Average cap: 0.016229653730988503\n",
      "Number capped: 7\n",
      "Average cap: 4.872328758239746\n",
      "Number capped: 7\n",
      "Average cap: 4.061951160430908\n",
      "Number capped: 3\n",
      "Average cap: 0.45479652285575867\n",
      "Number capped: 7\n",
      "Average cap: 3.4217782020568848\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 9\n",
      "Average cap: 3.6183366775512695\n",
      "Number capped: 4\n",
      "Average cap: 3.651660919189453\n",
      "Number capped: 2\n",
      "Average cap: 0.5032047629356384\n",
      "Number capped: 6\n",
      "Average cap: 0.005207248497754335\n",
      "Number capped: 7\n",
      "Average cap: 1.7507139444351196\n",
      "Number capped: 4\n",
      "Average cap: 0.4110061824321747\n",
      "Number capped: 5\n",
      "Average cap: 6.483311653137207\n",
      "Number capped: 8\n",
      "Average cap: 4.310758113861084\n",
      "Number capped: 3\n",
      "Average cap: 13.91046142578125\n",
      "Number capped: 4\n",
      "Average cap: 0.9545243978500366\n",
      "Number capped: 4\n",
      "Average cap: 0.5179760456085205\n",
      "Number capped: 1\n",
      "Average cap: 32.82383346557617\n",
      "Number capped: 4\n",
      "Average cap: 8.389872550964355\n",
      "Number capped: 1\n",
      "Average cap: 13.600565910339355\n",
      "Number capped: 3\n",
      "Average cap: 3.192647933959961\n",
      "Number capped: 6\n",
      "Average cap: 4.751016139984131\n",
      "Number capped: 1\n",
      "Average cap: 32.16351318359375\n",
      "Number capped: 5\n",
      "Average cap: 1.5833806991577148\n",
      "Number capped: 5\n",
      "Average cap: 3.866149425506592\n",
      "Number capped: 4\n",
      "Average cap: 1.547053337097168\n",
      "Number capped: 5\n",
      "Average cap: 7.426164150238037\n",
      "Number capped: 13\n",
      "Average cap: 2.8282887935638428\n",
      "Number capped: 7\n",
      "Average cap: 4.067421913146973\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 2\n",
      "Average cap: 0.009541000239551067\n",
      "Number capped: 2\n",
      "Average cap: 0.5117387771606445\n",
      "Number capped: 2\n",
      "Average cap: 17.34252166748047\n",
      "Number capped: 9\n",
      "Average cap: 0.9292527437210083\n",
      "Number capped: 5\n",
      "Average cap: 0.6993221044540405\n",
      "Number capped: 4\n",
      "Average cap: 1.8814077377319336\n",
      "Number capped: 1\n",
      "Average cap: 32.457237243652344\n",
      "Number capped: 3\n",
      "Average cap: 10.684874534606934\n",
      "Number capped: 4\n",
      "Average cap: 8.49829387664795\n",
      "Number capped: 3\n",
      "Average cap: 5.385010242462158\n",
      "Number capped: 3\n",
      "Average cap: 2.5841243267059326\n",
      "Number capped: 6\n",
      "Average cap: 5.055319309234619\n",
      "Number capped: 1\n",
      "Average cap: 32.95609664916992\n",
      "Number capped: 5\n",
      "Average cap: 0.6509160995483398\n",
      "Number capped: 8\n",
      "Average cap: 5.184779644012451\n",
      "Number capped: 6\n",
      "Average cap: 0.008971166796982288\n",
      "Number capped: 4\n",
      "Average cap: 12.313844680786133\n",
      "Number capped: 5\n",
      "Average cap: 6.531079292297363\n",
      "Number capped: 2\n",
      "Average cap: 16.70048713684082\n",
      "Number capped: 5\n",
      "Average cap: 6.903118133544922\n",
      "Number capped: 8\n",
      "Average cap: 4.913299083709717\n",
      "Number capped: 9\n",
      "Average cap: 3.02405047416687\n",
      "Number capped: 5\n",
      "Average cap: 6.564446926116943\n",
      "Number capped: 5\n",
      "Average cap: 0.5147885084152222\n",
      "Number capped: 6\n",
      "Average cap: 0.5119203925132751\n",
      "Number capped: 5\n",
      "Average cap: 0.19258008897304535\n",
      "Number capped: 10\n",
      "Average cap: 4.356412887573242\n",
      "Number capped: 5\n",
      "Average cap: 0.0005320124328136444\n",
      "Number capped: 7\n",
      "Average cap: 0.7467752695083618\n",
      "Number capped: 5\n",
      "Average cap: 6.5035080909729\n",
      "Number capped: 7\n",
      "Average cap: 4.513928413391113\n",
      "Number capped: 7\n",
      "Average cap: 4.023176670074463\n",
      "Number capped: 6\n",
      "Average cap: 0.7406711578369141\n",
      "Number capped: 6\n",
      "Average cap: 5.378892421722412\n",
      "Number capped: 6\n",
      "Average cap: 5.515735626220703\n",
      "Number capped: 7\n",
      "Average cap: 4.116035461425781\n",
      "Number capped: 6\n",
      "Average cap: 2.7342607975006104\n",
      "Number capped: 9\n",
      "Average cap: 0.8032252192497253\n",
      "Number capped: 5\n",
      "Average cap: 5.928926944732666\n",
      "Number capped: 1\n",
      "Average cap: 0.4924354553222656\n",
      "Number capped: 6\n",
      "Average cap: 5.341729640960693\n",
      "Number capped: 3\n",
      "Average cap: 10.800078392028809\n",
      "Number capped: 5\n",
      "Average cap: 6.0342254638671875\n",
      "Number capped: 5\n",
      "Average cap: 0.007405719719827175\n",
      "Number capped: 3\n",
      "Average cap: 16.765962600708008\n",
      "Number capped: 3\n",
      "Average cap: 10.351578712463379\n",
      "Number capped: 9\n",
      "Average cap: 1.5497195136049413e-06\n",
      "Number capped: 2\n",
      "Average cap: 15.369211196899414\n",
      "Number capped: 3\n",
      "Average cap: 14.000739097595215\n",
      "Number capped: 2\n",
      "Average cap: 6.924563407897949\n",
      "Number capped: 1\n",
      "Average cap: 0.8341163396835327\n",
      "Number capped: 7\n",
      "Average cap: 0.9427108764648438\n",
      "Number capped: 4\n",
      "Average cap: 8.194310188293457\n",
      "Number capped: 7\n",
      "Average cap: 4.118708610534668\n",
      "Number capped: 6\n",
      "Average cap: 5.523405075073242\n",
      "Number capped: 5\n",
      "Average cap: 2.458707809448242\n",
      "Number capped: 2\n",
      "Average cap: 15.786170959472656\n",
      "Number capped: 5\n",
      "Average cap: 6.391848564147949\n",
      "Number capped: 4\n",
      "Average cap: 7.561772346496582\n",
      "Number capped: 5\n",
      "Average cap: 6.796914577484131\n",
      "Number capped: 4\n",
      "Average cap: 8.205809593200684\n",
      "Number capped: 4\n",
      "Average cap: 7.573277473449707\n",
      "Number capped: 6\n",
      "Average cap: 2.392409563064575\n",
      "Number capped: 6\n",
      "Average cap: 5.283305644989014\n",
      "Number capped: 8\n",
      "Average cap: 4.279353141784668\n",
      "Number capped: 10\n",
      "Average cap: 4.638594150543213\n",
      "Number capped: 4\n",
      "Average cap: 7.588181495666504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 6\n",
      "Average cap: 0.5039843916893005\n",
      "Number capped: 7\n",
      "Average cap: 1.0396316051483154\n",
      "Number capped: 2\n",
      "Average cap: 5.870424747467041\n",
      "Number capped: 1\n",
      "Average cap: 3.0598630905151367\n",
      "Number capped: 5\n",
      "Average cap: 6.85167932510376\n",
      "Number capped: 6\n",
      "Average cap: 5.602149486541748\n",
      "Number capped: 6\n",
      "Average cap: 2.4283764362335205\n",
      "Number capped: 4\n",
      "Average cap: 7.8447160720825195\n",
      "Number capped: 4\n",
      "Average cap: 8.016106605529785\n",
      "Number capped: 4\n",
      "Average cap: 1.301150918006897\n",
      "Number capped: 2\n",
      "Average cap: 15.952457427978516\n",
      "Number capped: 2\n",
      "Average cap: 16.078962326049805\n",
      "Number capped: 1\n",
      "Average cap: 15.755142211914062\n",
      "Number capped: 3\n",
      "Average cap: 13.091949462890625\n",
      "Number capped: 6\n",
      "Average cap: 6.234403133392334\n",
      "Number capped: 7\n",
      "Average cap: 4.121714115142822\n",
      "Number capped: 10\n",
      "Average cap: 0.49055999517440796\n",
      "Number capped: 2\n",
      "Average cap: 12.261988639831543\n",
      "Number capped: 3\n",
      "Average cap: 0.012640518136322498\n",
      "Number capped: 4\n",
      "Average cap: 1.0033398866653442\n",
      "Number capped: 5\n",
      "Average cap: 0.5555034875869751\n",
      "Number capped: 6\n",
      "Average cap: 3.9908320903778076\n",
      "Number capped: 4\n",
      "Average cap: 27.92168617248535\n",
      "Number capped: 6\n",
      "Average cap: 0.25385940074920654\n",
      "Number capped: 7\n",
      "Average cap: 5.962281703948975\n",
      "Number capped: 6\n",
      "Average cap: 6.914000034332275\n",
      "Number capped: 7\n",
      "Average cap: 2.2432894706726074\n",
      "Number capped: 4\n",
      "Average cap: 7.671169281005859\n",
      "Number capped: 4\n",
      "Average cap: 0.7325596213340759\n",
      "Number capped: 7\n",
      "Average cap: 4.321547031402588\n",
      "Number capped: 10\n",
      "Average cap: 0.08041702210903168\n",
      "Number capped: 1\n",
      "Average cap: 0.13596883416175842\n",
      "Number capped: 6\n",
      "Average cap: 2.983198881149292\n",
      "Number capped: 3\n",
      "Average cap: 11.460624694824219\n",
      "Number capped: 6\n",
      "Average cap: 1.0173405408859253\n",
      "Number capped: 4\n",
      "Average cap: 7.463685035705566\n",
      "Number capped: 1\n",
      "Average cap: 3.5663318634033203\n",
      "Number capped: 7\n",
      "Average cap: 7.208767414093018\n",
      "Number capped: 5\n",
      "Average cap: 3.332120180130005\n",
      "Number capped: 6\n",
      "Average cap: 6.776989459991455\n",
      "Number capped: 4\n",
      "Average cap: 7.951665878295898\n",
      "Number capped: 5\n",
      "Average cap: 11.66083812713623\n",
      "Number capped: 7\n",
      "Average cap: 0.4954851567745209\n",
      "Number capped: 3\n",
      "Average cap: 10.711071968078613\n",
      "Number capped: 1\n",
      "Average cap: 35.608436584472656\n",
      "Number capped: 3\n",
      "Average cap: 19.034048080444336\n",
      "Number capped: 5\n",
      "Average cap: 10.140448570251465\n",
      "Number capped: 4\n",
      "Average cap: 8.005926132202148\n",
      "Number capped: 7\n",
      "Average cap: 4.199654579162598\n",
      "Number capped: 7\n",
      "Average cap: 7.481442451477051\n",
      "Number capped: 4\n",
      "Average cap: 7.53945779800415\n",
      "Number capped: 9\n",
      "Average cap: 1.5249114036560059\n",
      "Number capped: 4\n",
      "Average cap: 14.105531692504883\n",
      "Number capped: 8\n",
      "Average cap: 4.306513786315918\n",
      "Number capped: 5\n",
      "Average cap: 10.983800888061523\n",
      "Number capped: 7\n",
      "Average cap: 0.8058348894119263\n",
      "Number capped: 3\n",
      "Average cap: 1.6737931966781616\n",
      "Number capped: 5\n",
      "Average cap: 6.625175476074219\n",
      "Number capped: 4\n",
      "Average cap: 12.198524475097656\n",
      "Number capped: 3\n",
      "Average cap: 3.6625959873199463\n",
      "Number capped: 3\n",
      "Average cap: 10.553414344787598\n",
      "Number capped: 5\n",
      "Average cap: 6.2844014167785645\n",
      "Number capped: 4\n",
      "Average cap: 8.287867546081543\n",
      "Number capped: 4\n",
      "Average cap: 7.8881072998046875\n",
      "Number capped: 2\n",
      "Average cap: 16.61504364013672\n",
      "Number capped: 6\n",
      "Average cap: 5.3397650718688965\n",
      "Number capped: 1\n",
      "Average cap: 31.691146850585938\n",
      "Number capped: 6\n",
      "Average cap: 9.382214546203613\n",
      "Number capped: 4\n",
      "Average cap: 0.6630592346191406\n",
      "Number capped: 8\n",
      "Average cap: 0.5837209224700928\n",
      "Number capped: 7\n",
      "Average cap: 4.484055042266846\n",
      "Number capped: 7\n",
      "Average cap: 0.5715969800949097\n",
      "Number capped: 7\n",
      "Average cap: 0.6453381776809692\n",
      "Number capped: 4\n",
      "Average cap: 0.7159062623977661\n",
      "Number capped: 4\n",
      "Average cap: 0.6696329116821289\n",
      "Number capped: 4\n",
      "Average cap: 7.870858669281006\n",
      "Number capped: 6\n",
      "Average cap: 6.495792388916016\n",
      "Number capped: 7\n",
      "Average cap: 3.9494268894195557\n",
      "Number capped: 6\n",
      "Average cap: 5.184969902038574\n",
      "Number capped: 5\n",
      "Average cap: 7.219803810119629\n",
      "Number capped: 6\n",
      "Average cap: 0.49710845947265625\n",
      "Number capped: 4\n",
      "Average cap: 0.49616575241088867\n",
      "Number capped: 6\n",
      "Average cap: 0.5451086759567261\n",
      "Number capped: 9\n",
      "Average cap: 3.0405046939849854\n",
      "Number capped: 4\n",
      "Average cap: 12.8192720413208\n",
      "Number capped: 7\n",
      "Average cap: 4.802252292633057\n",
      "Number capped: 1\n",
      "Average cap: 35.12112808227539\n",
      "Number capped: 5\n",
      "Average cap: 5.333774566650391\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 0.5810938477516174\n",
      "Number capped: 4\n",
      "Average cap: 7.6265153884887695\n",
      "Number capped: 6\n",
      "Average cap: 8.711763381958008\n",
      "Number capped: 4\n",
      "Average cap: 10.91427230834961\n",
      "Number capped: 3\n",
      "Average cap: 10.529953956604004\n",
      "Number capped: 3\n",
      "Average cap: 12.078651428222656\n",
      "Number capped: 10\n",
      "Average cap: 7.8249077796936035\n",
      "Number capped: 8\n",
      "Average cap: 3.3425211906433105\n",
      "Number capped: 9\n",
      "Average cap: 2.569024085998535\n",
      "Number capped: 2\n",
      "Average cap: 17.47785186767578\n",
      "Number capped: 3\n",
      "Average cap: 0.02428198792040348\n",
      "Number capped: 5\n",
      "Average cap: 2.6058545112609863\n",
      "Number capped: 3\n",
      "Average cap: 10.731951713562012\n",
      "Number capped: 7\n",
      "Average cap: 7.469841957092285\n",
      "Number capped: 5\n",
      "Average cap: 5.90435791015625\n",
      "Number capped: 4\n",
      "Average cap: 8.306849479675293\n",
      "Number capped: 6\n",
      "Average cap: 3.455866575241089\n",
      "Number capped: 2\n",
      "Average cap: 26.60271453857422\n",
      "Number capped: 6\n",
      "Average cap: 4.886845111846924\n",
      "Number capped: 2\n",
      "Average cap: 0.49684739112854004\n",
      "Number capped: 6\n",
      "Average cap: 5.204225063323975\n",
      "Number capped: 4\n",
      "Average cap: 7.390362739562988\n",
      "Number capped: 4\n",
      "Average cap: 7.824028968811035\n",
      "Number capped: 4\n",
      "Average cap: 0.4848710894584656\n",
      "Number capped: 4\n",
      "Average cap: 0.49114906787872314\n",
      "Number capped: 6\n",
      "Average cap: 0.15819455683231354\n",
      "Number capped: 2\n",
      "Average cap: 20.78937530517578\n",
      "Number capped: 1\n",
      "Average cap: 2.015043258666992\n",
      "Number capped: 7\n",
      "Average cap: 5.712409019470215\n",
      "Number capped: 1\n",
      "Average cap: 31.57465934753418\n",
      "Number capped: 3\n",
      "Average cap: 3.5575058460235596\n",
      "Number capped: 4\n",
      "Average cap: 13.78542709350586\n",
      "Number capped: 5\n",
      "Average cap: 6.367193222045898\n",
      "Number capped: 2\n",
      "Average cap: 15.662529945373535\n",
      "Number capped: 4\n",
      "Average cap: 0.6396898031234741\n",
      "Number capped: 1\n",
      "Average cap: 23.502552032470703\n",
      "Number capped: 2\n",
      "Average cap: 0.42309650778770447\n",
      "Number capped: 4\n",
      "Average cap: 10.175078392028809\n",
      "Number capped: 7\n",
      "Average cap: 0.5918484330177307\n",
      "Number capped: 5\n",
      "Average cap: 1.7727962732315063\n",
      "Number capped: 7\n",
      "Average cap: 4.56060266494751\n",
      "Number capped: 2\n",
      "Average cap: 17.274763107299805\n",
      "Number capped: 8\n",
      "Average cap: 0.7869223952293396\n",
      "Number capped: 4\n",
      "Average cap: 2.81303334236145\n",
      "Number capped: 1\n",
      "Average cap: 63.122283935546875\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 2\n",
      "Average cap: 15.547905921936035\n",
      "Number capped: 8\n",
      "Average cap: 0.48321533203125\n",
      "Number capped: 4\n",
      "Average cap: 1.6843149662017822\n",
      "Number capped: 4\n",
      "Average cap: 4.613266944885254\n",
      "Number capped: 8\n",
      "Average cap: 0.5732274055480957\n",
      "Number capped: 6\n",
      "Average cap: 7.263807773590088\n",
      "Number capped: 4\n",
      "Average cap: 8.703327178955078\n",
      "Number capped: 6\n",
      "Average cap: 5.289062976837158\n",
      "Number capped: 5\n",
      "Average cap: 1.0574766397476196\n",
      "Number capped: 7\n",
      "Average cap: 0.9059545397758484\n",
      "Number capped: 4\n",
      "Average cap: 7.344675540924072\n",
      "Number capped: 9\n",
      "Average cap: 4.334179878234863\n",
      "Number capped: 5\n",
      "Average cap: 5.8231730461120605\n",
      "Number capped: 5\n",
      "Average cap: 8.848310470581055\n",
      "Number capped: 11\n",
      "Average cap: 2.3688127994537354\n",
      "Number capped: 6\n",
      "Average cap: 0.48470279574394226\n",
      "Number capped: 5\n",
      "Average cap: 4.7590742111206055\n",
      "Number capped: 7\n",
      "Average cap: 5.092923641204834\n",
      "Number capped: 3\n",
      "Average cap: 2.3683714866638184\n",
      "Number capped: 6\n",
      "Average cap: 7.577648162841797\n",
      "Number capped: 4\n",
      "Average cap: 7.728361129760742\n",
      "Number capped: 2\n",
      "Average cap: 26.808813095092773\n",
      "Number capped: 2\n",
      "Average cap: 15.089916229248047\n",
      "Number capped: 6\n",
      "Average cap: 5.5704874992370605\n",
      "Number capped: 3\n",
      "Average cap: 0.49775004386901855\n",
      "Number capped: 2\n",
      "Average cap: 11.388757705688477\n",
      "Number capped: 5\n",
      "Average cap: 0.750583291053772\n",
      "Number capped: 5\n",
      "Average cap: 7.823712348937988\n",
      "Number capped: 2\n",
      "Average cap: 16.072040557861328\n",
      "Number capped: 6\n",
      "Average cap: 9.498438835144043\n",
      "Number capped: 5\n",
      "Average cap: 0.16744263470172882\n",
      "Number capped: 3\n",
      "Average cap: 0.00075925188139081\n",
      "Number capped: 4\n",
      "Average cap: 0.0845702588558197\n",
      "Number capped: 5\n",
      "Average cap: 0.03435584157705307\n",
      "Number capped: 7\n",
      "Average cap: 0.17829866707324982\n",
      "Number capped: 11\n",
      "Average cap: 3.8380706310272217\n",
      "Number capped: 1\n",
      "Average cap: 0.8545646667480469\n",
      "Number capped: 4\n",
      "Average cap: 0.002721274271607399\n",
      "Number capped: 9\n",
      "Average cap: 0.005604900885373354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 6\n",
      "Average cap: 5.086030960083008\n",
      "Number capped: 7\n",
      "Average cap: 4.4710612297058105\n",
      "Number capped: 5\n",
      "Average cap: 0.012740653939545155\n",
      "Number capped: 4\n",
      "Average cap: 8.23962688446045\n",
      "Number capped: 7\n",
      "Average cap: 0.495380163192749\n",
      "Number capped: 5\n",
      "Average cap: 6.308579444885254\n",
      "Number capped: 5\n",
      "Average cap: 5.6485185623168945\n",
      "Number capped: 9\n",
      "Average cap: 4.791635513305664\n",
      "Number capped: 4\n",
      "Average cap: 7.688664436340332\n",
      "Number capped: 5\n",
      "Average cap: 0.9842878580093384\n",
      "Number capped: 3\n",
      "Average cap: 10.777652740478516\n",
      "Number capped: 5\n",
      "Average cap: 5.772961616516113\n",
      "Number capped: 5\n",
      "Average cap: 6.242371559143066\n",
      "Number capped: 7\n",
      "Average cap: 2.541588544845581\n",
      "Number capped: 5\n",
      "Average cap: 0.48860853910446167\n",
      "Number capped: 3\n",
      "Average cap: 10.53653621673584\n",
      "Number capped: 3\n",
      "Average cap: 10.51305103302002\n",
      "Number capped: 5\n",
      "Average cap: 6.6999030113220215\n",
      "Number capped: 5\n",
      "Average cap: 5.729559898376465\n",
      "Number capped: 4\n",
      "Average cap: 12.034333229064941\n",
      "Number capped: 2\n",
      "Average cap: 14.81990909576416\n",
      "Number capped: 4\n",
      "Average cap: 14.234455108642578\n",
      "Number capped: 14\n",
      "Average cap: 0.6047071814537048\n",
      "Number capped: 5\n",
      "Average cap: 6.151068687438965\n",
      "Number capped: 3\n",
      "Average cap: 1.2615078687667847\n",
      "Number capped: 2\n",
      "Average cap: 15.749778747558594\n",
      "Number capped: 5\n",
      "Average cap: 9.134119987487793\n",
      "Number capped: 5\n",
      "Average cap: 0.05684218928217888\n",
      "Number capped: 4\n",
      "Average cap: 11.815935134887695\n",
      "Number capped: 2\n",
      "Average cap: 2.618356704711914\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 2\n",
      "Average cap: 21.462032318115234\n",
      "Number capped: 5\n",
      "Average cap: 6.4943366050720215\n",
      "Number capped: 5\n",
      "Average cap: 6.347775936126709\n",
      "Number capped: 5\n",
      "Average cap: 0.003878453280776739\n",
      "Number capped: 5\n",
      "Average cap: 0.011455930769443512\n",
      "Number capped: 1\n",
      "Average cap: 0.46918243169784546\n",
      "Number capped: 5\n",
      "Average cap: 1.8027746677398682\n",
      "Number capped: 4\n",
      "Average cap: 0.6034002900123596\n",
      "Number capped: 4\n",
      "Average cap: 7.24636697769165\n",
      "Number capped: 2\n",
      "Average cap: 0.6968228816986084\n",
      "Number capped: 1\n",
      "Average cap: 31.31845474243164\n",
      "Number capped: 2\n",
      "Average cap: 15.739583015441895\n",
      "Number capped: 4\n",
      "Average cap: 7.862593173980713\n",
      "Number capped: 4\n",
      "Average cap: 7.5336151123046875\n",
      "Number capped: 5\n",
      "Average cap: 1.8733171224594116\n",
      "Number capped: 2\n",
      "Average cap: 15.137112617492676\n",
      "Number capped: 5\n",
      "Average cap: 8.152896881103516\n",
      "Number capped: 3\n",
      "Average cap: 0.563709557056427\n",
      "Number capped: 3\n",
      "Average cap: 9.831419944763184\n",
      "Number capped: 5\n",
      "Average cap: 6.091695785522461\n",
      "Number capped: 5\n",
      "Average cap: 2.2327632904052734\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 1\n",
      "Average cap: 121.329345703125\n",
      "Number capped: 6\n",
      "Average cap: 0.11892858147621155\n",
      "Number capped: 6\n",
      "Average cap: 0.48554620146751404\n",
      "Number capped: 4\n",
      "Average cap: 9.346416473388672\n",
      "Number capped: 4\n",
      "Average cap: 8.221904754638672\n",
      "Number capped: 5\n",
      "Average cap: 7.1610212326049805\n",
      "Number capped: 4\n",
      "Average cap: 8.776165962219238\n",
      "Number capped: 7\n",
      "Average cap: 4.278319358825684\n",
      "Number capped: 4\n",
      "Average cap: 0.05634584277868271\n",
      "Number capped: 4\n",
      "Average cap: 6.684393405914307\n",
      "Number capped: 7\n",
      "Average cap: 4.938548564910889\n",
      "Number capped: 2\n",
      "Average cap: 0.6594671607017517\n",
      "Number capped: 6\n",
      "Average cap: 0.552453339099884\n",
      "Number capped: 4\n",
      "Average cap: 8.038481712341309\n",
      "Number capped: 3\n",
      "Average cap: 11.322999000549316\n",
      "Number capped: 3\n",
      "Average cap: 1.845169186592102\n",
      "Number capped: 11\n",
      "Average cap: 2.909431219100952\n",
      "Number capped: 5\n",
      "Average cap: 6.720456123352051\n",
      "Number capped: 2\n",
      "Average cap: 3.624612331390381\n",
      "Number capped: 4\n",
      "Average cap: 7.955907821655273\n",
      "Number capped: 3\n",
      "Average cap: 10.459592819213867\n",
      "Number capped: 3\n",
      "Average cap: 10.185505867004395\n",
      "Number capped: 5\n",
      "Average cap: 3.094569683074951\n",
      "Number capped: 6\n",
      "Average cap: 1.6245756149291992\n",
      "Number capped: 4\n",
      "Average cap: 14.360827445983887\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 7\n",
      "Average cap: 0.676430344581604\n",
      "Number capped: 5\n",
      "Average cap: 11.276044845581055\n",
      "Number capped: 5\n",
      "Average cap: 0.2750077545642853\n",
      "Number capped: 1\n",
      "Average cap: 1.007699966430664\n",
      "Number capped: 4\n",
      "Average cap: 8.105826377868652\n",
      "Number capped: 5\n",
      "Average cap: 0.2796350121498108\n",
      "Number capped: 8\n",
      "Average cap: 0.7770969867706299\n",
      "Number capped: 3\n",
      "Average cap: 9.975479125976562\n",
      "Number capped: 5\n",
      "Average cap: 0.47739076614379883\n",
      "Number capped: 6\n",
      "Average cap: 0.8733639717102051\n",
      "Number capped: 4\n",
      "Average cap: 0.5090816020965576\n",
      "Number capped: 2\n",
      "Average cap: 3.337858652230352e-05\n",
      "Number capped: 4\n",
      "Average cap: 0.48045673966407776\n",
      "Number capped: 4\n",
      "Average cap: 7.760696887969971\n",
      "Number capped: 7\n",
      "Average cap: 5.408598899841309\n",
      "Number capped: 6\n",
      "Average cap: 5.489455699920654\n",
      "Number capped: 3\n",
      "Average cap: 11.692919731140137\n",
      "Number capped: 4\n",
      "Average cap: 0.3364914655685425\n",
      "Number capped: 2\n",
      "Average cap: 15.288229942321777\n",
      "Number capped: 8\n",
      "Average cap: 4.620746612548828\n",
      "Number capped: 3\n",
      "Average cap: 0.4745040237903595\n",
      "Number capped: 5\n",
      "Average cap: 0.24258604645729065\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 51.74424743652344\n",
      "Number capped: 5\n",
      "Average cap: 6.078805446624756\n",
      "Number capped: 4\n",
      "Average cap: 8.22718334197998\n",
      "Number capped: 6\n",
      "Average cap: 9.122493743896484\n",
      "Number capped: 1\n",
      "Average cap: 59.203800201416016\n",
      "Number capped: 3\n",
      "Average cap: 3.404383659362793\n",
      "Number capped: 8\n",
      "Average cap: 2.861583948135376\n",
      "Number capped: 1\n",
      "Average cap: 31.007503509521484\n",
      "Number capped: 4\n",
      "Average cap: 5.218197822570801\n",
      "Number capped: 6\n",
      "Average cap: 0.48724493384361267\n",
      "Number capped: 7\n",
      "Average cap: 0.7330880165100098\n",
      "Number capped: 6\n",
      "Average cap: 0.4941887855529785\n",
      "Number capped: 3\n",
      "Average cap: 1.6601694822311401\n",
      "Number capped: 9\n",
      "Average cap: 0.6143099069595337\n",
      "Number capped: 4\n",
      "Average cap: 8.223895072937012\n",
      "Number capped: 3\n",
      "Average cap: 1.8799995183944702\n",
      "Number capped: 3\n",
      "Average cap: 17.870742797851562\n",
      "Number capped: 3\n",
      "Average cap: 0.6270168423652649\n",
      "Number capped: 8\n",
      "Average cap: 3.257561206817627\n",
      "Number capped: 6\n",
      "Average cap: 1.4357637166976929\n",
      "Number capped: 6\n",
      "Average cap: 6.901556491851807\n",
      "Number capped: 4\n",
      "Average cap: 8.10531997680664\n",
      "Number capped: 7\n",
      "Average cap: 4.070192337036133\n",
      "Number capped: 2\n",
      "Average cap: 18.03180503845215\n",
      "Number capped: 3\n",
      "Average cap: 12.793206214904785\n",
      "Number capped: 3\n",
      "Average cap: 1.8159424143959768e-05\n",
      "Number capped: 6\n",
      "Average cap: 0.03844698891043663\n",
      "Number capped: 5\n",
      "Average cap: 1.0530704259872437\n",
      "Number capped: 4\n",
      "Average cap: 7.426019191741943\n",
      "Number capped: 2\n",
      "Average cap: 15.347074508666992\n",
      "Number capped: 6\n",
      "Average cap: 4.646650314331055\n",
      "Number capped: 3\n",
      "Average cap: 12.65160083770752\n",
      "Number capped: 4\n",
      "Average cap: 0.05517883226275444\n",
      "Number capped: 3\n",
      "Average cap: 19.196435928344727\n",
      "Number capped: 4\n",
      "Average cap: 0.845671534538269\n",
      "Number capped: 3\n",
      "Average cap: 10.490890502929688\n",
      "Number capped: 5\n",
      "Average cap: 0.05160068720579147\n",
      "Number capped: 5\n",
      "Average cap: 5.64623498916626\n",
      "Number capped: 3\n",
      "Average cap: 11.219487190246582\n",
      "Number capped: 2\n",
      "Average cap: 0.08318348228931427\n",
      "Number capped: 4\n",
      "Average cap: 10.075806617736816\n",
      "Number capped: 1\n",
      "Average cap: 4.764716148376465\n",
      "Number capped: 6\n",
      "Average cap: 3.3511435985565186\n",
      "Number capped: 7\n",
      "Average cap: 4.683297157287598\n",
      "Number capped: 3\n",
      "Average cap: 10.340540885925293\n",
      "Number capped: 3\n",
      "Average cap: 0.10377431660890579\n",
      "Number capped: 7\n",
      "Average cap: 4.4950079917907715\n",
      "Number capped: 5\n",
      "Average cap: 5.640041351318359\n",
      "Number capped: 7\n",
      "Average cap: 6.362525939941406\n",
      "Number capped: 6\n",
      "Average cap: 5.230948448181152\n",
      "Number capped: 4\n",
      "Average cap: 7.496774196624756\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 8\n",
      "Average cap: 4.808152198791504\n",
      "Number capped: 5\n",
      "Average cap: 8.62510871887207\n",
      "Number capped: 4\n",
      "Average cap: 7.098404407501221\n",
      "Number capped: 2\n",
      "Average cap: 4.031375885009766\n",
      "Number capped: 7\n",
      "Average cap: 4.632185935974121\n",
      "Number capped: 5\n",
      "Average cap: 0.6598480939865112\n",
      "Number capped: 3\n",
      "Average cap: 0.2510465085506439\n",
      "Number capped: 4\n",
      "Average cap: 7.681280136108398\n",
      "Number capped: 1\n",
      "Average cap: 16.526660919189453\n",
      "Number capped: 7\n",
      "Average cap: 8.627171516418457\n",
      "Number capped: 6\n",
      "Average cap: 0.5254498720169067\n",
      "Number capped: 2\n",
      "Average cap: 18.25576400756836\n",
      "Number capped: 5\n",
      "Average cap: 6.029070854187012\n",
      "Number capped: 6\n",
      "Average cap: 4.143986225128174\n",
      "Number capped: 1\n",
      "Average cap: 32.48752212524414\n",
      "Number capped: 8\n",
      "Average cap: 3.31569766998291\n",
      "Number capped: 6\n",
      "Average cap: 5.116376876831055\n",
      "Number capped: 6\n",
      "Average cap: 5.157756328582764\n",
      "Number capped: 6\n",
      "Average cap: 4.66947078704834\n",
      "Number capped: 5\n",
      "Average cap: 6.391045093536377\n",
      "Number capped: 5\n",
      "Average cap: 0.19308818876743317\n",
      "Number capped: 5\n",
      "Average cap: 3.591799259185791\n",
      "Number capped: 3\n",
      "Average cap: 9.899701118469238\n",
      "Number capped: 3\n",
      "Average cap: 1.033591866493225\n",
      "Number capped: 6\n",
      "Average cap: 6.246187686920166\n",
      "Number capped: 2\n",
      "Average cap: 20.160070419311523\n",
      "Number capped: 1\n",
      "Average cap: 29.654943466186523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 6\n",
      "Average cap: 0.48132407665252686\n",
      "Number capped: 8\n",
      "Average cap: 3.772350311279297\n",
      "Number capped: 3\n",
      "Average cap: 11.774467468261719\n",
      "Number capped: 5\n",
      "Average cap: 6.293598651885986\n",
      "Number capped: 4\n",
      "Average cap: 8.258671760559082\n",
      "Number capped: 3\n",
      "Average cap: 9.589556694030762\n",
      "Number capped: 3\n",
      "Average cap: 1.2645118236541748\n",
      "Number capped: 4\n",
      "Average cap: 8.082391738891602\n",
      "Number capped: 2\n",
      "Average cap: 15.894678115844727\n",
      "Number capped: 6\n",
      "Average cap: 4.524595260620117\n",
      "Number capped: 7\n",
      "Average cap: 0.6502658128738403\n",
      "Number capped: 6\n",
      "Average cap: 4.490406513214111\n",
      "Number capped: 9\n",
      "Average cap: 4.345111846923828\n",
      "Number capped: 5\n",
      "Average cap: 12.016526222229004\n",
      "Number capped: 5\n",
      "Average cap: 6.287769317626953\n",
      "Number capped: 6\n",
      "Average cap: 5.22969388961792\n",
      "Number capped: 4\n",
      "Average cap: 1.1727187633514404\n",
      "Number capped: 4\n",
      "Average cap: 7.608427047729492\n",
      "Number capped: 3\n",
      "Average cap: 0.5137276649475098\n",
      "Number capped: 2\n",
      "Average cap: 1.4676047563552856\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 5\n",
      "Average cap: 0.10770127922296524\n",
      "Number capped: 3\n",
      "Average cap: 12.830063819885254\n",
      "Number capped: 1\n",
      "Average cap: 5.967443943023682\n",
      "Number capped: 3\n",
      "Average cap: 0.04542404040694237\n",
      "Number capped: 7\n",
      "Average cap: 4.357943534851074\n",
      "Number capped: 5\n",
      "Average cap: 10.132692337036133\n",
      "Number capped: 4\n",
      "Average cap: 11.721702575683594\n",
      "Number capped: 1\n",
      "Average cap: 2.2216694355010986\n",
      "Number capped: 5\n",
      "Average cap: 0.0060286629013717175\n",
      "Number capped: 2\n",
      "Average cap: 14.939282417297363\n",
      "Number capped: 4\n",
      "Average cap: 13.257967948913574\n",
      "Number capped: 7\n",
      "Average cap: 4.3545942306518555\n",
      "Number capped: 5\n",
      "Average cap: 5.5373005867004395\n",
      "Number capped: 3\n",
      "Average cap: 5.390983581542969\n",
      "Number capped: 5\n",
      "Average cap: 6.6879472732543945\n",
      "Number capped: 4\n",
      "Average cap: 0.012009126134216785\n",
      "Number capped: 7\n",
      "Average cap: 4.934330463409424\n",
      "Number capped: 7\n",
      "Average cap: 3.7981340885162354\n",
      "Number capped: 5\n",
      "Average cap: 7.559411525726318\n",
      "Number capped: 3\n",
      "Average cap: 10.365264892578125\n",
      "Number capped: 4\n",
      "Average cap: 7.2355451583862305\n",
      "Number capped: 8\n",
      "Average cap: 6.08792781829834\n",
      "Number capped: 3\n",
      "Average cap: 15.302131652832031\n",
      "Number capped: 2\n",
      "Average cap: 14.54637336730957\n",
      "Number capped: 6\n",
      "Average cap: 0.38745489716529846\n",
      "Number capped: 5\n",
      "Average cap: 0.7139818668365479\n",
      "Number capped: 1\n",
      "Average cap: 7.335660934448242\n",
      "Number capped: 3\n",
      "Average cap: 11.071853637695312\n",
      "Number capped: 2\n",
      "Average cap: 46.342620849609375\n",
      "Number capped: 4\n",
      "Average cap: 2.8271310329437256\n",
      "Number capped: 5\n",
      "Average cap: 5.633967399597168\n",
      "Number capped: 2\n",
      "Average cap: 0.00015270667790900916\n",
      "Number capped: 3\n",
      "Average cap: 0.549231231212616\n",
      "Number capped: 5\n",
      "Average cap: 10.106074333190918\n",
      "Number capped: 4\n",
      "Average cap: 7.040084362030029\n",
      "Number capped: 5\n",
      "Average cap: 6.468560695648193\n",
      "Number capped: 4\n",
      "Average cap: 0.01275419257581234\n",
      "Number capped: 6\n",
      "Average cap: 1.4108271598815918\n",
      "Number capped: 5\n",
      "Average cap: 1.6451499462127686\n",
      "Number capped: 2\n",
      "Average cap: 0.0019699912518262863\n",
      "Number capped: 6\n",
      "Average cap: 0.6467015743255615\n",
      "Number capped: 6\n",
      "Average cap: 4.683727741241455\n",
      "Number capped: 5\n",
      "Average cap: 1.8742551803588867\n",
      "Number capped: 6\n",
      "Average cap: 4.522275447845459\n",
      "Number capped: 2\n",
      "Average cap: 3.219789505004883\n",
      "Number capped: 3\n",
      "Average cap: 0.48005810379981995\n",
      "Number capped: 8\n",
      "Average cap: 1.443205714225769\n",
      "Number capped: 1\n",
      "Average cap: 30.78476905822754\n",
      "Number capped: 3\n",
      "Average cap: 0.4018993377685547\n",
      "Number capped: 3\n",
      "Average cap: 0.7112757563591003\n",
      "Number capped: 8\n",
      "Average cap: 4.358191967010498\n",
      "Number capped: 4\n",
      "Average cap: 6.030069351196289\n",
      "Number capped: 5\n",
      "Average cap: 0.0005499949911609292\n",
      "Number capped: 4\n",
      "Average cap: 1.3160768747329712\n",
      "Number capped: 7\n",
      "Average cap: 4.110813617706299\n",
      "Number capped: 3\n",
      "Average cap: 10.33469295501709\n",
      "Number capped: 5\n",
      "Average cap: 5.858154773712158\n",
      "Number capped: 4\n",
      "Average cap: 13.412019729614258\n",
      "Number capped: 5\n",
      "Average cap: 6.495701789855957\n",
      "Number capped: 6\n",
      "Average cap: 4.423274040222168\n",
      "Number capped: 3\n",
      "Average cap: 10.042891502380371\n",
      "Number capped: 2\n",
      "Average cap: 1.1447911262512207\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 6\n",
      "Average cap: 4.970936298370361\n",
      "Number capped: 3\n",
      "Average cap: 0.8630754351615906\n",
      "Number capped: 3\n",
      "Average cap: 0.4782157838344574\n",
      "Number capped: 6\n",
      "Average cap: 5.410579681396484\n",
      "Number capped: 6\n",
      "Average cap: 0.47126317024230957\n",
      "Number capped: 3\n",
      "Average cap: 7.409637451171875\n",
      "Number capped: 4\n",
      "Average cap: 0.48605597019195557\n",
      "Number capped: 8\n",
      "Average cap: 0.7136437296867371\n",
      "Number capped: 2\n",
      "Average cap: 15.176767349243164\n",
      "Number capped: 5\n",
      "Average cap: 6.219362735748291\n",
      "Number capped: 11\n",
      "Average cap: 3.227123498916626\n",
      "Number capped: 7\n",
      "Average cap: 4.063253879547119\n",
      "Number capped: 4\n",
      "Average cap: 7.6358795166015625\n",
      "Number capped: 4\n",
      "Average cap: 7.970815658569336\n",
      "Number capped: 8\n",
      "Average cap: 2.613431453704834\n",
      "Number capped: 4\n",
      "Average cap: 8.891979217529297\n",
      "Number capped: 4\n",
      "Average cap: 13.548412322998047\n",
      "Number capped: 3\n",
      "Average cap: 0.4783306419849396\n",
      "Number capped: 4\n",
      "Average cap: 9.839862823486328\n",
      "Number capped: 3\n",
      "Average cap: 0.5739526152610779\n",
      "Number capped: 5\n",
      "Average cap: 6.112112522125244\n",
      "Number capped: 2\n",
      "Average cap: 17.178756713867188\n",
      "Number capped: 8\n",
      "Average cap: 0.033894531428813934\n",
      "Number capped: 1\n",
      "Average cap: 42.10343933105469\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 3.7870748043060303\n",
      "Number capped: 3\n",
      "Average cap: 82.99845123291016\n",
      "Number capped: 3\n",
      "Average cap: 0.08403114229440689\n",
      "Number capped: 2\n",
      "Average cap: 14.968765258789062\n",
      "Number capped: 1\n",
      "Average cap: 33.70109558105469\n",
      "Number capped: 10\n",
      "Average cap: 0.4552178978919983\n",
      "Number capped: 6\n",
      "Average cap: 5.04231595993042\n",
      "Number capped: 5\n",
      "Average cap: 0.05918063595890999\n",
      "Number capped: 6\n",
      "Average cap: 0.0064176470041275024\n",
      "Number capped: 8\n",
      "Average cap: 0.30310359597206116\n",
      "Number capped: 3\n",
      "Average cap: 9.665818214416504\n",
      "Number capped: 4\n",
      "Average cap: 7.030200481414795\n",
      "Number capped: 5\n",
      "Average cap: 0.5106512308120728\n",
      "Number capped: 4\n",
      "Average cap: 1.1082524061203003\n",
      "Number capped: 8\n",
      "Average cap: 3.9986228942871094\n",
      "Number capped: 8\n",
      "Average cap: 5.395633220672607\n",
      "Number capped: 5\n",
      "Average cap: 10.0281343460083\n",
      "Number capped: 5\n",
      "Average cap: 6.006282806396484\n",
      "Number capped: 8\n",
      "Average cap: 3.684626340866089\n",
      "Number capped: 1\n",
      "Average cap: 55.835479736328125\n",
      "Number capped: 1\n",
      "Average cap: 0.06631504744291306\n",
      "Number capped: 6\n",
      "Average cap: 2.6128995418548584\n",
      "Number capped: 1\n",
      "Average cap: 0.686737596988678\n",
      "Number capped: 5\n",
      "Average cap: 6.011005401611328\n",
      "Number capped: 3\n",
      "Average cap: 1.1120424270629883\n",
      "Number capped: 5\n",
      "Average cap: 6.024631023406982\n",
      "Number capped: 3\n",
      "Average cap: 0.7579803466796875\n",
      "Number capped: 3\n",
      "Average cap: 10.718125343322754\n",
      "Number capped: 4\n",
      "Average cap: 7.8845977783203125\n",
      "Number capped: 4\n",
      "Average cap: 8.078971862792969\n",
      "Number capped: 8\n",
      "Average cap: 3.9504685401916504\n",
      "Number capped: 3\n",
      "Average cap: 0.5204976201057434\n",
      "Number capped: 7\n",
      "Average cap: 5.081991672515869\n",
      "Number capped: 3\n",
      "Average cap: 0.2178899049758911\n",
      "Number capped: 5\n",
      "Average cap: 15.292811393737793\n",
      "Number capped: 1\n",
      "Average cap: 2.2888181774760596e-05\n",
      "Number capped: 5\n",
      "Average cap: 0.48885583877563477\n",
      "Number capped: 6\n",
      "Average cap: 4.677736759185791\n",
      "Number capped: 7\n",
      "Average cap: 0.20133261382579803\n",
      "Number capped: 10\n",
      "Average cap: 3.320355176925659\n",
      "Number capped: 8\n",
      "Average cap: 0.27558547258377075\n",
      "Number capped: 8\n",
      "Average cap: 1.6275720596313477\n",
      "Number capped: 8\n",
      "Average cap: 4.782041072845459\n",
      "Number capped: 2\n",
      "Average cap: 17.728240966796875\n",
      "Number capped: 2\n",
      "Average cap: 0.8652432560920715\n",
      "Number capped: 4\n",
      "Average cap: 7.137526035308838\n",
      "Number capped: 3\n",
      "Average cap: 9.386716842651367\n",
      "Number capped: 8\n",
      "Average cap: 5.481868743896484\n",
      "Number capped: 5\n",
      "Average cap: 3.3122692108154297\n",
      "Number capped: 2\n",
      "Average cap: 7.573581218719482\n",
      "Number capped: 6\n",
      "Average cap: 8.253530502319336\n",
      "Number capped: 2\n",
      "Average cap: 14.351067543029785\n",
      "Number capped: 5\n",
      "Average cap: 6.075913429260254\n",
      "Number capped: 7\n",
      "Average cap: 6.191806793212891\n",
      "Number capped: 11\n",
      "Average cap: 2.2971038818359375\n",
      "Number capped: 14\n",
      "Average cap: 2.9517531394958496\n",
      "Number capped: 3\n",
      "Average cap: 9.828520774841309\n",
      "Number capped: 2\n",
      "Average cap: 0.45722028613090515\n",
      "Number capped: 4\n",
      "Average cap: 3.911156415939331\n",
      "Number capped: 5\n",
      "Average cap: 0.5461621284484863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 2\n",
      "Average cap: 17.105520248413086\n",
      "Number capped: 1\n",
      "Average cap: 78.77520751953125\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 8\n",
      "Average cap: 3.809926986694336\n",
      "Number capped: 2\n",
      "Average cap: 25.62457847595215\n",
      "Number capped: 3\n",
      "Average cap: 9.664605140686035\n",
      "Number capped: 8\n",
      "Average cap: 3.7075881958007812\n",
      "Number capped: 7\n",
      "Average cap: 4.454525947570801\n",
      "Number capped: 7\n",
      "Average cap: 4.646891117095947\n",
      "Number capped: 5\n",
      "Average cap: 7.028245449066162\n",
      "Number capped: 1\n",
      "Average cap: 0.509493887424469\n",
      "Number capped: 5\n",
      "Average cap: 1.0502389669418335\n",
      "Number capped: 2\n",
      "Average cap: 15.01043701171875\n",
      "Number capped: 4\n",
      "Average cap: 7.34165096282959\n",
      "Number capped: 3\n",
      "Average cap: 10.022614479064941\n",
      "Number capped: 7\n",
      "Average cap: 7.780510902404785\n",
      "Number capped: 5\n",
      "Average cap: 3.481822967529297\n",
      "Number capped: 2\n",
      "Average cap: 15.053067207336426\n",
      "Number capped: 1\n",
      "Average cap: 29.833736419677734\n",
      "Number capped: 7\n",
      "Average cap: 0.1947292983531952\n",
      "Number capped: 5\n",
      "Average cap: 11.973645210266113\n",
      "Number capped: 4\n",
      "Average cap: 7.1564435958862305\n",
      "Number capped: 6\n",
      "Average cap: 4.976434707641602\n",
      "Number capped: 6\n",
      "Average cap: 0.08734022825956345\n",
      "Number capped: 7\n",
      "Average cap: 0.5101420879364014\n",
      "Number capped: 6\n",
      "Average cap: 0.020633850246667862\n",
      "Number capped: 6\n",
      "Average cap: 1.4139527082443237\n",
      "Number capped: 6\n",
      "Average cap: 4.582437992095947\n",
      "Number capped: 4\n",
      "Average cap: 6.790205955505371\n",
      "Number capped: 5\n",
      "Average cap: 1.5157642364501953\n",
      "Number capped: 4\n",
      "Average cap: 7.767355442047119\n",
      "Number capped: 4\n",
      "Average cap: 11.839669227600098\n",
      "Number capped: 1\n",
      "Average cap: 33.11628723144531\n",
      "Number capped: 4\n",
      "Average cap: 25.650615692138672\n",
      "Number capped: 7\n",
      "Average cap: 1.3630746603012085\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "\n",
      "Test set: Avg. loss: 0.0006013191640377044, AUC: 0.8692755\n",
      "\n",
      "Number capped: 1\n",
      "Average cap: 44.806434631347656\n",
      "Number capped: 11\n",
      "Average cap: 0.028219686821103096\n",
      "Number capped: 5\n",
      "Average cap: 5.88145112991333\n",
      "Number capped: 3\n",
      "Average cap: 0.5171434283256531\n",
      "Number capped: 3\n",
      "Average cap: 7.443840026855469\n",
      "Number capped: 4\n",
      "Average cap: 7.5939412117004395\n",
      "Number capped: 3\n",
      "Average cap: 10.25977897644043\n",
      "Number capped: 3\n",
      "Average cap: 0.3523963391780853\n",
      "Number capped: 7\n",
      "Average cap: 4.46956205368042\n",
      "Number capped: 3\n",
      "Average cap: 9.50139331817627\n",
      "Number capped: 4\n",
      "Average cap: 8.728381156921387\n",
      "Number capped: 6\n",
      "Average cap: 0.005859882105141878\n",
      "Number capped: 6\n",
      "Average cap: 4.920848369598389\n",
      "Number capped: 3\n",
      "Average cap: 0.0166509747505188\n",
      "Number capped: 5\n",
      "Average cap: 0.003179868683218956\n",
      "Number capped: 3\n",
      "Average cap: 10.726475715637207\n",
      "Number capped: 2\n",
      "Average cap: 0.8454620242118835\n",
      "Number capped: 5\n",
      "Average cap: 6.557914733886719\n",
      "Number capped: 7\n",
      "Average cap: 4.277803897857666\n",
      "Number capped: 4\n",
      "Average cap: 9.645341873168945\n",
      "Number capped: 9\n",
      "Average cap: 0.30058014392852783\n",
      "Number capped: 2\n",
      "Average cap: 14.64036750793457\n",
      "Number capped: 5\n",
      "Average cap: 0.662164032459259\n",
      "Number capped: 4\n",
      "Average cap: 7.068118095397949\n",
      "Number capped: 7\n",
      "Average cap: 4.7097063064575195\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 9.31359577178955\n",
      "Number capped: 4\n",
      "Average cap: 0.5183213949203491\n",
      "Number capped: 4\n",
      "Average cap: 4.430203914642334\n",
      "Number capped: 5\n",
      "Average cap: 6.040238380432129\n",
      "Number capped: 7\n",
      "Average cap: 4.322513580322266\n",
      "Number capped: 5\n",
      "Average cap: 5.934317588806152\n",
      "Number capped: 3\n",
      "Average cap: 11.798477172851562\n",
      "Number capped: 6\n",
      "Average cap: 0.6277486681938171\n",
      "Number capped: 2\n",
      "Average cap: 4.432401180267334\n",
      "Number capped: 3\n",
      "Average cap: 2.874457359313965\n",
      "Number capped: 6\n",
      "Average cap: 6.326267719268799\n",
      "Number capped: 6\n",
      "Average cap: 0.4676658809185028\n",
      "Number capped: 3\n",
      "Average cap: 10.03055191040039\n",
      "Number capped: 2\n",
      "Average cap: 2.1002283096313477\n",
      "Number capped: 4\n",
      "Average cap: 6.938240051269531\n",
      "Number capped: 3\n",
      "Average cap: 9.821721076965332\n",
      "Number capped: 2\n",
      "Average cap: 16.889738082885742\n",
      "Number capped: 5\n",
      "Average cap: 5.859871864318848\n",
      "Number capped: 5\n",
      "Average cap: 5.985918998718262\n",
      "Number capped: 5\n",
      "Average cap: 0.155696839094162\n",
      "Number capped: 3\n",
      "Average cap: 0.462049275636673\n",
      "Number capped: 4\n",
      "Average cap: 0.000920263584703207\n",
      "Number capped: 3\n",
      "Average cap: 9.816462516784668\n",
      "Number capped: 7\n",
      "Average cap: 0.47014784812927246\n",
      "Number capped: 4\n",
      "Average cap: 7.773395538330078\n",
      "Number capped: 4\n",
      "Average cap: 6.797138214111328\n",
      "Number capped: 7\n",
      "Average cap: 5.778250217437744\n",
      "Number capped: 3\n",
      "Average cap: 0.13348828256130219\n",
      "Number capped: 5\n",
      "Average cap: 5.945757865905762\n",
      "Number capped: 7\n",
      "Average cap: 5.586236000061035\n",
      "Number capped: 5\n",
      "Average cap: 0.47504958510398865\n",
      "Number capped: 5\n",
      "Average cap: 8.605925559997559\n",
      "Number capped: 6\n",
      "Average cap: 0.4800734221935272\n",
      "Number capped: 5\n",
      "Average cap: 0.6405563950538635\n",
      "Number capped: 4\n",
      "Average cap: 8.64437484741211\n",
      "Number capped: 9\n",
      "Average cap: 0.8699904680252075\n",
      "Number capped: 5\n",
      "Average cap: 7.314103603363037\n",
      "Number capped: 4\n",
      "Average cap: 4.6604132652282715\n",
      "Number capped: 4\n",
      "Average cap: 9.623173713684082\n",
      "Number capped: 2\n",
      "Average cap: 5.033153057098389\n",
      "Number capped: 5\n",
      "Average cap: 0.9622467160224915\n",
      "Number capped: 1\n",
      "Average cap: 12.948225975036621\n",
      "Number capped: 6\n",
      "Average cap: 5.428081512451172\n",
      "Number capped: 3\n",
      "Average cap: 11.296250343322754\n",
      "Number capped: 4\n",
      "Average cap: 0.00031029857927933335\n",
      "Number capped: 2\n",
      "Average cap: 0.9175074100494385\n",
      "Number capped: 4\n",
      "Average cap: 10.265727043151855\n",
      "Number capped: 4\n",
      "Average cap: 7.086770057678223\n",
      "Number capped: 5\n",
      "Average cap: 6.361800193786621\n",
      "Number capped: 6\n",
      "Average cap: 4.879195690155029\n",
      "Number capped: 6\n",
      "Average cap: 4.813318729400635\n",
      "Number capped: 7\n",
      "Average cap: 3.7727928161621094\n",
      "Number capped: 6\n",
      "Average cap: 4.890857219696045\n",
      "Number capped: 5\n",
      "Average cap: 35.03713607788086\n",
      "Number capped: 5\n",
      "Average cap: 6.5760817527771\n",
      "Number capped: 6\n",
      "Average cap: 0.5466597676277161\n",
      "Number capped: 6\n",
      "Average cap: 4.603929042816162\n",
      "Number capped: 3\n",
      "Average cap: 21.354881286621094\n",
      "Number capped: 4\n",
      "Average cap: 7.498231410980225\n",
      "Number capped: 6\n",
      "Average cap: 6.7134552001953125\n",
      "Number capped: 3\n",
      "Average cap: 0.713742733001709\n",
      "Number capped: 4\n",
      "Average cap: 0.014061342924833298\n",
      "Number capped: 4\n",
      "Average cap: 6.754922866821289\n",
      "Number capped: 3\n",
      "Average cap: 10.132966041564941\n",
      "Number capped: 4\n",
      "Average cap: 7.319789409637451\n",
      "Number capped: 1\n",
      "Average cap: 29.031200408935547\n",
      "Number capped: 4\n",
      "Average cap: 6.67860746383667\n",
      "Number capped: 2\n",
      "Average cap: 14.564336776733398\n",
      "Number capped: 4\n",
      "Average cap: 5.12496280670166\n",
      "Number capped: 2\n",
      "Average cap: 14.506616592407227\n",
      "Number capped: 7\n",
      "Average cap: 3.1542580127716064\n",
      "Number capped: 2\n",
      "Average cap: 18.439329147338867\n",
      "Number capped: 5\n",
      "Average cap: 0.8087254762649536\n",
      "Number capped: 3\n",
      "Average cap: 21.344205856323242\n",
      "Number capped: 2\n",
      "Average cap: 15.002004623413086\n",
      "Number capped: 6\n",
      "Average cap: 8.7487211227417\n",
      "Number capped: 5\n",
      "Average cap: 1.640108346939087\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 3.104179620742798\n",
      "Number capped: 3\n",
      "Average cap: 10.206267356872559\n",
      "Number capped: 6\n",
      "Average cap: 0.19133035838603973\n",
      "Number capped: 5\n",
      "Average cap: 1.2923215627670288\n",
      "Number capped: 6\n",
      "Average cap: 0.11040955036878586\n",
      "Number capped: 8\n",
      "Average cap: 0.7033838033676147\n",
      "Number capped: 8\n",
      "Average cap: 4.568241596221924\n",
      "Number capped: 7\n",
      "Average cap: 8.352892875671387\n",
      "Number capped: 7\n",
      "Average cap: 4.229422092437744\n",
      "Number capped: 2\n",
      "Average cap: 0.5507268905639648\n",
      "Number capped: 4\n",
      "Average cap: 0.016753321513533592\n",
      "Number capped: 4\n",
      "Average cap: 7.148658752441406\n",
      "Number capped: 4\n",
      "Average cap: 7.122478485107422\n",
      "Number capped: 5\n",
      "Average cap: 6.543211460113525\n",
      "Number capped: 6\n",
      "Average cap: 0.7442708015441895\n",
      "Number capped: 3\n",
      "Average cap: 0.2277100533246994\n",
      "Number capped: 6\n",
      "Average cap: 0.8020147681236267\n",
      "Number capped: 3\n",
      "Average cap: 10.4940767288208\n",
      "Number capped: 11\n",
      "Average cap: 2.4799962043762207\n",
      "Number capped: 4\n",
      "Average cap: 11.835323333740234\n",
      "Number capped: 6\n",
      "Average cap: 4.789000988006592\n",
      "Number capped: 6\n",
      "Average cap: 0.07941767573356628\n",
      "Number capped: 4\n",
      "Average cap: 12.075571060180664\n",
      "Number capped: 6\n",
      "Average cap: 4.281966686248779\n",
      "Number capped: 3\n",
      "Average cap: 9.770495414733887\n",
      "Number capped: 8\n",
      "Average cap: 4.695326328277588\n",
      "Number capped: 2\n",
      "Average cap: 15.471495628356934\n",
      "Number capped: 1\n",
      "Average cap: 28.822078704833984\n",
      "Number capped: 2\n",
      "Average cap: 13.226428031921387\n",
      "Number capped: 2\n",
      "Average cap: 17.031850814819336\n",
      "Number capped: 3\n",
      "Average cap: 9.648335456848145\n",
      "Number capped: 5\n",
      "Average cap: 0.2437814176082611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 7\n",
      "Average cap: 3.6726367473602295\n",
      "Number capped: 3\n",
      "Average cap: 9.655681610107422\n",
      "Number capped: 6\n",
      "Average cap: 4.762052536010742\n",
      "Number capped: 3\n",
      "Average cap: 0.0029407136607915163\n",
      "Number capped: 4\n",
      "Average cap: 7.312742233276367\n",
      "Number capped: 4\n",
      "Average cap: 0.5056625604629517\n",
      "Number capped: 3\n",
      "Average cap: 9.863380432128906\n",
      "Number capped: 3\n",
      "Average cap: 9.294671058654785\n",
      "Number capped: 4\n",
      "Average cap: 0.6918822526931763\n",
      "Number capped: 3\n",
      "Average cap: 9.61252498626709\n",
      "Number capped: 12\n",
      "Average cap: 2.607029676437378\n",
      "Number capped: 3\n",
      "Average cap: 0.0916246771812439\n",
      "Number capped: 2\n",
      "Average cap: 0.0002622270258143544\n",
      "Number capped: 4\n",
      "Average cap: 0.5304253697395325\n",
      "Number capped: 4\n",
      "Average cap: 8.187591552734375\n",
      "Number capped: 3\n",
      "Average cap: 0.4738096296787262\n",
      "Number capped: 2\n",
      "Average cap: 10.39599323272705\n",
      "Number capped: 3\n",
      "Average cap: 0.5166917443275452\n",
      "Number capped: 4\n",
      "Average cap: 1.703149437904358\n",
      "Number capped: 5\n",
      "Average cap: 0.012404071167111397\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 9.753605842590332\n",
      "Number capped: 3\n",
      "Average cap: 8.441435813903809\n",
      "Number capped: 2\n",
      "Average cap: 0.4653978943824768\n",
      "Number capped: 9\n",
      "Average cap: 4.855875492095947\n",
      "Number capped: 6\n",
      "Average cap: 4.650055408477783\n",
      "Number capped: 1\n",
      "Average cap: 36.23868179321289\n",
      "Number capped: 1\n",
      "Average cap: 36.529335021972656\n",
      "Number capped: 3\n",
      "Average cap: 0.480843186378479\n",
      "Number capped: 5\n",
      "Average cap: 5.769515037536621\n",
      "Number capped: 5\n",
      "Average cap: 0.46877622604370117\n",
      "Number capped: 3\n",
      "Average cap: 9.587291717529297\n",
      "Number capped: 2\n",
      "Average cap: 7.539948463439941\n",
      "Number capped: 3\n",
      "Average cap: 20.99774742126465\n",
      "Number capped: 9\n",
      "Average cap: 0.407000333070755\n",
      "Number capped: 4\n",
      "Average cap: 0.6077175736427307\n",
      "Number capped: 7\n",
      "Average cap: 10.075593948364258\n",
      "Number capped: 2\n",
      "Average cap: 4.186191558837891\n",
      "Number capped: 6\n",
      "Average cap: 4.850160121917725\n",
      "Number capped: 5\n",
      "Average cap: 5.172863960266113\n",
      "Number capped: 3\n",
      "Average cap: 10.916380882263184\n",
      "Number capped: 2\n",
      "Average cap: 8.214363098144531\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 6\n",
      "Average cap: 4.9153852462768555\n",
      "Number capped: 3\n",
      "Average cap: 17.764577865600586\n",
      "Number capped: 2\n",
      "Average cap: 16.617881774902344\n",
      "Number capped: 8\n",
      "Average cap: 0.7299005389213562\n",
      "Number capped: 3\n",
      "Average cap: 0.10747071355581284\n",
      "Number capped: 5\n",
      "Average cap: 5.911267280578613\n",
      "Number capped: 1\n",
      "Average cap: 31.101726531982422\n",
      "Number capped: 2\n",
      "Average cap: 0.4764449894428253\n",
      "Number capped: 1\n",
      "Average cap: 0.0866180881857872\n",
      "Number capped: 7\n",
      "Average cap: 1.089850664138794\n",
      "Number capped: 1\n",
      "Average cap: 27.835206985473633\n",
      "Number capped: 2\n",
      "Average cap: 0.011710218153893948\n",
      "Number capped: 4\n",
      "Average cap: 1.3397725820541382\n",
      "Number capped: 5\n",
      "Average cap: 2.3178329467773438\n",
      "Number capped: 4\n",
      "Average cap: 10.966310501098633\n",
      "Number capped: 4\n",
      "Average cap: 9.77270793914795\n",
      "Number capped: 1\n",
      "Average cap: 1.9095228910446167\n",
      "Number capped: 4\n",
      "Average cap: 1.479326844215393\n",
      "Number capped: 3\n",
      "Average cap: 9.305413246154785\n",
      "Number capped: 5\n",
      "Average cap: 5.684019565582275\n",
      "Number capped: 6\n",
      "Average cap: 5.9161057472229\n",
      "Number capped: 6\n",
      "Average cap: 0.05681069567799568\n",
      "Number capped: 5\n",
      "Average cap: 7.207758903503418\n",
      "Number capped: 3\n",
      "Average cap: 16.334882736206055\n",
      "Number capped: 4\n",
      "Average cap: 0.020467670634388924\n",
      "Number capped: 2\n",
      "Average cap: 14.554142951965332\n",
      "Number capped: 2\n",
      "Average cap: 14.360163688659668\n",
      "Number capped: 3\n",
      "Average cap: 2.899968385696411\n",
      "Number capped: 7\n",
      "Average cap: 0.4610047936439514\n",
      "Number capped: 1\n",
      "Average cap: 0.7172877192497253\n",
      "Number capped: 3\n",
      "Average cap: 9.85132884979248\n",
      "Number capped: 4\n",
      "Average cap: 7.662110328674316\n",
      "Number capped: 7\n",
      "Average cap: 4.584705829620361\n",
      "Number capped: 2\n",
      "Average cap: 4.052096366882324\n",
      "Number capped: 4\n",
      "Average cap: 7.3053717613220215\n",
      "Number capped: 3\n",
      "Average cap: 0.7140728831291199\n",
      "Number capped: 2\n",
      "Average cap: 24.272987365722656\n",
      "Number capped: 3\n",
      "Average cap: 15.742514610290527\n",
      "Number capped: 1\n",
      "Average cap: 1.086734414100647\n",
      "Number capped: 5\n",
      "Average cap: 0.025840872898697853\n",
      "Number capped: 8\n",
      "Average cap: 0.4589230418205261\n",
      "Number capped: 6\n",
      "Average cap: 5.421134948730469\n",
      "Number capped: 6\n",
      "Average cap: 4.783632278442383\n",
      "Number capped: 4\n",
      "Average cap: 7.8197126388549805\n",
      "Number capped: 3\n",
      "Average cap: 9.974591255187988\n",
      "Number capped: 10\n",
      "Average cap: 2.9075639247894287\n",
      "Number capped: 4\n",
      "Average cap: 11.605196952819824\n",
      "Number capped: 4\n",
      "Average cap: 8.581929206848145\n",
      "Number capped: 4\n",
      "Average cap: 0.00020823805243708193\n",
      "Number capped: 6\n",
      "Average cap: 4.921679973602295\n",
      "Number capped: 2\n",
      "Average cap: 0.5332554578781128\n",
      "Number capped: 1\n",
      "Average cap: 0.4157934784889221\n",
      "Number capped: 8\n",
      "Average cap: 3.179816722869873\n",
      "Number capped: 5\n",
      "Average cap: 5.879051208496094\n",
      "Number capped: 5\n",
      "Average cap: 7.183447360992432\n",
      "Number capped: 5\n",
      "Average cap: 5.47222900390625\n",
      "Number capped: 3\n",
      "Average cap: 9.505906105041504\n",
      "Number capped: 6\n",
      "Average cap: 0.045316990464925766\n",
      "Number capped: 5\n",
      "Average cap: 5.496197700500488\n",
      "Number capped: 6\n",
      "Average cap: 4.705295562744141\n",
      "Number capped: 4\n",
      "Average cap: 7.105792045593262\n",
      "Number capped: 2\n",
      "Average cap: 31.2775936126709\n",
      "Number capped: 8\n",
      "Average cap: 1.2818349599838257\n",
      "Number capped: 7\n",
      "Average cap: 4.902871608734131\n",
      "Number capped: 2\n",
      "Average cap: 3.740204095840454\n",
      "Number capped: 3\n",
      "Average cap: 8.969687461853027\n",
      "Number capped: 2\n",
      "Average cap: 0.4842274487018585\n",
      "Number capped: 6\n",
      "Average cap: 0.46760228276252747\n",
      "Number capped: 4\n",
      "Average cap: 7.366062641143799\n",
      "Number capped: 5\n",
      "Average cap: 2.10538387298584\n",
      "Number capped: 6\n",
      "Average cap: 5.047159194946289\n",
      "Number capped: 11\n",
      "Average cap: 0.029979461804032326\n",
      "Number capped: 3\n",
      "Average cap: 9.553536415100098\n",
      "Number capped: 1\n",
      "Average cap: 0.004469083156436682\n",
      "Number capped: 5\n",
      "Average cap: 0.7421132326126099\n",
      "Number capped: 3\n",
      "Average cap: 9.562444686889648\n",
      "Number capped: 4\n",
      "Average cap: 0.45290306210517883\n",
      "Number capped: 8\n",
      "Average cap: 5.922621726989746\n",
      "Number capped: 11\n",
      "Average cap: 2.162984609603882\n",
      "Number capped: 3\n",
      "Average cap: 1.594031810760498\n",
      "Number capped: 5\n",
      "Average cap: 5.799229621887207\n",
      "Number capped: 4\n",
      "Average cap: 0.0003398079425096512\n",
      "Number capped: 2\n",
      "Average cap: 4.8604736328125\n",
      "Number capped: 5\n",
      "Average cap: 6.674770355224609\n",
      "Number capped: 3\n",
      "Average cap: 23.150781631469727\n",
      "Number capped: 1\n",
      "Average cap: 0.48348915576934814\n",
      "Number capped: 2\n",
      "Average cap: 0.44287022948265076\n",
      "Number capped: 3\n",
      "Average cap: 1.2639961242675781\n",
      "Number capped: 4\n",
      "Average cap: 7.317826747894287\n",
      "Number capped: 4\n",
      "Average cap: 6.666064262390137\n",
      "Number capped: 7\n",
      "Average cap: 4.621257305145264\n",
      "Number capped: 8\n",
      "Average cap: 3.7277169227600098\n",
      "Number capped: 2\n",
      "Average cap: 15.442317962646484\n",
      "Number capped: 4\n",
      "Average cap: 0.9340927600860596\n",
      "Number capped: 4\n",
      "Average cap: 0.05132502689957619\n",
      "Number capped: 9\n",
      "Average cap: 7.204898357391357\n",
      "Number capped: 6\n",
      "Average cap: 1.370654582977295\n",
      "Number capped: 7\n",
      "Average cap: 4.0459513664245605\n",
      "Number capped: 2\n",
      "Average cap: 0.4837980270385742\n",
      "Number capped: 1\n",
      "Average cap: 28.027463912963867\n",
      "Number capped: 9\n",
      "Average cap: 4.093019485473633\n",
      "Number capped: 3\n",
      "Average cap: 18.999773025512695\n",
      "Number capped: 6\n",
      "Average cap: 2.8150269985198975\n",
      "Number capped: 5\n",
      "Average cap: 0.4661900997161865\n",
      "Number capped: 5\n",
      "Average cap: 12.611013412475586\n",
      "Number capped: 4\n",
      "Average cap: 8.16944408416748\n",
      "Number capped: 7\n",
      "Average cap: 4.233712196350098\n",
      "Number capped: 8\n",
      "Average cap: 4.4981465339660645\n",
      "Number capped: 3\n",
      "Average cap: 9.507384300231934\n",
      "Number capped: 7\n",
      "Average cap: 4.007741928100586\n",
      "Number capped: 6\n",
      "Average cap: 4.596867084503174\n",
      "Number capped: 2\n",
      "Average cap: 13.997913360595703\n",
      "Number capped: 2\n",
      "Average cap: 0.13950304687023163\n",
      "Number capped: 5\n",
      "Average cap: 5.880540370941162\n",
      "Number capped: 4\n",
      "Average cap: 0.23815223574638367\n",
      "Number capped: 1\n",
      "Average cap: 0.0758102685213089\n",
      "Number capped: 1\n",
      "Average cap: 29.02269172668457\n",
      "Number capped: 1\n",
      "Average cap: 27.830486297607422\n",
      "Number capped: 3\n",
      "Average cap: 0.03351365402340889\n",
      "Number capped: 3\n",
      "Average cap: 12.286253929138184\n",
      "Number capped: 4\n",
      "Average cap: 6.888132095336914\n",
      "Number capped: 8\n",
      "Average cap: 3.905766487121582\n",
      "Number capped: 3\n",
      "Average cap: 1.964930534362793\n",
      "Number capped: 7\n",
      "Average cap: 0.0008636942366138101\n",
      "Number capped: 10\n",
      "Average cap: 2.861759662628174\n",
      "Number capped: 10\n",
      "Average cap: 2.819838285446167\n",
      "Number capped: 10\n",
      "Average cap: 2.920650005340576\n",
      "Number capped: 4\n",
      "Average cap: 0.4984041154384613\n",
      "Number capped: 7\n",
      "Average cap: 7.245716571807861\n",
      "Number capped: 10\n",
      "Average cap: 5.536208629608154\n",
      "Number capped: 4\n",
      "Average cap: 0.4402630925178528\n",
      "Number capped: 2\n",
      "Average cap: 0.5647348761558533\n",
      "Number capped: 3\n",
      "Average cap: 9.776402473449707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 2\n",
      "Average cap: 0.43487659096717834\n",
      "Number capped: 2\n",
      "Average cap: 0.5614233016967773\n",
      "Number capped: 5\n",
      "Average cap: 11.925874710083008\n",
      "Number capped: 2\n",
      "Average cap: 0.005915291607379913\n",
      "Number capped: 3\n",
      "Average cap: 10.694979667663574\n",
      "Number capped: 3\n",
      "Average cap: 0.016788424924016\n",
      "Number capped: 6\n",
      "Average cap: 0.9110899567604065\n",
      "Number capped: 3\n",
      "Average cap: 8.875543594360352\n",
      "Number capped: 3\n",
      "Average cap: 9.896954536437988\n",
      "Number capped: 3\n",
      "Average cap: 3.4315760135650635\n",
      "Number capped: 4\n",
      "Average cap: 6.58533239364624\n",
      "Number capped: 1\n",
      "Average cap: 37.58622360229492\n",
      "Number capped: 6\n",
      "Average cap: 4.750565052032471\n",
      "Number capped: 4\n",
      "Average cap: 7.21790885925293\n",
      "Number capped: 5\n",
      "Average cap: 6.529581546783447\n",
      "Number capped: 3\n",
      "Average cap: 0.06660623103380203\n",
      "Number capped: 6\n",
      "Average cap: 0.6377587914466858\n",
      "Number capped: 4\n",
      "Average cap: 7.1093292236328125\n",
      "Number capped: 3\n",
      "Average cap: 11.382987022399902\n",
      "Number capped: 4\n",
      "Average cap: 0.4478767216205597\n",
      "Number capped: 4\n",
      "Average cap: 12.49806022644043\n",
      "Number capped: 4\n",
      "Average cap: 6.71266508102417\n",
      "Number capped: 4\n",
      "Average cap: 6.810226917266846\n",
      "Number capped: 7\n",
      "Average cap: 4.280078411102295\n",
      "Number capped: 4\n",
      "Average cap: 6.655505180358887\n",
      "Number capped: 5\n",
      "Average cap: 0.44047173857688904\n",
      "Number capped: 5\n",
      "Average cap: 0.4011186957359314\n",
      "Number capped: 2\n",
      "Average cap: 14.216225624084473\n",
      "Number capped: 3\n",
      "Average cap: 0.7542591094970703\n",
      "Number capped: 5\n",
      "Average cap: 0.4417063295841217\n",
      "Number capped: 3\n",
      "Average cap: 12.389366149902344\n",
      "Number capped: 10\n",
      "Average cap: 0.004716819617897272\n",
      "Number capped: 3\n",
      "Average cap: 0.00020161049906164408\n",
      "Number capped: 5\n",
      "Average cap: 10.905308723449707\n",
      "Number capped: 5\n",
      "Average cap: 5.615603446960449\n",
      "Number capped: 7\n",
      "Average cap: 4.707675457000732\n",
      "Number capped: 3\n",
      "Average cap: 10.1991605758667\n",
      "Number capped: 6\n",
      "Average cap: 4.1935906410217285\n",
      "Number capped: 2\n",
      "Average cap: 15.24074935913086\n",
      "Number capped: 4\n",
      "Average cap: 0.5893018841743469\n",
      "Number capped: 6\n",
      "Average cap: 4.832650184631348\n",
      "Number capped: 7\n",
      "Average cap: 0.9876543879508972\n",
      "Number capped: 3\n",
      "Average cap: 0.09578540176153183\n",
      "Number capped: 4\n",
      "Average cap: 0.4504038989543915\n",
      "Number capped: 4\n",
      "Average cap: 0.4391917586326599\n",
      "Number capped: 3\n",
      "Average cap: 9.518754005432129\n",
      "Number capped: 11\n",
      "Average cap: 4.632020950317383\n",
      "Number capped: 5\n",
      "Average cap: 5.19071102142334\n",
      "Number capped: 4\n",
      "Average cap: 7.028045654296875\n",
      "Number capped: 8\n",
      "Average cap: 0.47301530838012695\n",
      "Number capped: 5\n",
      "Average cap: 5.841721534729004\n",
      "Number capped: 3\n",
      "Average cap: 9.224411964416504\n",
      "Number capped: 4\n",
      "Average cap: 0.003430911572650075\n",
      "Number capped: 3\n",
      "Average cap: 9.481069564819336\n",
      "Number capped: 5\n",
      "Average cap: 10.621101379394531\n",
      "Number capped: 3\n",
      "Average cap: 0.3888889253139496\n",
      "Number capped: 6\n",
      "Average cap: 1.0520318746566772\n",
      "Number capped: 5\n",
      "Average cap: 5.053999900817871\n",
      "Number capped: 3\n",
      "Average cap: 10.01158618927002\n",
      "Number capped: 2\n",
      "Average cap: 0.028859030455350876\n",
      "Number capped: 10\n",
      "Average cap: 2.8904976844787598\n",
      "Number capped: 6\n",
      "Average cap: 0.5465274453163147\n",
      "Number capped: 4\n",
      "Average cap: 8.327048301696777\n",
      "Number capped: 2\n",
      "Average cap: 0.6934188604354858\n",
      "Number capped: 6\n",
      "Average cap: 5.58088493347168\n",
      "Number capped: 4\n",
      "Average cap: 8.914419174194336\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 9.315909385681152\n",
      "Number capped: 3\n",
      "Average cap: 1.0352340936660767\n",
      "Number capped: 4\n",
      "Average cap: 7.254099369049072\n",
      "Number capped: 4\n",
      "Average cap: 6.761026382446289\n",
      "Number capped: 5\n",
      "Average cap: 2.4986244170577265e-05\n",
      "Number capped: 2\n",
      "Average cap: 14.787272453308105\n",
      "Number capped: 2\n",
      "Average cap: 14.298807144165039\n",
      "Number capped: 4\n",
      "Average cap: 3.3245766162872314\n",
      "Number capped: 4\n",
      "Average cap: 6.294566631317139\n",
      "Number capped: 5\n",
      "Average cap: 0.21926383674144745\n",
      "Number capped: 5\n",
      "Average cap: 12.617388725280762\n",
      "Number capped: 7\n",
      "Average cap: 1.0318437814712524\n",
      "Number capped: 6\n",
      "Average cap: 0.0001561134704388678\n",
      "Number capped: 8\n",
      "Average cap: 7.288269996643066\n",
      "Number capped: 3\n",
      "Average cap: 0.9715331196784973\n",
      "Number capped: 1\n",
      "Average cap: 27.840734481811523\n",
      "Number capped: 2\n",
      "Average cap: 0.04875122383236885\n",
      "Number capped: 5\n",
      "Average cap: 5.610971450805664\n",
      "Number capped: 4\n",
      "Average cap: 1.4999561309814453\n",
      "Number capped: 9\n",
      "Average cap: 3.1140270233154297\n",
      "Number capped: 4\n",
      "Average cap: 7.00534725189209\n",
      "Number capped: 6\n",
      "Average cap: 0.011041375808417797\n",
      "Number capped: 1\n",
      "Average cap: 1.3388895988464355\n",
      "Number capped: 5\n",
      "Average cap: 5.335352897644043\n",
      "Number capped: 5\n",
      "Average cap: 0.08807455748319626\n",
      "Number capped: 3\n",
      "Average cap: 12.921135902404785\n",
      "Number capped: 5\n",
      "Average cap: 5.539487838745117\n",
      "Number capped: 4\n",
      "Average cap: 8.486845970153809\n",
      "Number capped: 4\n",
      "Average cap: 11.360931396484375\n",
      "Number capped: 11\n",
      "Average cap: 2.0738651752471924\n",
      "Number capped: 7\n",
      "Average cap: 3.4764862060546875\n",
      "Number capped: 3\n",
      "Average cap: 8.672585487365723\n",
      "Number capped: 1\n",
      "Average cap: 28.716440200805664\n",
      "Number capped: 4\n",
      "Average cap: 7.147833824157715\n",
      "Number capped: 5\n",
      "Average cap: 0.44545871019363403\n",
      "Number capped: 5\n",
      "Average cap: 5.668644905090332\n",
      "Number capped: 7\n",
      "Average cap: 3.555391550064087\n",
      "Number capped: 3\n",
      "Average cap: 3.7702856063842773\n",
      "Number capped: 2\n",
      "Average cap: 15.029520988464355\n",
      "Number capped: 9\n",
      "Average cap: 2.6933913230895996\n",
      "Number capped: 2\n",
      "Average cap: 0.43198123574256897\n",
      "Number capped: 3\n",
      "Average cap: 13.667376518249512\n",
      "Number capped: 5\n",
      "Average cap: 0.04125715047121048\n",
      "Number capped: 2\n",
      "Average cap: 32.63369369506836\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 5\n",
      "Average cap: 8.114018440246582\n",
      "Number capped: 6\n",
      "Average cap: 3.68404221534729\n",
      "Number capped: 6\n",
      "Average cap: 0.44372472167015076\n",
      "Number capped: 5\n",
      "Average cap: 0.44472041726112366\n",
      "Number capped: 1\n",
      "Average cap: 32.195716857910156\n",
      "Number capped: 3\n",
      "Average cap: 18.591691970825195\n",
      "Number capped: 8\n",
      "Average cap: 0.43060484528541565\n",
      "Number capped: 1\n",
      "Average cap: 0.006422908045351505\n",
      "Number capped: 3\n",
      "Average cap: 10.220017433166504\n",
      "Number capped: 4\n",
      "Average cap: 0.6602559685707092\n",
      "Number capped: 4\n",
      "Average cap: 6.8611249923706055\n",
      "Number capped: 1\n",
      "Average cap: 31.389381408691406\n",
      "Number capped: 8\n",
      "Average cap: 1.504675030708313\n",
      "Number capped: 6\n",
      "Average cap: 5.642016887664795\n",
      "Number capped: 2\n",
      "Average cap: 1.6221888065338135\n",
      "Number capped: 8\n",
      "Average cap: 3.4110240936279297\n",
      "Number capped: 3\n",
      "Average cap: 8.877725601196289\n",
      "Number capped: 6\n",
      "Average cap: 4.185643672943115\n",
      "Number capped: 4\n",
      "Average cap: 15.31260871887207\n",
      "Number capped: 6\n",
      "Average cap: 5.3471550941467285\n",
      "Number capped: 7\n",
      "Average cap: 3.663069009780884\n",
      "Number capped: 4\n",
      "Average cap: 0.1884126365184784\n",
      "Number capped: 8\n",
      "Average cap: 1.3059751987457275\n",
      "Number capped: 3\n",
      "Average cap: 9.967751502990723\n",
      "Number capped: 5\n",
      "Average cap: 0.42940062284469604\n",
      "Number capped: 3\n",
      "Average cap: 0.0843723714351654\n",
      "Number capped: 2\n",
      "Average cap: 0.4191282391548157\n",
      "Number capped: 6\n",
      "Average cap: 7.887359619140625\n",
      "Number capped: 8\n",
      "Average cap: 3.5190041065216064\n",
      "Number capped: 3\n",
      "Average cap: 2.6336252689361572\n",
      "Number capped: 2\n",
      "Average cap: 13.567804336547852\n",
      "Number capped: 5\n",
      "Average cap: 5.502072334289551\n",
      "Number capped: 8\n",
      "Average cap: 3.5413691997528076\n",
      "Number capped: 5\n",
      "Average cap: 0.46051502227783203\n",
      "Number capped: 5\n",
      "Average cap: 5.745032787322998\n",
      "Number capped: 6\n",
      "Average cap: 0.5029057860374451\n",
      "Number capped: 4\n",
      "Average cap: 7.449052333831787\n",
      "Number capped: 5\n",
      "Average cap: 6.013524532318115\n",
      "Number capped: 9\n",
      "Average cap: 4.136811256408691\n",
      "Number capped: 5\n",
      "Average cap: 0.4336015582084656\n",
      "Number capped: 5\n",
      "Average cap: 12.220987319946289\n",
      "Number capped: 2\n",
      "Average cap: 14.511811256408691\n",
      "Number capped: 4\n",
      "Average cap: 0.4515531361103058\n",
      "Number capped: 3\n",
      "Average cap: 8.819079399108887\n",
      "Number capped: 5\n",
      "Average cap: 0.6330134272575378\n",
      "Number capped: 5\n",
      "Average cap: 11.504898071289062\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 0.574958860874176\n",
      "Number capped: 2\n",
      "Average cap: 24.407413482666016\n",
      "Number capped: 9\n",
      "Average cap: 3.083458423614502\n",
      "Number capped: 9\n",
      "Average cap: 2.9899775981903076\n",
      "Number capped: 5\n",
      "Average cap: 6.045647621154785\n",
      "Number capped: 3\n",
      "Average cap: 8.925869941711426\n",
      "Number capped: 9\n",
      "Average cap: 3.4576961994171143\n",
      "Number capped: 5\n",
      "Average cap: 0.06602929532527924\n",
      "Number capped: 6\n",
      "Average cap: 5.283350467681885\n",
      "Number capped: 3\n",
      "Average cap: 1.0420705080032349\n",
      "Number capped: 4\n",
      "Average cap: 6.6764349937438965\n",
      "Number capped: 7\n",
      "Average cap: 13.220149040222168\n",
      "Number capped: 5\n",
      "Average cap: 0.7244726419448853\n",
      "Number capped: 8\n",
      "Average cap: 3.0884552001953125\n",
      "Number capped: 4\n",
      "Average cap: 7.750620365142822\n",
      "Number capped: 6\n",
      "Average cap: 0.9987487196922302\n",
      "Number capped: 2\n",
      "Average cap: 13.398564338684082\n",
      "Number capped: 1\n",
      "Average cap: 30.0986385345459\n",
      "Number capped: 4\n",
      "Average cap: 11.011140823364258\n",
      "Number capped: 7\n",
      "Average cap: 0.5912590026855469\n",
      "Number capped: 4\n",
      "Average cap: 5.751336097717285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 5\n",
      "Average cap: 1.6695992946624756\n",
      "Number capped: 2\n",
      "Average cap: 13.63290786743164\n",
      "Number capped: 2\n",
      "Average cap: 27.434707641601562\n",
      "Number capped: 5\n",
      "Average cap: 11.811226844787598\n",
      "Number capped: 1\n",
      "Average cap: 1.0362224578857422\n",
      "Number capped: 4\n",
      "Average cap: 8.025151252746582\n",
      "Number capped: 4\n",
      "Average cap: 0.4379676878452301\n",
      "Number capped: 4\n",
      "Average cap: 6.410652160644531\n",
      "Number capped: 9\n",
      "Average cap: 0.4569679796695709\n",
      "Number capped: 3\n",
      "Average cap: 8.92088794708252\n",
      "Number capped: 7\n",
      "Average cap: 0.638927161693573\n",
      "Number capped: 5\n",
      "Average cap: 0.0013647832674905658\n",
      "Number capped: 7\n",
      "Average cap: 0.6013266444206238\n",
      "Number capped: 2\n",
      "Average cap: 0.44071558117866516\n",
      "Number capped: 5\n",
      "Average cap: 0.24326559901237488\n",
      "Number capped: 4\n",
      "Average cap: 6.965396881103516\n",
      "Number capped: 3\n",
      "Average cap: 20.276397705078125\n",
      "Number capped: 2\n",
      "Average cap: 14.27302074432373\n",
      "Number capped: 4\n",
      "Average cap: 0.29184460639953613\n",
      "Number capped: 7\n",
      "Average cap: 0.5703072547912598\n",
      "Number capped: 2\n",
      "Average cap: 14.457342147827148\n",
      "Number capped: 8\n",
      "Average cap: 3.145268678665161\n",
      "Number capped: 4\n",
      "Average cap: 2.3096601963043213\n",
      "Number capped: 4\n",
      "Average cap: 6.869870185852051\n",
      "Number capped: 5\n",
      "Average cap: 0.016670893877744675\n",
      "Number capped: 5\n",
      "Average cap: 0.8034517168998718\n",
      "Number capped: 4\n",
      "Average cap: 6.504823684692383\n",
      "Number capped: 5\n",
      "Average cap: 5.601689338684082\n",
      "Number capped: 5\n",
      "Average cap: 12.168911933898926\n",
      "Number capped: 2\n",
      "Average cap: 3.719996452331543\n",
      "Number capped: 6\n",
      "Average cap: 1.1297963857650757\n",
      "Number capped: 2\n",
      "Average cap: 21.408092498779297\n",
      "Number capped: 3\n",
      "Average cap: 21.845930099487305\n",
      "Number capped: 7\n",
      "Average cap: 0.44571635127067566\n",
      "Number capped: 3\n",
      "Average cap: 0.5009944438934326\n",
      "Number capped: 3\n",
      "Average cap: 13.881858825683594\n",
      "Number capped: 4\n",
      "Average cap: 0.007128806784749031\n",
      "Number capped: 2\n",
      "Average cap: 1.6939315795898438\n",
      "Number capped: 8\n",
      "Average cap: 1.0830484628677368\n",
      "Number capped: 2\n",
      "Average cap: 0.4171692132949829\n",
      "Number capped: 6\n",
      "Average cap: 6.055716037750244\n",
      "Number capped: 8\n",
      "Average cap: 0.0019281399436295033\n",
      "Number capped: 8\n",
      "Average cap: 3.5468451976776123\n",
      "Number capped: 2\n",
      "Average cap: 13.676681518554688\n",
      "Number capped: 5\n",
      "Average cap: 5.413176536560059\n",
      "Number capped: 3\n",
      "Average cap: 9.383696556091309\n",
      "Number capped: 6\n",
      "Average cap: 5.134909152984619\n",
      "Number capped: 6\n",
      "Average cap: 0.4447093904018402\n",
      "Number capped: 1\n",
      "Average cap: 0.49493247270584106\n",
      "Number capped: 5\n",
      "Average cap: 5.234723091125488\n",
      "Number capped: 3\n",
      "Average cap: 0.09596890956163406\n",
      "Number capped: 3\n",
      "Average cap: 8.671427726745605\n",
      "Number capped: 6\n",
      "Average cap: 0.822805643081665\n",
      "Number capped: 5\n",
      "Average cap: 0.641494870185852\n",
      "Number capped: 3\n",
      "Average cap: 9.764659881591797\n",
      "Number capped: 4\n",
      "Average cap: 6.767613410949707\n",
      "Number capped: 3\n",
      "Average cap: 10.770472526550293\n",
      "Number capped: 3\n",
      "Average cap: 21.357404708862305\n",
      "Number capped: 2\n",
      "Average cap: 13.82551383972168\n",
      "Number capped: 3\n",
      "Average cap: 1.3992811441421509\n",
      "Number capped: 5\n",
      "Average cap: 7.60153865814209\n",
      "Number capped: 1\n",
      "Average cap: 27.564929962158203\n",
      "Number capped: 4\n",
      "Average cap: 4.9926838874816895\n",
      "Number capped: 6\n",
      "Average cap: 5.213928699493408\n",
      "Number capped: 2\n",
      "Average cap: 15.422395706176758\n",
      "Number capped: 7\n",
      "Average cap: 1.3393527269363403\n",
      "Number capped: 3\n",
      "Average cap: 9.207854270935059\n",
      "Number capped: 7\n",
      "Average cap: 0.0005735139711759984\n",
      "Number capped: 4\n",
      "Average cap: 0.43080732226371765\n",
      "Number capped: 5\n",
      "Average cap: 0.4467197358608246\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 2\n",
      "Average cap: 20.351247787475586\n",
      "Number capped: 2\n",
      "Average cap: 1.8667383193969727\n",
      "Number capped: 9\n",
      "Average cap: 0.3929126560688019\n",
      "Number capped: 8\n",
      "Average cap: 4.610515594482422\n",
      "Number capped: 3\n",
      "Average cap: 9.095756530761719\n",
      "Number capped: 4\n",
      "Average cap: 8.407369613647461\n",
      "Number capped: 6\n",
      "Average cap: 4.760296821594238\n",
      "Number capped: 1\n",
      "Average cap: 27.49319839477539\n",
      "Number capped: 8\n",
      "Average cap: 3.6385912895202637\n",
      "Number capped: 3\n",
      "Average cap: 0.4270962178707123\n",
      "Number capped: 3\n",
      "Average cap: 12.828628540039062\n",
      "Number capped: 6\n",
      "Average cap: 6.538560390472412\n",
      "Number capped: 6\n",
      "Average cap: 4.3110270500183105\n",
      "Number capped: 3\n",
      "Average cap: 11.874638557434082\n",
      "Number capped: 1\n",
      "Average cap: 32.469642639160156\n",
      "Number capped: 4\n",
      "Average cap: 9.338066101074219\n",
      "Number capped: 1\n",
      "Average cap: 27.708646774291992\n",
      "Number capped: 4\n",
      "Average cap: 6.5838727951049805\n",
      "Number capped: 6\n",
      "Average cap: 4.242773532867432\n",
      "Number capped: 1\n",
      "Average cap: 0.015935074537992477\n",
      "Number capped: 8\n",
      "Average cap: 3.0387065410614014\n",
      "Number capped: 5\n",
      "Average cap: 5.1071038246154785\n",
      "Number capped: 5\n",
      "Average cap: 5.174806118011475\n",
      "Number capped: 3\n",
      "Average cap: 8.781216621398926\n",
      "Number capped: 7\n",
      "Average cap: 4.411299228668213\n",
      "Number capped: 2\n",
      "Average cap: 4.853657245635986\n",
      "Number capped: 4\n",
      "Average cap: 0.07345599681138992\n",
      "Number capped: 6\n",
      "Average cap: 0.40630224347114563\n",
      "Number capped: 4\n",
      "Average cap: 10.209749221801758\n",
      "Number capped: 4\n",
      "Average cap: 6.839911460876465\n",
      "Number capped: 4\n",
      "Average cap: 5.278074741363525\n",
      "Number capped: 1\n",
      "Average cap: 45.25958251953125\n",
      "Number capped: 2\n",
      "Average cap: 0.26465728878974915\n",
      "Number capped: 6\n",
      "Average cap: 0.41441860795021057\n",
      "Number capped: 2\n",
      "Average cap: 13.891733169555664\n",
      "Number capped: 3\n",
      "Average cap: 0.6445819139480591\n",
      "Number capped: 3\n",
      "Average cap: 0.01590283028781414\n",
      "Number capped: 5\n",
      "Average cap: 0.01711772009730339\n",
      "Number capped: 7\n",
      "Average cap: 3.8214192390441895\n",
      "Number capped: 8\n",
      "Average cap: 6.388461589813232\n",
      "Number capped: 3\n",
      "Average cap: 0.0007071373402141035\n",
      "Number capped: 6\n",
      "Average cap: 4.939440727233887\n",
      "Number capped: 4\n",
      "Average cap: 14.791864395141602\n",
      "Number capped: 4\n",
      "Average cap: 0.5359965562820435\n",
      "Number capped: 3\n",
      "Average cap: 5.793701171875\n",
      "Number capped: 7\n",
      "Average cap: 0.0025725963059812784\n",
      "Number capped: 4\n",
      "Average cap: 6.354322910308838\n",
      "Number capped: 2\n",
      "Average cap: 137.08926391601562\n",
      "Number capped: 6\n",
      "Average cap: 0.6418899893760681\n",
      "Number capped: 3\n",
      "Average cap: 0.5004444718360901\n",
      "Number capped: 5\n",
      "Average cap: 0.977001965045929\n",
      "Number capped: 2\n",
      "Average cap: 13.914206504821777\n",
      "Number capped: 3\n",
      "Average cap: 9.23725700378418\n",
      "Number capped: 7\n",
      "Average cap: 4.0185956954956055\n",
      "Number capped: 6\n",
      "Average cap: 5.789890289306641\n",
      "Number capped: 6\n",
      "Average cap: 0.8180077075958252\n",
      "Number capped: 3\n",
      "Average cap: 8.74811840057373\n",
      "Number capped: 2\n",
      "Average cap: 14.585249900817871\n",
      "Number capped: 3\n",
      "Average cap: 10.023421287536621\n",
      "Number capped: 3\n",
      "Average cap: 0.4308900535106659\n",
      "Number capped: 5\n",
      "Average cap: 6.062040328979492\n",
      "Number capped: 5\n",
      "Average cap: 0.9078835248947144\n",
      "Number capped: 4\n",
      "Average cap: 8.632692337036133\n",
      "Number capped: 4\n",
      "Average cap: 1.3019757270812988\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 5\n",
      "Average cap: 0.5768172740936279\n",
      "Number capped: 9\n",
      "Average cap: 2.723536491394043\n",
      "Number capped: 4\n",
      "Average cap: 6.681751251220703\n",
      "Number capped: 3\n",
      "Average cap: 3.852522850036621\n",
      "Number capped: 3\n",
      "Average cap: 8.813401222229004\n",
      "Number capped: 9\n",
      "Average cap: 4.580581188201904\n",
      "Number capped: 3\n",
      "Average cap: 12.00772762298584\n",
      "Number capped: 9\n",
      "Average cap: 3.0839178562164307\n",
      "Number capped: 3\n",
      "Average cap: 8.875418663024902\n",
      "Number capped: 3\n",
      "Average cap: 2.0991904735565186\n",
      "Number capped: 6\n",
      "Average cap: 0.12996886670589447\n",
      "Number capped: 7\n",
      "Average cap: 0.8499671816825867\n",
      "Number capped: 6\n",
      "Average cap: 4.052361965179443\n",
      "Number capped: 5\n",
      "Average cap: 8.228986740112305\n",
      "Number capped: 6\n",
      "Average cap: 4.175962924957275\n",
      "Number capped: 5\n",
      "Average cap: 5.267755031585693\n",
      "Number capped: 3\n",
      "Average cap: 0.4157487452030182\n",
      "Number capped: 4\n",
      "Average cap: 0.00024316157214343548\n",
      "Number capped: 4\n",
      "Average cap: 0.04747142642736435\n",
      "Number capped: 3\n",
      "Average cap: 8.891403198242188\n",
      "Number capped: 6\n",
      "Average cap: 5.402217388153076\n",
      "Number capped: 1\n",
      "Average cap: 0.41111481189727783\n",
      "Number capped: 2\n",
      "Average cap: 14.122407913208008\n",
      "Number capped: 5\n",
      "Average cap: 5.367591381072998\n",
      "Number capped: 5\n",
      "Average cap: 6.5268073081970215\n",
      "Number capped: 2\n",
      "Average cap: 26.418962478637695\n",
      "Number capped: 8\n",
      "Average cap: 5.922969818115234\n",
      "Number capped: 4\n",
      "Average cap: 6.999180793762207\n",
      "Number capped: 5\n",
      "Average cap: 0.983454704284668\n",
      "Number capped: 2\n",
      "Average cap: 13.993521690368652\n",
      "Number capped: 2\n",
      "Average cap: 1.8174166679382324\n",
      "Number capped: 4\n",
      "Average cap: 6.612895488739014\n",
      "Number capped: 4\n",
      "Average cap: 7.4210662841796875\n",
      "Number capped: 3\n",
      "Average cap: 9.037397384643555\n",
      "Number capped: 4\n",
      "Average cap: 5.285905838012695\n",
      "Number capped: 2\n",
      "Average cap: 15.517057418823242\n",
      "Number capped: 6\n",
      "Average cap: 5.984528064727783\n",
      "Number capped: 5\n",
      "Average cap: 0.4547650218009949\n",
      "Number capped: 4\n",
      "Average cap: 2.3841855636419496e-06\n",
      "Number capped: 4\n",
      "Average cap: 7.106868743896484\n",
      "Number capped: 5\n",
      "Average cap: 5.57271671295166\n",
      "Number capped: 5\n",
      "Average cap: 2.4156477451324463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 4\n",
      "Average cap: 13.805532455444336\n",
      "Number capped: 7\n",
      "Average cap: 3.9975554943084717\n",
      "Number capped: 1\n",
      "Average cap: 28.60829734802246\n",
      "Number capped: 4\n",
      "Average cap: 0.422261118888855\n",
      "Number capped: 8\n",
      "Average cap: 0.001215716009028256\n",
      "Number capped: 3\n",
      "Average cap: 8.042351722717285\n",
      "Number capped: 4\n",
      "Average cap: 10.974159240722656\n",
      "Number capped: 5\n",
      "Average cap: 6.155730247497559\n",
      "Number capped: 6\n",
      "Average cap: 0.4276365339756012\n",
      "Number capped: 2\n",
      "Average cap: 0.4273594319820404\n",
      "Number capped: 2\n",
      "Average cap: 0.6421091556549072\n",
      "Number capped: 2\n",
      "Average cap: 12.927498817443848\n",
      "Number capped: 6\n",
      "Average cap: 0.45742154121398926\n",
      "Number capped: 7\n",
      "Average cap: 3.9752955436706543\n",
      "Number capped: 3\n",
      "Average cap: 8.989320755004883\n",
      "Number capped: 2\n",
      "Average cap: 13.236199378967285\n",
      "Number capped: 2\n",
      "Average cap: 16.25136375427246\n",
      "Number capped: 5\n",
      "Average cap: 5.766266822814941\n",
      "Number capped: 5\n",
      "Average cap: 5.516858100891113\n",
      "Number capped: 1\n",
      "Average cap: 27.04896354675293\n",
      "Number capped: 5\n",
      "Average cap: 6.187326908111572\n",
      "Number capped: 2\n",
      "Average cap: 13.06981086730957\n",
      "Number capped: 3\n",
      "Average cap: 0.8686575293540955\n",
      "Number capped: 1\n",
      "Average cap: 26.0540771484375\n",
      "Number capped: 6\n",
      "Average cap: 3.9815053939819336\n",
      "Number capped: 2\n",
      "Average cap: 0.0024019740521907806\n",
      "Number capped: 4\n",
      "Average cap: 7.541360855102539\n",
      "Number capped: 5\n",
      "Average cap: 8.147298812866211\n",
      "Number capped: 2\n",
      "Average cap: 13.835670471191406\n",
      "Number capped: 3\n",
      "Average cap: 0.09871381521224976\n",
      "Number capped: 4\n",
      "Average cap: 6.487260341644287\n",
      "Number capped: 4\n",
      "Average cap: 11.33234977722168\n",
      "Number capped: 7\n",
      "Average cap: 3.739588975906372\n",
      "Number capped: 3\n",
      "Average cap: 0.07827151566743851\n",
      "Number capped: 5\n",
      "Average cap: 5.347543716430664\n",
      "Number capped: 4\n",
      "Average cap: 1.112130045890808\n",
      "Number capped: 1\n",
      "Average cap: 0.010563676245510578\n",
      "Number capped: 3\n",
      "Average cap: 0.4715717136859894\n",
      "Number capped: 3\n",
      "Average cap: 9.134206771850586\n",
      "Number capped: 2\n",
      "Average cap: 13.013317108154297\n",
      "Number capped: 4\n",
      "Average cap: 0.8207083344459534\n",
      "Number capped: 3\n",
      "Average cap: 0.5010766983032227\n",
      "Number capped: 6\n",
      "Average cap: 4.56233024597168\n",
      "Number capped: 4\n",
      "Average cap: 0.41265150904655457\n",
      "Number capped: 5\n",
      "Average cap: 5.269761085510254\n",
      "Number capped: 5\n",
      "Average cap: 5.075305461883545\n",
      "Number capped: 5\n",
      "Average cap: 7.0368852615356445\n",
      "Number capped: 6\n",
      "Average cap: 0.4237150251865387\n",
      "Number capped: 4\n",
      "Average cap: 6.8896894454956055\n",
      "Number capped: 7\n",
      "Average cap: 3.478893280029297\n",
      "Number capped: 7\n",
      "Average cap: 0.32607579231262207\n",
      "Number capped: 1\n",
      "Average cap: 26.384443283081055\n",
      "Number capped: 6\n",
      "Average cap: 4.699063301086426\n",
      "Number capped: 4\n",
      "Average cap: 0.4349837899208069\n",
      "Number capped: 5\n",
      "Average cap: 5.833524703979492\n",
      "Number capped: 4\n",
      "Average cap: 7.318911552429199\n",
      "Number capped: 3\n",
      "Average cap: 16.75974464416504\n",
      "Number capped: 5\n",
      "Average cap: 1.4825549125671387\n",
      "Number capped: 3\n",
      "Average cap: 1.1302905082702637\n",
      "Number capped: 3\n",
      "Average cap: 9.197373390197754\n",
      "Number capped: 5\n",
      "Average cap: 5.443432807922363\n",
      "Number capped: 2\n",
      "Average cap: 14.444310188293457\n",
      "Number capped: 1\n",
      "Average cap: 112.93636322021484\n",
      "Number capped: 7\n",
      "Average cap: 7.866440773010254\n",
      "Number capped: 4\n",
      "Average cap: 6.451820373535156\n",
      "Number capped: 5\n",
      "Average cap: 6.440958499908447\n",
      "Number capped: 6\n",
      "Average cap: 0.4602300226688385\n",
      "Number capped: 5\n",
      "Average cap: 3.128535509109497\n",
      "Number capped: 4\n",
      "Average cap: 6.611571311950684\n",
      "Number capped: 4\n",
      "Average cap: 7.072745323181152\n",
      "Number capped: 2\n",
      "Average cap: 14.217823028564453\n",
      "Number capped: 8\n",
      "Average cap: 0.050412293523550034\n",
      "Number capped: 6\n",
      "Average cap: 0.47606348991394043\n",
      "Number capped: 3\n",
      "Average cap: 8.831669807434082\n",
      "Number capped: 9\n",
      "Average cap: 2.98024582862854\n",
      "Number capped: 5\n",
      "Average cap: 0.7276994585990906\n",
      "Number capped: 5\n",
      "Average cap: 0.43236058950424194\n",
      "Number capped: 9\n",
      "Average cap: 0.2865506708621979\n",
      "Number capped: 5\n",
      "Average cap: 8.739236831665039\n",
      "Number capped: 7\n",
      "Average cap: 0.4104670584201813\n",
      "Number capped: 7\n",
      "Average cap: 0.4384148418903351\n",
      "Number capped: 1\n",
      "Average cap: 25.87823486328125\n",
      "Number capped: 9\n",
      "Average cap: 3.8562774658203125\n",
      "Number capped: 5\n",
      "Average cap: 5.011850833892822\n",
      "Number capped: 4\n",
      "Average cap: 0.2713528573513031\n",
      "Number capped: 4\n",
      "Average cap: 0.5131974816322327\n",
      "Number capped: 5\n",
      "Average cap: 0.5259943604469299\n",
      "Number capped: 7\n",
      "Average cap: 3.4688968658447266\n",
      "Number capped: 8\n",
      "Average cap: 3.049217939376831\n",
      "Number capped: 8\n",
      "Average cap: 2.9287872314453125\n",
      "Number capped: 7\n",
      "Average cap: 4.339207225712016e-05\n",
      "Number capped: 8\n",
      "Average cap: 1.707986831665039\n",
      "Number capped: 3\n",
      "Average cap: 18.003156661987305\n",
      "Number capped: 11\n",
      "Average cap: 2.720104217529297\n",
      "Number capped: 6\n",
      "Average cap: 3.3698790073394775\n",
      "Number capped: 1\n",
      "Average cap: 0.007022743113338947\n",
      "Number capped: 6\n",
      "Average cap: 14.795321464538574\n",
      "Number capped: 3\n",
      "Average cap: 10.022028923034668\n",
      "Number capped: 6\n",
      "Average cap: 0.4129800796508789\n",
      "Number capped: 3\n",
      "Average cap: 0.4278731346130371\n",
      "Number capped: 4\n",
      "Average cap: 2.265089273452759\n",
      "Number capped: 4\n",
      "Average cap: 0.5634308457374573\n",
      "Number capped: 4\n",
      "Average cap: 6.513360500335693\n",
      "Number capped: 2\n",
      "Average cap: 33.695091247558594\n",
      "Number capped: 6\n",
      "Average cap: 0.6442309021949768\n",
      "Number capped: 1\n",
      "Average cap: 24.58702850341797\n",
      "Number capped: 4\n",
      "Average cap: 6.71196985244751\n",
      "Number capped: 2\n",
      "Average cap: 0.40704652667045593\n",
      "Number capped: 1\n",
      "Average cap: 27.73632049560547\n",
      "Number capped: 5\n",
      "Average cap: 3.434436082839966\n",
      "Number capped: 2\n",
      "Average cap: 9.703001976013184\n",
      "Number capped: 5\n",
      "Average cap: 0.11186089366674423\n",
      "Number capped: 4\n",
      "Average cap: 6.560235977172852\n",
      "Number capped: 4\n",
      "Average cap: 7.773698806762695\n",
      "Number capped: 2\n",
      "Average cap: 8.161829948425293\n",
      "Number capped: 8\n",
      "Average cap: 2.941478729248047\n",
      "Number capped: 8\n",
      "Average cap: 0.40641990303993225\n",
      "Number capped: 6\n",
      "Average cap: 4.439239025115967\n",
      "Number capped: 4\n",
      "Average cap: 6.769373893737793\n",
      "Number capped: 5\n",
      "Average cap: 0.010747727937996387\n",
      "Number capped: 5\n",
      "Average cap: 0.45380330085754395\n",
      "Number capped: 3\n",
      "Average cap: 8.53598690032959\n",
      "Number capped: 1\n",
      "Average cap: 20.261085510253906\n",
      "Number capped: 1\n",
      "Average cap: 35.25836181640625\n",
      "Number capped: 1\n",
      "Average cap: 26.456562042236328\n",
      "Number capped: 2\n",
      "Average cap: 12.972116470336914\n",
      "Number capped: 4\n",
      "Average cap: 6.536543846130371\n",
      "Number capped: 5\n",
      "Average cap: 9.31717300415039\n",
      "Number capped: 3\n",
      "Average cap: 8.316040992736816\n",
      "Number capped: 2\n",
      "Average cap: 0.27270105481147766\n",
      "Number capped: 6\n",
      "Average cap: 4.076815128326416\n",
      "Number capped: 2\n",
      "Average cap: 19.30299186706543\n",
      "Number capped: 4\n",
      "Average cap: 6.177336692810059\n",
      "Number capped: 4\n",
      "Average cap: 3.1469225883483887\n",
      "Number capped: 6\n",
      "Average cap: 4.376438140869141\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 4\n",
      "Average cap: 0.6460368633270264\n",
      "Number capped: 3\n",
      "Average cap: 0.04197308421134949\n",
      "Number capped: 2\n",
      "Average cap: 14.380242347717285\n",
      "Number capped: 2\n",
      "Average cap: 0.209926575422287\n",
      "Number capped: 7\n",
      "Average cap: 7.287719249725342\n",
      "Number capped: 2\n",
      "Average cap: 0.5906741619110107\n",
      "Number capped: 2\n",
      "Average cap: 0.442696213722229\n",
      "Number capped: 1\n",
      "Average cap: 26.40416717529297\n",
      "Number capped: 8\n",
      "Average cap: 0.9418720006942749\n",
      "Number capped: 4\n",
      "Average cap: 15.882396697998047\n",
      "Number capped: 4\n",
      "Average cap: 0.4559502601623535\n",
      "Number capped: 3\n",
      "Average cap: 10.133578300476074\n",
      "Number capped: 4\n",
      "Average cap: 7.340030670166016\n",
      "Number capped: 6\n",
      "Average cap: 6.278142929077148\n",
      "Number capped: 6\n",
      "Average cap: 4.473540782928467\n",
      "Number capped: 6\n",
      "Average cap: 0.022897837683558464\n",
      "Number capped: 10\n",
      "Average cap: 3.9904379844665527\n",
      "Number capped: 5\n",
      "Average cap: 0.01219475269317627\n",
      "Number capped: 2\n",
      "Average cap: 13.13172435760498\n",
      "Number capped: 6\n",
      "Average cap: 4.582189083099365\n",
      "Number capped: 6\n",
      "Average cap: 1.257548451423645\n",
      "Number capped: 9\n",
      "Average cap: 3.624863862991333\n",
      "Number capped: 6\n",
      "Average cap: 3.9169387817382812\n",
      "Number capped: 5\n",
      "Average cap: 5.439160346984863\n",
      "Number capped: 3\n",
      "Average cap: 8.919513702392578\n",
      "Number capped: 3\n",
      "Average cap: 0.4375098645687103\n",
      "Number capped: 5\n",
      "Average cap: 0.48542889952659607\n",
      "Number capped: 4\n",
      "Average cap: 0.8700180053710938\n",
      "Number capped: 1\n",
      "Average cap: 1.16721510887146\n",
      "Number capped: 5\n",
      "Average cap: 5.4442877769470215\n",
      "Number capped: 3\n",
      "Average cap: 0.7175643444061279\n",
      "Number capped: 4\n",
      "Average cap: 6.413909912109375\n",
      "Number capped: 5\n",
      "Average cap: 5.531713008880615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 1\n",
      "Average cap: 26.70292854309082\n",
      "Number capped: 1\n",
      "Average cap: 13.086671829223633\n",
      "Number capped: 4\n",
      "Average cap: 8.170807838439941\n",
      "Number capped: 1\n",
      "Average cap: 52.39128875732422\n",
      "Number capped: 3\n",
      "Average cap: 0.00474353926256299\n",
      "Number capped: 6\n",
      "Average cap: 11.054068565368652\n",
      "Number capped: 10\n",
      "Average cap: 2.510920524597168\n",
      "Number capped: 2\n",
      "Average cap: 0.838131308555603\n",
      "Number capped: 2\n",
      "Average cap: 12.821102142333984\n",
      "Number capped: 4\n",
      "Average cap: 6.111728668212891\n",
      "Number capped: 6\n",
      "Average cap: 0.6269121766090393\n",
      "Number capped: 4\n",
      "Average cap: 1.8975619077682495\n",
      "Number capped: 1\n",
      "Average cap: 0.021065887063741684\n",
      "Number capped: 4\n",
      "Average cap: 6.148319721221924\n",
      "Number capped: 3\n",
      "Average cap: 2.2310314178466797\n",
      "Number capped: 3\n",
      "Average cap: 7.6178765296936035\n",
      "Number capped: 5\n",
      "Average cap: 4.993229866027832\n",
      "Number capped: 4\n",
      "Average cap: 6.108267784118652\n",
      "Number capped: 1\n",
      "Average cap: 33.64057159423828\n",
      "Number capped: 7\n",
      "Average cap: 4.0492424964904785\n",
      "Number capped: 3\n",
      "Average cap: 8.90006160736084\n",
      "Number capped: 4\n",
      "Average cap: 1.5112920999526978\n",
      "Number capped: 4\n",
      "Average cap: 0.2692899703979492\n",
      "Number capped: 4\n",
      "Average cap: 7.809778690338135\n",
      "Number capped: 2\n",
      "Average cap: 0.08915656805038452\n",
      "Number capped: 1\n",
      "Average cap: 7.776432037353516\n",
      "Number capped: 3\n",
      "Average cap: 10.486946105957031\n",
      "Number capped: 4\n",
      "Average cap: 10.110981941223145\n",
      "Number capped: 5\n",
      "Average cap: 5.134097099304199\n",
      "Number capped: 5\n",
      "Average cap: 5.6790947914123535\n",
      "Number capped: 2\n",
      "Average cap: 21.22277069091797\n",
      "Number capped: 4\n",
      "Average cap: 7.141749382019043\n",
      "Number capped: 7\n",
      "Average cap: 4.242969036102295\n",
      "Number capped: 3\n",
      "Average cap: 8.445929527282715\n",
      "Number capped: 4\n",
      "Average cap: 21.085329055786133\n",
      "Number capped: 8\n",
      "Average cap: 0.14970038831233978\n",
      "Number capped: 5\n",
      "Average cap: 1.4394667148590088\n",
      "Number capped: 6\n",
      "Average cap: 4.5235700607299805\n",
      "Number capped: 8\n",
      "Average cap: 5.8308820724487305\n",
      "Number capped: 4\n",
      "Average cap: 6.596534729003906\n",
      "Number capped: 3\n",
      "Average cap: 0.7847999930381775\n",
      "Number capped: 5\n",
      "Average cap: 5.169760227203369\n",
      "Number capped: 4\n",
      "Average cap: 6.441823959350586\n",
      "Number capped: 4\n",
      "Average cap: 1.2880432605743408\n",
      "Number capped: 7\n",
      "Average cap: 8.27145004272461\n",
      "Number capped: 3\n",
      "Average cap: 7.457785129547119\n",
      "Number capped: 4\n",
      "Average cap: 10.073556900024414\n",
      "Number capped: 5\n",
      "Average cap: 7.430937767028809\n",
      "Number capped: 5\n",
      "Average cap: 5.454372882843018\n",
      "Number capped: 5\n",
      "Average cap: 4.068206310272217\n",
      "Number capped: 9\n",
      "Average cap: 3.674387216567993\n",
      "Number capped: 4\n",
      "Average cap: 3.814692536252551e-05\n",
      "Number capped: 3\n",
      "Average cap: 0.0\n",
      "Number capped: 8\n",
      "Average cap: 1.3491771221160889\n",
      "Number capped: 9\n",
      "Average cap: 2.5842702388763428\n",
      "Number capped: 4\n",
      "Average cap: 4.106989860534668\n",
      "Number capped: 9\n",
      "Average cap: 2.876243829727173\n",
      "Number capped: 4\n",
      "Average cap: 6.548583507537842\n",
      "Number capped: 2\n",
      "Average cap: 15.6333589553833\n",
      "Number capped: 3\n",
      "Average cap: 3.171398878097534\n",
      "Number capped: 3\n",
      "Average cap: 8.253056526184082\n",
      "Number capped: 3\n",
      "Average cap: 10.237792015075684\n",
      "Number capped: 3\n",
      "Average cap: 4.079188823699951\n",
      "Number capped: 2\n",
      "Average cap: 0.4067496657371521\n",
      "Number capped: 2\n",
      "Average cap: 32.340972900390625\n",
      "Number capped: 4\n",
      "Average cap: 0.714120090007782\n",
      "Number capped: 7\n",
      "Average cap: 1.029953956604004\n",
      "Number capped: 5\n",
      "Average cap: 9.191765785217285\n",
      "Number capped: 6\n",
      "Average cap: 5.841237452841597e-06\n",
      "Number capped: 6\n",
      "Average cap: 4.5507588386535645\n",
      "Number capped: 4\n",
      "Average cap: 0.9046292901039124\n",
      "Number capped: 4\n",
      "Average cap: 6.157526969909668\n",
      "Number capped: 4\n",
      "Average cap: 0.5789069533348083\n",
      "Number capped: 6\n",
      "Average cap: 4.509103298187256\n",
      "Number capped: 5\n",
      "Average cap: 0.4646305441856384\n",
      "Number capped: 4\n",
      "Average cap: 0.01645570807158947\n",
      "Number capped: 5\n",
      "Average cap: 0.6945799589157104\n",
      "Number capped: 2\n",
      "Average cap: 13.502973556518555\n",
      "Number capped: 6\n",
      "Average cap: 4.312489986419678\n",
      "Number capped: 5\n",
      "Average cap: 0.014867806807160378\n",
      "Number capped: 12\n",
      "Average cap: 4.474623203277588\n",
      "Number capped: 1\n",
      "Average cap: 0.006017885636538267\n",
      "Number capped: 2\n",
      "Average cap: 12.687433242797852\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 2\n",
      "Average cap: 0.01057502068579197\n",
      "Number capped: 4\n",
      "Average cap: 5.269494533538818\n",
      "Number capped: 3\n",
      "Average cap: 8.631478309631348\n",
      "Number capped: 7\n",
      "Average cap: 3.7072479724884033\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 20.197208404541016\n",
      "Number capped: 9\n",
      "Average cap: 0.8921147584915161\n",
      "Number capped: 4\n",
      "Average cap: 0.4263191521167755\n",
      "Number capped: 8\n",
      "Average cap: 3.072202444076538\n",
      "Number capped: 4\n",
      "Average cap: 9.620072364807129\n",
      "Number capped: 3\n",
      "Average cap: 2.503530740737915\n",
      "Number capped: 3\n",
      "Average cap: 1.6498661041259766\n",
      "Number capped: 6\n",
      "Average cap: 4.643273830413818\n",
      "Number capped: 2\n",
      "Average cap: 0.8712693452835083\n",
      "Number capped: 6\n",
      "Average cap: 5.059112071990967\n",
      "Number capped: 1\n",
      "Average cap: 28.66600799560547\n",
      "Number capped: 5\n",
      "Average cap: 2.0238397121429443\n",
      "Number capped: 2\n",
      "Average cap: 0.3229277729988098\n",
      "Number capped: 1\n",
      "Average cap: 50.53813171386719\n",
      "Number capped: 6\n",
      "Average cap: 4.35010290145874\n",
      "Number capped: 2\n",
      "Average cap: 13.437891960144043\n",
      "Number capped: 5\n",
      "Average cap: 5.351762294769287\n",
      "Number capped: 4\n",
      "Average cap: 6.177938461303711\n",
      "Number capped: 7\n",
      "Average cap: 3.2667529582977295\n",
      "Number capped: 4\n",
      "Average cap: 3.689295768737793\n",
      "Number capped: 4\n",
      "Average cap: 6.029868125915527\n",
      "Number capped: 2\n",
      "Average cap: 21.053787231445312\n",
      "Number capped: 5\n",
      "Average cap: 1.9658381938934326\n",
      "Number capped: 3\n",
      "Average cap: 26.65062141418457\n",
      "Number capped: 2\n",
      "Average cap: 22.885066986083984\n",
      "Number capped: 1\n",
      "Average cap: 29.65312385559082\n",
      "Number capped: 2\n",
      "Average cap: 0.44057267904281616\n",
      "Number capped: 6\n",
      "Average cap: 3.783026933670044\n",
      "Number capped: 5\n",
      "Average cap: 7.021432399749756\n",
      "Number capped: 3\n",
      "Average cap: 0.46267905831336975\n",
      "Number capped: 4\n",
      "Average cap: 1.3742039203643799\n",
      "Number capped: 8\n",
      "Average cap: 0.4294435977935791\n",
      "Number capped: 2\n",
      "Average cap: 17.575759887695312\n",
      "Number capped: 4\n",
      "Average cap: 10.93456745147705\n",
      "Number capped: 7\n",
      "Average cap: 0.002738101175054908\n",
      "Number capped: 4\n",
      "Average cap: 0.8509473204612732\n",
      "Number capped: 7\n",
      "Average cap: 3.761777639389038\n",
      "Number capped: 4\n",
      "Average cap: 0.3372098207473755\n",
      "Number capped: 4\n",
      "Average cap: 5.895793437957764\n",
      "Number capped: 3\n",
      "Average cap: 1.6070088148117065\n",
      "Number capped: 6\n",
      "Average cap: 0.42943620681762695\n",
      "Number capped: 4\n",
      "Average cap: 0.5727585554122925\n",
      "Number capped: 4\n",
      "Average cap: 6.115711212158203\n",
      "Number capped: 5\n",
      "Average cap: 6.9618181441910565e-06\n",
      "Number capped: 8\n",
      "Average cap: 2.9710659980773926\n",
      "Number capped: 6\n",
      "Average cap: 9.538677215576172\n",
      "Number capped: 7\n",
      "Average cap: 0.889397144317627\n",
      "Number capped: 5\n",
      "Average cap: 5.500853538513184\n",
      "Number capped: 3\n",
      "Average cap: 0.721331775188446\n",
      "Number capped: 1\n",
      "Average cap: 3.6388068199157715\n",
      "Number capped: 6\n",
      "Average cap: 4.940592288970947\n",
      "Number capped: 1\n",
      "Average cap: 25.514659881591797\n",
      "Number capped: 4\n",
      "Average cap: 1.2801337242126465\n",
      "Number capped: 2\n",
      "Average cap: 0.7331781387329102\n",
      "Number capped: 6\n",
      "Average cap: 0.002851661527529359\n",
      "Number capped: 3\n",
      "Average cap: 8.664566993713379\n",
      "Number capped: 3\n",
      "Average cap: 1.625509262084961\n",
      "Number capped: 7\n",
      "Average cap: 0.5579109787940979\n",
      "Number capped: 4\n",
      "Average cap: 6.6038126945495605\n",
      "Number capped: 7\n",
      "Average cap: 5.897488594055176\n",
      "Number capped: 3\n",
      "Average cap: 0.30406245589256287\n",
      "Number capped: 3\n",
      "Average cap: 15.793530464172363\n",
      "Number capped: 1\n",
      "Average cap: 0.669969379901886\n",
      "Number capped: 3\n",
      "Average cap: 17.46657371520996\n",
      "Number capped: 6\n",
      "Average cap: 1.406861662864685\n",
      "Number capped: 5\n",
      "Average cap: 4.956900596618652\n",
      "Number capped: 4\n",
      "Average cap: 6.36759614944458\n",
      "Number capped: 4\n",
      "Average cap: 6.235894203186035\n",
      "Number capped: 3\n",
      "Average cap: 9.841981887817383\n",
      "Number capped: 5\n",
      "Average cap: 5.553561210632324\n",
      "Number capped: 3\n",
      "Average cap: 8.898613929748535\n",
      "Number capped: 7\n",
      "Average cap: 0.954426109790802\n",
      "Number capped: 2\n",
      "Average cap: 12.062522888183594\n",
      "Number capped: 4\n",
      "Average cap: 6.67086124420166\n",
      "Number capped: 7\n",
      "Average cap: 0.00015531117969658226\n",
      "Number capped: 1\n",
      "Average cap: 53.940399169921875\n",
      "Number capped: 4\n",
      "Average cap: 14.350284576416016\n",
      "Number capped: 3\n",
      "Average cap: 16.6595516204834\n",
      "Number capped: 4\n",
      "Average cap: 6.53408670425415\n",
      "Number capped: 4\n",
      "Average cap: 6.06709098815918\n",
      "Number capped: 2\n",
      "Average cap: 12.930368423461914\n",
      "Number capped: 5\n",
      "Average cap: 5.0668134689331055\n",
      "Number capped: 3\n",
      "Average cap: 1.4288244247436523\n",
      "Number capped: 4\n",
      "Average cap: 0.011267861351370811\n",
      "Number capped: 3\n",
      "Average cap: 6.311336994171143\n",
      "Number capped: 5\n",
      "Average cap: 0.4185876250267029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 4\n",
      "Average cap: 0.6220099925994873\n",
      "Number capped: 2\n",
      "Average cap: 12.744257926940918\n",
      "Number capped: 2\n",
      "Average cap: 12.98748779296875\n",
      "Number capped: 4\n",
      "Average cap: 0.4174787104129791\n",
      "Number capped: 3\n",
      "Average cap: 8.693628311157227\n",
      "Number capped: 7\n",
      "Average cap: 2.07606840133667\n",
      "Number capped: 3\n",
      "Average cap: 22.355939865112305\n",
      "Number capped: 4\n",
      "Average cap: 6.709818363189697\n",
      "Number capped: 2\n",
      "Average cap: 12.804908752441406\n",
      "Number capped: 7\n",
      "Average cap: 5.587507724761963\n",
      "Number capped: 6\n",
      "Average cap: 4.109566688537598\n",
      "Number capped: 4\n",
      "Average cap: 7.042386054992676\n",
      "Number capped: 6\n",
      "Average cap: 4.627072811126709\n",
      "Number capped: 7\n",
      "Average cap: 0.022311141714453697\n",
      "Number capped: 2\n",
      "Average cap: 8.859745025634766\n",
      "Number capped: 2\n",
      "Average cap: 16.475116729736328\n",
      "Number capped: 3\n",
      "Average cap: 0.00197297683916986\n",
      "Number capped: 5\n",
      "Average cap: 5.232940673828125\n",
      "Number capped: 7\n",
      "Average cap: 0.03645595908164978\n",
      "Number capped: 3\n",
      "Average cap: 8.700287818908691\n",
      "Number capped: 2\n",
      "Average cap: 12.951848983764648\n",
      "Number capped: 1\n",
      "Average cap: 0.4306582510471344\n",
      "Number capped: 1\n",
      "Average cap: 0.40175196528434753\n",
      "Number capped: 6\n",
      "Average cap: 4.249680995941162\n",
      "Number capped: 1\n",
      "Average cap: 0.6683162450790405\n",
      "Number capped: 5\n",
      "Average cap: 4.863733011006843e-06\n",
      "Number capped: 2\n",
      "Average cap: 6.12716976320371e-05\n",
      "Number capped: 5\n",
      "Average cap: 10.978185653686523\n",
      "Number capped: 3\n",
      "Average cap: 0.4673703610897064\n",
      "Number capped: 4\n",
      "Average cap: 0.46214303374290466\n",
      "Number capped: 4\n",
      "Average cap: 6.293542385101318\n",
      "Number capped: 6\n",
      "Average cap: 4.7050862312316895\n",
      "Number capped: 8\n",
      "Average cap: 3.190817356109619\n",
      "Number capped: 3\n",
      "Average cap: 8.8698091506958\n",
      "Number capped: 7\n",
      "Average cap: 0.4061501920223236\n",
      "Number capped: 8\n",
      "Average cap: 0.5135326385498047\n",
      "Number capped: 1\n",
      "Average cap: 0.9838480949401855\n",
      "Number capped: 3\n",
      "Average cap: 10.065315246582031\n",
      "Number capped: 5\n",
      "Average cap: 1.3123304843902588\n",
      "Number capped: 4\n",
      "Average cap: 0.33514517545700073\n",
      "Number capped: 2\n",
      "Average cap: 0.2712068259716034\n",
      "Number capped: 4\n",
      "Average cap: 6.180309295654297\n",
      "Number capped: 6\n",
      "Average cap: 5.013096809387207\n",
      "Number capped: 2\n",
      "Average cap: 1.458486795425415\n",
      "Number capped: 6\n",
      "Average cap: 0.3730282783508301\n",
      "Number capped: 5\n",
      "Average cap: 5.426597595214844\n",
      "Number capped: 4\n",
      "Average cap: 0.16013674437999725\n",
      "Number capped: 3\n",
      "Average cap: 12.609177589416504\n",
      "Number capped: 8\n",
      "Average cap: 0.00791233405470848\n",
      "Number capped: 2\n",
      "Average cap: 12.876404762268066\n",
      "Number capped: 4\n",
      "Average cap: 6.945466995239258\n",
      "Number capped: 1\n",
      "Average cap: 1.9736638069152832\n",
      "Number capped: 5\n",
      "Average cap: 0.44543662667274475\n",
      "Number capped: 2\n",
      "Average cap: 0.9439710378646851\n",
      "Number capped: 4\n",
      "Average cap: 6.318244457244873\n",
      "Number capped: 3\n",
      "Average cap: 5.316716487868689e-05\n",
      "Number capped: 4\n",
      "Average cap: 6.392214298248291\n",
      "Number capped: 3\n",
      "Average cap: 11.191523551940918\n",
      "Number capped: 6\n",
      "Average cap: 4.331973075866699\n",
      "Number capped: 3\n",
      "Average cap: 8.06006908416748\n",
      "Number capped: 1\n",
      "Average cap: 27.14978790283203\n",
      "Number capped: 5\n",
      "Average cap: 0.4481261372566223\n",
      "Number capped: 1\n",
      "Average cap: 29.918228149414062\n",
      "Number capped: 7\n",
      "Average cap: 4.053033626405522e-05\n",
      "Number capped: 3\n",
      "Average cap: 0.04295462369918823\n",
      "Number capped: 3\n",
      "Average cap: 8.481705665588379\n",
      "Number capped: 6\n",
      "Average cap: 1.0234426259994507\n",
      "Number capped: 4\n",
      "Average cap: 11.340347290039062\n",
      "Number capped: 6\n",
      "Average cap: 3.8848695755004883\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 11\n",
      "Average cap: 1.023113489151001\n",
      "Number capped: 6\n",
      "Average cap: 6.199535846710205\n",
      "Number capped: 10\n",
      "Average cap: 5.127927303314209\n",
      "Number capped: 3\n",
      "Average cap: 8.10423755645752\n",
      "Number capped: 3\n",
      "Average cap: 0.8878901600837708\n",
      "Number capped: 2\n",
      "Average cap: 12.440303802490234\n",
      "Number capped: 5\n",
      "Average cap: 5.2809247970581055\n",
      "Number capped: 4\n",
      "Average cap: 0.4406704604625702\n",
      "Number capped: 5\n",
      "Average cap: 0.10911629348993301\n",
      "Number capped: 6\n",
      "Average cap: 1.190583348274231\n",
      "Number capped: 3\n",
      "Average cap: 8.483025550842285\n",
      "Number capped: 8\n",
      "Average cap: 3.663862466812134\n",
      "Number capped: 2\n",
      "Average cap: 0.2852349579334259\n",
      "Number capped: 2\n",
      "Average cap: 2.532982110977173\n",
      "Number capped: 6\n",
      "Average cap: 0.9729127883911133\n",
      "Number capped: 1\n",
      "Average cap: 24.794857025146484\n",
      "Number capped: 4\n",
      "Average cap: 6.245891571044922\n",
      "Number capped: 5\n",
      "Average cap: 5.351199150085449\n",
      "Number capped: 3\n",
      "Average cap: 8.525612831115723\n",
      "Number capped: 2\n",
      "Average cap: 12.820152282714844\n",
      "Number capped: 4\n",
      "Average cap: 1.5166152715682983\n",
      "Number capped: 7\n",
      "Average cap: 3.2817327976226807\n",
      "Number capped: 2\n",
      "Average cap: 18.173036575317383\n",
      "Number capped: 6\n",
      "Average cap: 4.414224147796631\n",
      "Number capped: 3\n",
      "Average cap: 7.96086311340332\n",
      "Number capped: 3\n",
      "Average cap: 8.360160827636719\n",
      "Number capped: 6\n",
      "Average cap: 9.76944351196289\n",
      "Number capped: 4\n",
      "Average cap: 0.5123533010482788\n",
      "Number capped: 5\n",
      "Average cap: 8.948086738586426\n",
      "Number capped: 4\n",
      "Average cap: 8.457687377929688\n",
      "Number capped: 4\n",
      "Average cap: 26.814559936523438\n",
      "Number capped: 4\n",
      "Average cap: 6.572746276855469\n",
      "Number capped: 5\n",
      "Average cap: 5.124393939971924\n",
      "Number capped: 5\n",
      "Average cap: 0.02350728027522564\n",
      "Number capped: 3\n",
      "Average cap: 8.659235954284668\n",
      "Number capped: 1\n",
      "Average cap: 69.1035385131836\n",
      "Number capped: 4\n",
      "Average cap: 0.5047769546508789\n",
      "Number capped: 5\n",
      "Average cap: 5.865636825561523\n",
      "Number capped: 4\n",
      "Average cap: 6.664595603942871\n",
      "Number capped: 4\n",
      "Average cap: 13.256412506103516\n",
      "Number capped: 7\n",
      "Average cap: 0.6105228066444397\n",
      "Number capped: 5\n",
      "Average cap: 3.3090202808380127\n",
      "Number capped: 7\n",
      "Average cap: 1.2791107892990112\n",
      "Number capped: 2\n",
      "Average cap: 14.140761375427246\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 6\n",
      "Average cap: 3.8560142517089844\n",
      "Number capped: 3\n",
      "Average cap: 7.829329967498779\n",
      "Number capped: 4\n",
      "Average cap: 6.661465167999268\n",
      "Number capped: 4\n",
      "Average cap: 6.349359035491943\n",
      "Number capped: 5\n",
      "Average cap: 5.0924577713012695\n",
      "Number capped: 5\n",
      "Average cap: 9.453907012939453\n",
      "Number capped: 6\n",
      "Average cap: 0.5303239226341248\n",
      "Number capped: 2\n",
      "Average cap: 13.466867446899414\n",
      "Number capped: 4\n",
      "Average cap: 6.17616081237793\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 2\n",
      "Average cap: 12.591114044189453\n",
      "Number capped: 3\n",
      "Average cap: 0.0651894137263298\n",
      "Number capped: 8\n",
      "Average cap: 3.339273691177368\n",
      "Number capped: 10\n",
      "Average cap: 2.5661799907684326\n",
      "Number capped: 4\n",
      "Average cap: 6.034903049468994\n",
      "Number capped: 3\n",
      "Average cap: 13.107017517089844\n",
      "Number capped: 6\n",
      "Average cap: 5.46208381652832\n",
      "Number capped: 5\n",
      "Average cap: 4.755363464355469\n",
      "Number capped: 4\n",
      "Average cap: 4.235177040100098\n",
      "Number capped: 3\n",
      "Average cap: 8.395697593688965\n",
      "Number capped: 1\n",
      "Average cap: 25.577085494995117\n",
      "Number capped: 5\n",
      "Average cap: 0.6607920527458191\n",
      "Number capped: 5\n",
      "Average cap: 0.1402912139892578\n",
      "Number capped: 4\n",
      "Average cap: 7.785529136657715\n",
      "Number capped: 6\n",
      "Average cap: 4.239172458648682\n",
      "Number capped: 6\n",
      "Average cap: 6.2338480949401855\n",
      "Number capped: 5\n",
      "Average cap: 0.40376442670822144\n",
      "Number capped: 3\n",
      "Average cap: 8.210909843444824\n",
      "Number capped: 4\n",
      "Average cap: 2.2561511993408203\n",
      "Number capped: 4\n",
      "Average cap: 7.0969977378845215\n",
      "Number capped: 10\n",
      "Average cap: 3.4198360443115234\n",
      "Number capped: 6\n",
      "Average cap: 6.494716644287109\n",
      "Number capped: 4\n",
      "Average cap: 9.239860534667969\n",
      "Number capped: 7\n",
      "Average cap: 0.42914697527885437\n",
      "Number capped: 4\n",
      "Average cap: 6.900280952453613\n",
      "Number capped: 3\n",
      "Average cap: 0.6634547710418701\n",
      "Number capped: 2\n",
      "Average cap: 12.686710357666016\n",
      "Number capped: 1\n",
      "Average cap: 6.247890949249268\n",
      "Number capped: 1\n",
      "Average cap: 24.631025314331055\n",
      "Number capped: 9\n",
      "Average cap: 2.023449659347534\n",
      "Number capped: 1\n",
      "Average cap: 26.149280548095703\n",
      "Number capped: 4\n",
      "Average cap: 6.516360282897949\n",
      "Number capped: 2\n",
      "Average cap: 11.934850692749023\n",
      "Number capped: 12\n",
      "Average cap: 0.036328308284282684\n",
      "Number capped: 1\n",
      "Average cap: 43.174137115478516\n",
      "Number capped: 3\n",
      "Average cap: 8.556329727172852\n",
      "Number capped: 5\n",
      "Average cap: 5.001434326171875\n",
      "Number capped: 5\n",
      "Average cap: 8.934020042419434\n",
      "Number capped: 4\n",
      "Average cap: 0.0008519647526554763\n",
      "Number capped: 6\n",
      "Average cap: 1.24032723903656\n",
      "Number capped: 5\n",
      "Average cap: 0.006344890687614679\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 5\n",
      "Average cap: 4.748809814453125\n",
      "Number capped: 3\n",
      "Average cap: 9.983275413513184\n",
      "Number capped: 5\n",
      "Average cap: 12.095399856567383\n",
      "Number capped: 7\n",
      "Average cap: 4.502376556396484\n",
      "Number capped: 4\n",
      "Average cap: 11.926017761230469\n",
      "Number capped: 2\n",
      "Average cap: 0.453176885843277\n",
      "Number capped: 4\n",
      "Average cap: 0.011730344966053963\n",
      "Number capped: 6\n",
      "Average cap: 7.07288122177124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 2\n",
      "Average cap: 0.2571394145488739\n",
      "Number capped: 5\n",
      "Average cap: 5.403371810913086\n",
      "Number capped: 1\n",
      "Average cap: 26.177257537841797\n",
      "Number capped: 5\n",
      "Average cap: 0.6104647517204285\n",
      "Number capped: 4\n",
      "Average cap: 6.245428085327148\n",
      "Number capped: 7\n",
      "Average cap: 2.18190598487854\n",
      "Number capped: 3\n",
      "Average cap: 4.098390102386475\n",
      "Number capped: 2\n",
      "Average cap: 12.318610191345215\n",
      "Number capped: 8\n",
      "Average cap: 3.0682621002197266\n",
      "Number capped: 7\n",
      "Average cap: 1.0832502841949463\n",
      "Number capped: 4\n",
      "Average cap: 6.52804708480835\n",
      "Number capped: 2\n",
      "Average cap: 0.3999720513820648\n",
      "Number capped: 6\n",
      "Average cap: 7.699466705322266\n",
      "Number capped: 8\n",
      "Average cap: 3.387955665588379\n",
      "Number capped: 4\n",
      "Average cap: 0.0013034968869760633\n",
      "Number capped: 3\n",
      "Average cap: 0.03763873502612114\n",
      "Number capped: 4\n",
      "Average cap: 0.4007868468761444\n",
      "Number capped: 2\n",
      "Average cap: 12.327190399169922\n",
      "Number capped: 5\n",
      "Average cap: 5.17604923248291\n",
      "Number capped: 7\n",
      "Average cap: 3.1393704414367676\n",
      "Number capped: 3\n",
      "Average cap: 0.29533636569976807\n",
      "Number capped: 4\n",
      "Average cap: 1.038782000541687\n",
      "Number capped: 1\n",
      "Average cap: 25.15399932861328\n",
      "Number capped: 6\n",
      "Average cap: 4.732315540313721\n",
      "Number capped: 1\n",
      "Average cap: 0.7049360871315002\n",
      "Number capped: 6\n",
      "Average cap: 3.8079898357391357\n",
      "Number capped: 3\n",
      "Average cap: 0.008601129055023193\n",
      "Number capped: 4\n",
      "Average cap: 6.174427509307861\n",
      "Number capped: 9\n",
      "Average cap: 2.8842430114746094\n",
      "Number capped: 6\n",
      "Average cap: 5.155897617340088\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 4\n",
      "Average cap: 0.38218608498573303\n",
      "Number capped: 6\n",
      "Average cap: 1.0748753547668457\n",
      "Number capped: 3\n",
      "Average cap: 8.171433448791504\n",
      "Number capped: 4\n",
      "Average cap: 0.18568472564220428\n",
      "Number capped: 2\n",
      "Average cap: 13.68109130859375\n",
      "Number capped: 5\n",
      "Average cap: 0.7156513333320618\n",
      "Number capped: 3\n",
      "Average cap: 8.126852989196777\n",
      "Number capped: 2\n",
      "Average cap: 4.570513725280762\n",
      "Number capped: 5\n",
      "Average cap: 5.349031925201416\n",
      "Number capped: 7\n",
      "Average cap: 7.682063102722168\n",
      "Number capped: 9\n",
      "Average cap: 4.4516777992248535\n",
      "Number capped: 3\n",
      "Average cap: 8.320579528808594\n",
      "Number capped: 2\n",
      "Average cap: 2.324579872947652e-05\n",
      "Number capped: 4\n",
      "Average cap: 0.46890509128570557\n",
      "Number capped: 6\n",
      "Average cap: 3.9777934551239014\n",
      "Number capped: 4\n",
      "Average cap: 6.337977886199951\n",
      "Number capped: 3\n",
      "Average cap: 8.416736602783203\n",
      "Number capped: 7\n",
      "Average cap: 1.3489323854446411\n",
      "Number capped: 3\n",
      "Average cap: 9.3165864944458\n",
      "Number capped: 3\n",
      "Average cap: 0.0366826169192791\n",
      "Number capped: 4\n",
      "Average cap: 6.353977203369141\n",
      "Number capped: 3\n",
      "Average cap: 15.487358093261719\n",
      "Number capped: 3\n",
      "Average cap: 0.0\n",
      "Number capped: 5\n",
      "Average cap: 0.4257737994194031\n",
      "Number capped: 4\n",
      "Average cap: 6.464206695556641\n",
      "Number capped: 5\n",
      "Average cap: 5.59366512298584\n",
      "Number capped: 7\n",
      "Average cap: 1.0221346616744995\n",
      "Number capped: 12\n",
      "Average cap: 4.262539863586426\n",
      "Number capped: 5\n",
      "Average cap: 8.422530174255371\n",
      "Number capped: 2\n",
      "Average cap: 19.911884307861328\n",
      "Number capped: 3\n",
      "Average cap: 8.223727226257324\n",
      "Number capped: 5\n",
      "Average cap: 4.7367987632751465\n",
      "Number capped: 4\n",
      "Average cap: 0.5794898271560669\n",
      "Number capped: 3\n",
      "Average cap: 8.119242668151855\n",
      "Number capped: 1\n",
      "Average cap: 6.873675346374512\n",
      "Number capped: 4\n",
      "Average cap: 6.4453558921813965\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 3\n",
      "Average cap: 28.069923400878906\n",
      "Number capped: 3\n",
      "Average cap: 8.192538261413574\n",
      "Number capped: 2\n",
      "Average cap: 12.110567092895508\n",
      "Number capped: 2\n",
      "Average cap: 13.162382125854492\n",
      "Number capped: 3\n",
      "Average cap: 7.94198751449585\n",
      "Number capped: 5\n",
      "Average cap: 0.42908430099487305\n",
      "Number capped: 2\n",
      "Average cap: 18.149259567260742\n",
      "Number capped: 2\n",
      "Average cap: 20.833213806152344\n",
      "Number capped: 3\n",
      "Average cap: 8.225159645080566\n",
      "Number capped: 7\n",
      "Average cap: 0.10449237376451492\n",
      "Number capped: 6\n",
      "Average cap: 4.253438949584961\n",
      "Number capped: 2\n",
      "Average cap: 12.015531539916992\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 5\n",
      "Average cap: 0.26717302203178406\n",
      "Number capped: 5\n",
      "Average cap: 5.086723327636719\n",
      "Number capped: 5\n",
      "Average cap: 7.420710563659668\n",
      "Number capped: 3\n",
      "Average cap: 9.657868385314941\n",
      "Number capped: 4\n",
      "Average cap: 8.608283996582031\n",
      "Number capped: 2\n",
      "Average cap: 0.09311066567897797\n",
      "Number capped: 7\n",
      "Average cap: 4.475454807281494\n",
      "Number capped: 10\n",
      "Average cap: 0.7465776801109314\n",
      "Number capped: 3\n",
      "Average cap: 12.622322082519531\n",
      "Number capped: 5\n",
      "Average cap: 4.44437313079834\n",
      "Number capped: 3\n",
      "Average cap: 0.06242537871003151\n",
      "Number capped: 4\n",
      "Average cap: 0.901625394821167\n",
      "Number capped: 5\n",
      "Average cap: 0.5176652073860168\n",
      "Number capped: 2\n",
      "Average cap: 12.548975944519043\n",
      "Number capped: 3\n",
      "Average cap: 0.3985367715358734\n",
      "Number capped: 2\n",
      "Average cap: 13.14253044128418\n",
      "Number capped: 3\n",
      "Average cap: 0.32485145330429077\n",
      "Number capped: 5\n",
      "Average cap: 9.835405349731445\n",
      "Number capped: 4\n",
      "Average cap: 0.6741763353347778\n",
      "Number capped: 3\n",
      "Average cap: 8.01871395111084\n",
      "Number capped: 3\n",
      "Average cap: 12.19926929473877\n",
      "Number capped: 2\n",
      "Average cap: 11.94694995880127\n",
      "Number capped: 5\n",
      "Average cap: 4.600210666656494\n",
      "Number capped: 4\n",
      "Average cap: 7.168294429779053\n",
      "Number capped: 2\n",
      "Average cap: 11.94782829284668\n",
      "Number capped: 4\n",
      "Average cap: 6.338564395904541\n",
      "Number capped: 6\n",
      "Average cap: 0.38993215560913086\n",
      "Number capped: 3\n",
      "Average cap: 19.802818298339844\n",
      "Number capped: 2\n",
      "Average cap: 0.38069936633110046\n",
      "Number capped: 6\n",
      "Average cap: 4.065113067626953\n",
      "Number capped: 4\n",
      "Average cap: 5.6396331787109375\n",
      "Number capped: 5\n",
      "Average cap: 11.290632247924805\n",
      "Number capped: 3\n",
      "Average cap: 0.00305164884775877\n",
      "Number capped: 5\n",
      "Average cap: 3.461090087890625\n",
      "Number capped: 5\n",
      "Average cap: 4.913842678070068\n",
      "Number capped: 6\n",
      "Average cap: 0.0023382960353046656\n",
      "Number capped: 2\n",
      "Average cap: 17.33539390563965\n",
      "Number capped: 3\n",
      "Average cap: 7.3966546058654785\n",
      "Number capped: 2\n",
      "Average cap: 0.39630746841430664\n",
      "Number capped: 1\n",
      "Average cap: 36.46820831298828\n",
      "Number capped: 5\n",
      "Average cap: 0.015856537967920303\n",
      "Number capped: 9\n",
      "Average cap: 2.84061598777771\n",
      "Number capped: 2\n",
      "Average cap: 1.8282928466796875\n",
      "Number capped: 3\n",
      "Average cap: 8.30020523071289\n",
      "Number capped: 9\n",
      "Average cap: 2.7157678604125977\n",
      "Number capped: 3\n",
      "Average cap: 9.171327590942383\n",
      "Number capped: 4\n",
      "Average cap: 2.688326597213745\n",
      "Number capped: 2\n",
      "Average cap: 0.3947894871234894\n",
      "Number capped: 4\n",
      "Average cap: 14.892762184143066\n",
      "Number capped: 3\n",
      "Average cap: 9.118117332458496\n",
      "Number capped: 4\n",
      "Average cap: 5.756586074829102\n",
      "Number capped: 3\n",
      "Average cap: 0.4088559150695801\n",
      "Number capped: 3\n",
      "Average cap: 8.00412654876709\n",
      "Number capped: 5\n",
      "Average cap: 11.114752769470215\n",
      "Number capped: 5\n",
      "Average cap: 4.879287242889404\n",
      "Number capped: 3\n",
      "Average cap: 16.04891014099121\n",
      "Number capped: 3\n",
      "Average cap: 1.2769304513931274\n",
      "Number capped: 6\n",
      "Average cap: 9.739428520202637\n",
      "Number capped: 8\n",
      "Average cap: 11.48150634765625\n",
      "Number capped: 3\n",
      "Average cap: 10.389771461486816\n",
      "Number capped: 6\n",
      "Average cap: 4.649608612060547\n",
      "Number capped: 2\n",
      "Average cap: 0.011071627959609032\n",
      "Number capped: 2\n",
      "Average cap: 23.11154556274414\n",
      "Number capped: 3\n",
      "Average cap: 17.336124420166016\n",
      "Number capped: 7\n",
      "Average cap: 3.9820523262023926\n",
      "Number capped: 6\n",
      "Average cap: 3.701528310775757\n",
      "Number capped: 7\n",
      "Average cap: 5.988342761993408\n",
      "Number capped: 4\n",
      "Average cap: 5.71582555770874\n",
      "Number capped: 7\n",
      "Average cap: 3.671300172805786\n",
      "Number capped: 8\n",
      "Average cap: 3.312452793121338\n",
      "Number capped: 1\n",
      "Average cap: 0.4188552498817444\n",
      "Number capped: 3\n",
      "Average cap: 7.896173000335693\n",
      "Number capped: 5\n",
      "Average cap: 9.49342155456543\n",
      "Number capped: 5\n",
      "Average cap: 1.54667329788208\n",
      "Number capped: 4\n",
      "Average cap: 0.4234088361263275\n",
      "Number capped: 4\n",
      "Average cap: 15.079296112060547\n",
      "Number capped: 5\n",
      "Average cap: 10.041708946228027\n",
      "Number capped: 7\n",
      "Average cap: 0.38958337903022766\n",
      "Number capped: 2\n",
      "Average cap: 0.011872462928295135\n",
      "Number capped: 12\n",
      "Average cap: 2.6026363372802734\n",
      "Number capped: 1\n",
      "Average cap: 25.09598159790039\n",
      "Number capped: 5\n",
      "Average cap: 4.5853776931762695\n",
      "Number capped: 8\n",
      "Average cap: 3.464967727661133\n",
      "Number capped: 6\n",
      "Average cap: 9.375934600830078\n",
      "Number capped: 3\n",
      "Average cap: 1.9342275857925415\n",
      "Number capped: 3\n",
      "Average cap: 10.980698585510254\n",
      "Number capped: 5\n",
      "Average cap: 4.193991661071777\n",
      "Number capped: 4\n",
      "Average cap: 12.601033210754395\n",
      "Number capped: 4\n",
      "Average cap: 0.8441581726074219\n",
      "Number capped: 2\n",
      "Average cap: 17.29679298400879\n",
      "Number capped: 4\n",
      "Average cap: 3.8084933757781982\n",
      "Number capped: 2\n",
      "Average cap: 1.0481224060058594\n",
      "Number capped: 3\n",
      "Average cap: 0.48784908652305603\n",
      "Number capped: 7\n",
      "Average cap: 3.375185012817383\n",
      "Number capped: 1\n",
      "Average cap: 25.701705932617188\n",
      "Number capped: 6\n",
      "Average cap: 3.4338629245758057\n",
      "Number capped: 6\n",
      "Average cap: 4.311258316040039\n",
      "Number capped: 6\n",
      "Average cap: 0.24391727149486542\n",
      "Number capped: 4\n",
      "Average cap: 7.444347381591797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number capped: 4\n",
      "Average cap: 6.177268028259277\n",
      "Number capped: 3\n",
      "Average cap: 8.7367525100708\n",
      "Number capped: 1\n",
      "Average cap: 0.342437207698822\n",
      "Number capped: 3\n",
      "Average cap: 9.719437599182129\n",
      "Number capped: 3\n",
      "Average cap: 7.820082187652588\n",
      "Number capped: 4\n",
      "Average cap: 5.902248382568359\n",
      "Number capped: 4\n",
      "Average cap: 7.122652053833008\n",
      "Number capped: 4\n",
      "Average cap: 8.553239822387695\n",
      "Number capped: 4\n",
      "Average cap: 6.653624534606934\n",
      "Number capped: 3\n",
      "Average cap: 0.674393892288208\n",
      "Number capped: 2\n",
      "Average cap: 45.50435256958008\n",
      "Number capped: 4\n",
      "Average cap: 2.098937749862671\n",
      "Number capped: 4\n",
      "Average cap: 5.859213829040527\n",
      "Number capped: 5\n",
      "Average cap: 5.157308578491211\n",
      "Number capped: 6\n",
      "Average cap: 0.4002164900302887\n",
      "Number capped: 0\n",
      "Average cap: inf\n",
      "Number capped: 5\n",
      "Average cap: 8.885470390319824\n",
      "Number capped: 5\n",
      "Average cap: 0.45205020904541016\n",
      "Number capped: 3\n",
      "Average cap: 10.166467666625977\n",
      "Number capped: 5\n",
      "Average cap: 6.36267614364624\n",
      "Number capped: 2\n",
      "Average cap: 0.37800148129463196\n",
      "Number capped: 7\n",
      "Average cap: 1.099233865737915\n",
      "Number capped: 8\n",
      "Average cap: 0.4781964123249054\n",
      "Number capped: 5\n",
      "Average cap: 9.298280929215252e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprint_capped\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_cap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_cap\n\u001b[0;32m---> 34\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sigmoid_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     36\u001b[0m     _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:94\u001b[0m, in \u001b[0;36mtrain_sigmoid_with_embeddings\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat(), smote_target, embeds\u001b[38;5;241m=\u001b[39membeds)\n\u001b[1;32m     93\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 94\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cosine distance + capped loss\n",
    "momentum=0\n",
    "learning_rates = [1e-3, 5e-4, 5e-3]\n",
    "\n",
    "cap_aucs = []\n",
    "    \n",
    "loss_fn_args = {}\n",
    "loss_caps = [0.5]\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps: \n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(100):\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['print_capped'] = False\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['print_capped'] = True\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, 'start_epoch=2, num_models=100']\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab098399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17af0e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.004998538017272949, AUC: 0.5778245\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m model_aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     _, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_losses))))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:128\u001b[0m, in \u001b[0;36mtrain_triplet_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, loss_fn_args)\u001b[0m\n\u001b[1;32m    126\u001b[0m neg_embeds \u001b[38;5;241m=\u001b[39m network(neg_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    127\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(anchor_embeds, pos_embeds, neg_embeds)\n\u001b[0;32m--> 128\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m LOG_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss no ratio \n",
    "# no smote \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(5e-6, 1e-3), (1e-6, 5e-4), (1e-6, 1e-4), (5e-6, 5e-4)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "    \n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(n_epochs):\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "    \n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, (1, 1), learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], 0.0, norm]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8afa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f34cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0007769797742366791, AUC: 0.581019\n",
      "\n",
      "Train loss: 16.096790712090986\n",
      "Train loss: 7.95101783849016\n",
      "Train loss: 1.6742434924161886\n",
      "Train loss: 0.0\n",
      "Train loss: 6.369154097158698\n",
      "Train loss: 1.0685905746266813\n",
      "Train loss: 0.9919730077815961\n",
      "Train loss: 4.3510085480122624\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 1.729099128819719\n",
      "Train loss: 0.0\n",
      "Train loss: 2.9074361294130737\n",
      "Train loss: 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0007081505060195923, AUC: 0.595766\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918745040893554, AUC: 0.599685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006822091042995453, AUC: 0.606827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006758022308349609, AUC: 0.614831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006717154681682586, AUC: 0.620797\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006670982539653778, AUC: 0.6299465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006633270978927612, AUC: 0.641555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006615970730781555, AUC: 0.647514\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006594435572624207, AUC: 0.6563729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006575994789600372, AUC: 0.666123\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006559013724327088, AUC: 0.6735810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006542154252529145, AUC: 0.6845415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006518358290195465, AUC: 0.698683\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006498202979564666, AUC: 0.7103269999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006485094726085662, AUC: 0.717825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006470210254192352, AUC: 0.7272419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000646136075258255, AUC: 0.7334495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000645109623670578, AUC: 0.7394789999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006443145573139191, AUC: 0.7454485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000643904983997345, AUC: 0.746941\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006429647505283355, AUC: 0.7538564999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000642709881067276, AUC: 0.7577389999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006426226794719696, AUC: 0.760882\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006425316333770752, AUC: 0.764992\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006421145796775818, AUC: 0.768756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006414653062820435, AUC: 0.773288\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006413009464740754, AUC: 0.774419\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006411257684230805, AUC: 0.7756689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000640982300043106, AUC: 0.7791325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006405025720596314, AUC: 0.780958\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006400455534458161, AUC: 0.7839640000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398386657238006, AUC: 0.7868439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006397152841091156, AUC: 0.7885960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006394894123077393, AUC: 0.791209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006395555436611175, AUC: 0.7915060000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006393527686595916, AUC: 0.7925110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006391496956348419, AUC: 0.794049\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006389279961585999, AUC: 0.795107\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006387557983398437, AUC: 0.7956540000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000638495922088623, AUC: 0.79725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006383010745048523, AUC: 0.797925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006380480825901032, AUC: 0.7989930000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006377894580364227, AUC: 0.8008280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006376721858978271, AUC: 0.8018989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006373582482337952, AUC: 0.8024609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006369325518608093, AUC: 0.803453\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367611885070801, AUC: 0.8036590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006366349756717682, AUC: 0.8035285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367577910423278, AUC: 0.8037650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006367309987545014, AUC: 0.8036895000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0023003580570220947, AUC: 0.5429729999999999\n",
      "\n",
      "Train loss: 136.23961547658413\n",
      "Train loss: 17.986870584608635\n",
      "Train loss: 15.341035212142557\n",
      "Train loss: 4.752364243133159\n",
      "Train loss: 4.356625907028778\n",
      "Train loss: 0.7004882715925386\n",
      "Train loss: 1.1259002685546875\n",
      "Train loss: 6.454442579534989\n",
      "Train loss: 4.747905163825313\n",
      "Train loss: 0.3334539147871959\n",
      "Train loss: 3.8129760645612887\n",
      "Train loss: 0.0\n",
      "Train loss: 3.7372857588755934\n",
      "Train loss: 3.130930067617682\n",
      "Train loss: 3.3125161279605915\n",
      "\n",
      "Test set: Avg. loss: 0.0007151560485363006, AUC: 0.45533100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007060944736003875, AUC: 0.4688575000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007020347714424133, AUC: 0.4808285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007004459500312805, AUC: 0.4868415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991749107837677, AUC: 0.49186949999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006983735859394073, AUC: 0.49412800000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977417469024659, AUC: 0.5011685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969517469406128, AUC: 0.511384\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006968002021312714, AUC: 0.5048075000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965127289295197, AUC: 0.5116275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006962951421737671, AUC: 0.514332\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006961811780929565, AUC: 0.5129704999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006960669755935669, AUC: 0.505204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695921242237091, AUC: 0.5012044999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958386301994324, AUC: 0.4976050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006957364678382874, AUC: 0.49709449999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006956190168857574, AUC: 0.500152\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006955111622810364, AUC: 0.493364\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006954317688941955, AUC: 0.48901300000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006953255832195282, AUC: 0.48931600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006952438652515411, AUC: 0.4857020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951936483383179, AUC: 0.48956999999999984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951353847980499, AUC: 0.495209\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000695084810256958, AUC: 0.49530450000000015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950275301933289, AUC: 0.49494200000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949717700481415, AUC: 0.49411399999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006949259042739868, AUC: 0.49331900000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948856711387634, AUC: 0.493639\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948323547840119, AUC: 0.49492200000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947891116142273, AUC: 0.498717\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069475057721138, AUC: 0.503362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006947134435176849, AUC: 0.505495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946909725666046, AUC: 0.508237\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946579813957214, AUC: 0.504954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946228444576263, AUC: 0.5067439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945843696594238, AUC: 0.5100250000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006945613324642182, AUC: 0.506382\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694526344537735, AUC: 0.5097929999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944917440414429, AUC: 0.507986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944626867771149, AUC: 0.506527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944490969181061, AUC: 0.506454\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000694442331790924, AUC: 0.5044559999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944300532341003, AUC: 0.502201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006944151818752288, AUC: 0.5003179999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943970620632171, AUC: 0.49821999999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943894028663635, AUC: 0.4986275\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943775415420532, AUC: 0.49697849999999993\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943643987178803, AUC: 0.500112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943606734275818, AUC: 0.50076\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006943549513816834, AUC: 0.49980600000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001080885648727417, AUC: 0.41414299999999993\n",
      "\n",
      "Train loss: 24.389437904840783\n",
      "Train loss: 0.8292029658450356\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 1.7853070995475673\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 0.0\n",
      "Train loss: 3.6756908561609967\n",
      "Train loss: 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0007218130528926849, AUC: 0.518904\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006989753544330597, AUC: 0.540902\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946681439876556, AUC: 0.5388165\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931599974632263, AUC: 0.5372144999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006918653249740601, AUC: 0.5308125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006907552778720856, AUC: 0.5248035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006905039250850678, AUC: 0.5236075\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006901566684246063, AUC: 0.5230110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899235546588897, AUC: 0.5261325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006896623373031616, AUC: 0.5253125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894396543502808, AUC: 0.5309490000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897644698619842, AUC: 0.52842\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899001598358154, AUC: 0.525083\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006898137331008911, AUC: 0.5242764999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006899007260799408, AUC: 0.5234755\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006897071897983551, AUC: 0.5284414999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894522905349732, AUC: 0.5316019999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006894703507423401, AUC: 0.5321385000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006892255246639252, AUC: 0.5359525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006892721951007843, AUC: 0.5328990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891983449459076, AUC: 0.537638\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006890941560268402, AUC: 0.5369265000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006891008913516998, AUC: 0.53901\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893084347248077, AUC: 0.5359465\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893291175365448, AUC: 0.5363020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893750429153442, AUC: 0.535502\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000689245194196701, AUC: 0.536058\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888734996318817, AUC: 0.5422319999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006889899671077729, AUC: 0.538168\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006888663470745087, AUC: 0.5418495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006884230673313141, AUC: 0.5464029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006882088780403137, AUC: 0.550612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006883909702301025, AUC: 0.546473\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006878928244113922, AUC: 0.5528475000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881430745124817, AUC: 0.5479885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000688222199678421, AUC: 0.546259\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006877389848232269, AUC: 0.5549315\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872588694095611, AUC: 0.5582575\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870459020137787, AUC: 0.5611119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006872363984584809, AUC: 0.5597035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870978772640228, AUC: 0.5620409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006871461272239685, AUC: 0.561431\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870918273925781, AUC: 0.561923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006871341466903687, AUC: 0.5619075\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000687317818403244, AUC: 0.551667\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006870152950286865, AUC: 0.5539374999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868258118629455, AUC: 0.5586725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006866075992584228, AUC: 0.560983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006869733929634094, AUC: 0.5588695000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006868163049221039, AUC: 0.5607644999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 class triplet loss with ratio \n",
    "# no smote \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(1e-7, 1e-5)]\n",
    "\n",
    "learning_rate_aucs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(3): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(15):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_ratio, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(50):\n",
    "            _, _ = train.train_linear_probe(epoch, train_loader_reduced, complete_network, linear_optimizer, verbose=False)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], \n",
    "           auc_mean[i][4], auc_variance[i][4],\n",
    "           auc_mean[i][5], auc_variance[i][5],\n",
    "           None]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10048fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "14 columns passed, passed data had 18 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:934\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:981\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 14 columns passed, passed data had 18 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1, df2])\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/convnet_aucs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:781\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 781\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    790\u001b[0m         arrays,\n\u001b[1;32m    791\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:498\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 498\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:937\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    940\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 14 columns passed, passed data had 18 columns"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de718f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041af005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006921598315238952, AUC: 0.5632135\n",
      "\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.9810e-01, 1.5458e+00, 1.6848e+01, 1.5036e+01, 1.0000e-05,\n",
      "        2.2834e-01, 6.2688e+00, 1.0000e-05, 3.2163e+00, 1.0000e-05, 7.8707e+00,\n",
      "        7.7019e+00, 6.4259e+00, 6.5882e+00, 2.3989e+01, 2.4655e-01, 5.3974e+01,\n",
      "        1.5999e+01, 1.1811e+00, 2.1506e+01, 2.7749e+00, 1.0000e-05, 3.0049e+00,\n",
      "        1.0000e-05, 3.4720e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.6800e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.4092e+02, 1.0000e-05, 1.6968e+01, 2.8015e+00, 7.9793e-01, 2.9657e+00,\n",
      "        6.2538e-01, 1.0000e-05, 2.3499e+01, 2.4282e+01, 6.7770e+01, 1.0000e-05,\n",
      "        9.8537e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4370e+01, 5.5514e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0707e+01, 1.0000e-05, 6.6199e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.9218e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 6.9518e+00, 3.0094e+01, 1.0000e-05, 5.7642e+00, 9.5099e+00,\n",
      "        1.0000e-05, 1.1625e+01, 1.0000e-05, 4.3892e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.0294e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.2101e+01, 1.0000e-05,\n",
      "        2.2890e+02, 1.3084e+01, 5.0670e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3719e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.3049e+00, 1.0000e-05, 1.6318e+00, 1.0000e-05,\n",
      "        1.0000e-05, 2.8902e+01, 2.0815e+01, 1.0000e-05, 1.2433e+01, 2.4487e+01,\n",
      "        1.0000e-05, 5.8444e+00, 1.0000e-05, 1.0000e-05, 3.8971e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.4348e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.2456e+02, 1.0000e-05, 1.0000e-05, 3.3153e+00,\n",
      "        1.0000e-05, 1.0052e+00, 1.0000e-05, 6.7949e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 6.9926e+00, 1.1623e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.2753e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0262e+02,\n",
      "        1.0000e-05, 2.2293e+00, 1.0146e+01, 1.0000e-05, 1.0422e+01, 7.3951e+00,\n",
      "        5.5069e+01, 9.1311e-01, 1.0000e-05, 4.7311e+00, 3.4377e+01, 1.0000e-05,\n",
      "        4.7039e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.8967e+00, 1.0000e-05,\n",
      "        7.1934e+00, 2.1237e+01, 4.1615e+00, 5.0780e+00, 1.8998e+02, 1.6187e+01,\n",
      "        1.4891e+00, 2.3622e+00, 1.0000e-05, 2.5665e+01, 5.3024e+00, 1.0000e-05,\n",
      "        1.0000e-05, 4.7473e+00, 1.7368e+01, 1.0000e-05, 1.0000e-05, 7.8439e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.8155e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0756e+01, 1.0000e-05, 7.6361e+01, 1.5848e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.6837e+01, 5.0740e+00, 3.6043e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.4349e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.4107e+01, 7.0109e-01, 3.7495e+00, 1.0000e-05,\n",
      "        4.9313e+00, 1.0000e-05, 1.0000e-05, 1.9820e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5366e+02, 1.0000e-05, 7.2653e+00,\n",
      "        1.0000e-05, 2.2752e+01, 1.0000e-05, 9.8527e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0255e+01, 7.1129e+01, 1.0000e-05, 9.6010e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.4549e-01, 1.0876e+01, 1.0000e-05, 6.5752e+00,\n",
      "        5.9330e+00, 1.0000e-05, 1.0000e-05, 6.8576e+00, 5.2095e+00, 1.0000e-05,\n",
      "        5.8625e+00, 1.0000e-05, 1.0000e-05, 2.8267e+00, 1.0000e-05, 1.0000e-05,\n",
      "        7.1087e+01, 1.3382e+01, 1.0000e-05, 9.3467e-01, 6.1170e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0623e+01, 1.6181e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.1087e+00, 1.0000e-05, 5.4458e+00, 1.0000e-05, 1.0000e-05, 3.1749e+00,\n",
      "        2.6246e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7857e+01, 1.1586e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5744e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0465e+01, 4.8086e+00, 1.5864e+01, 1.0000e-05, 1.0993e+01, 4.2261e+00,\n",
      "        4.7900e+01, 6.3077e+00, 3.4670e+01, 1.0000e-05, 6.5799e+00, 8.2433e+01,\n",
      "        2.5006e+01, 1.0000e-05, 4.0842e+00, 9.5198e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0276e+02, 6.4110e+00, 1.0085e+01, 4.9007e+01, 2.1078e+00,\n",
      "        1.0000e-05, 1.0000e-05, 5.4304e+00, 5.3343e+00, 1.0000e-05, 1.5682e+02,\n",
      "        1.0000e-05, 8.9187e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.3255e-01,\n",
      "        1.0000e-05, 1.0000e-05, 4.9186e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0443e+01, 1.2624e+01, 4.6419e+00, 2.7176e+01, 1.0000e-05, 3.7460e-02,\n",
      "        1.0000e-05, 1.9946e+01, 1.0000e-05, 8.8073e+00, 1.0000e-05, 1.3992e+01,\n",
      "        2.5920e+00, 1.0000e-05, 1.5083e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.3322e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.0626e+01, 1.5855e+01, 1.0000e-05, 7.4757e+00, 9.5035e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.6872e+01, 5.8571e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.3471e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.9030e+01, 6.8566e+00, 1.0000e-05, 1.0000e-05, 1.3019e+01, 5.7508e+00,\n",
      "        1.0000e-05, 5.8177e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.6868e+01, 1.0000e-05, 4.7451e+01, 1.0000e-05, 5.2172e+00, 1.0000e-05,\n",
      "        1.0000e-05, 5.3640e+00, 1.4985e+01, 7.5196e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 8.8561e+00, 1.0000e-05, 1.0000e-05, 6.2071e+00, 1.0000e-05,\n",
      "        3.4340e+00, 7.4413e+01, 7.1693e+00, 1.0000e-05, 1.0636e+01, 2.8862e+00,\n",
      "        1.0000e-05, 6.3711e+01, 1.2243e+00, 4.9497e+00, 4.3174e+01, 3.3592e+01,\n",
      "        1.0000e-05, 1.0000e-05, 7.0770e+00, 2.0247e-01, 1.0000e-05, 1.0000e-05,\n",
      "        4.0341e+00, 6.8840e-01, 1.2026e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.1483e+01, 3.2623e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.9097e-01, 1.0000e-05, 8.2275e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.9753e+01, 1.5029e+01, 4.9901e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.8805e+01, 1.0000e-05, 1.9548e+02, 1.0000e-05,\n",
      "        1.0000e-05, 7.7305e+00, 1.1009e+01, 1.0000e-05, 3.6598e+01, 1.2795e+00,\n",
      "        2.6355e+01, 1.0000e-05, 8.3502e+01, 2.2991e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 2.3878e+00, 3.8752e+01, 2.7550e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.1330e+02, 1.0000e-05, 5.5368e+01, 2.5665e+01,\n",
      "        1.0000e-05, 2.5782e+01, 1.8247e+01, 1.0000e-05, 2.6834e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.3359e+01, 1.0000e-05,\n",
      "        1.0000e-05, 9.1637e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.1942e+00,\n",
      "        1.0000e-05, 1.0000e-05, 9.4043e+00, 3.0023e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.8078e+01, 8.9835e+00, 3.9245e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.9568e+01, 1.5163e+00, 1.8816e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.3260e+00, 2.8724e+01, 1.0000e-05, 1.0000e-05, 1.2952e+00, 1.0000e-05,\n",
      "        1.4218e+01, 1.0000e-05, 1.9576e-01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 4.2283e+01, 8.7142e+01, 1.0000e-05, 7.8430e+01, 4.3752e+01,\n",
      "        5.3373e+01, 1.2935e+00, 1.0000e-05, 5.6911e+00, 1.0000e-05, 4.7531e+00,\n",
      "        1.0417e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.2537e+01, 1.4679e+02, 1.0000e-05, 3.6407e+01, 1.0000e-05, 1.6136e+00,\n",
      "        2.4300e+00, 3.4018e+01, 1.0000e-05, 1.5768e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.3531e+01, 3.0294e+01, 1.6073e+01, 7.7718e+01,\n",
      "        1.2242e+00, 2.0890e+01, 1.0000e-05, 1.0000e-05, 5.1065e-01, 1.0000e-05,\n",
      "        1.8007e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.1920e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.1231e+00, 5.5213e-01, 1.0000e-05, 1.0302e+00, 2.9548e+01,\n",
      "        1.0000e-05, 1.6197e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([7.7364e+00, 1.0000e-05, 1.2418e+01, 1.0000e-05, 1.0000e-05, 1.8112e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.7145e+00,\n",
      "        1.0970e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1792e+01, 1.0000e-05,\n",
      "        3.8414e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.7982e+01, 1.0000e-05,\n",
      "        6.6513e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.4453e+01, 2.2629e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0892e+02, 3.0923e+01, 1.0000e-05, 1.7724e+01, 2.2308e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.3566e+01, 1.0000e-05, 1.0000e-05, 6.9074e+00,\n",
      "        1.0000e-05, 1.0000e-05, 8.2342e+00, 1.0080e+00, 6.5051e+01, 1.0000e-05,\n",
      "        4.9991e-01, 1.3792e+01, 1.6586e+01, 4.6195e+01, 1.0000e-05, 2.0631e+01,\n",
      "        1.6207e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([8.1309e+01, 1.0000e-05, 3.2372e+01, 3.4662e+01, 1.0000e-05, 1.4970e+01,\n",
      "        9.5361e+01, 6.7757e+01, 8.4828e+01, 1.0000e-05, 4.7372e+01, 1.0000e-05,\n",
      "        6.9025e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.0591e+01, 4.6644e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.9640e+01, 1.1249e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.6429e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.3334e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 8.5215e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.8436e+01,\n",
      "        3.9171e+00, 1.1749e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.1211e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.2269e+00, 1.8661e+00,\n",
      "        1.0000e-05, 1.8646e+02, 2.8457e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 3.7766e-02, 1.0000e-05, 1.1705e+01, 1.0000e-05, 1.0187e+01,\n",
      "        5.5376e-01, 5.9577e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7675e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.4404e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5160e+02, 3.1658e+01,\n",
      "        1.0000e-05, 1.0000e-05, 4.1032e+01, 1.0000e-05, 2.8029e+00, 6.8477e+00,\n",
      "        1.0000e-05, 1.0000e-05, 4.3370e+00, 1.0000e-05, 1.0000e-05, 3.6807e+01,\n",
      "        1.0000e-05, 1.0000e-05, 3.0270e+00, 1.9909e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.7248e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.6774e+00, 1.0000e-05, 1.0000e-05, 1.9412e+00,\n",
      "        2.5522e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.9173e+01, 6.6594e+00, 9.4131e+01, 1.0000e-05, 1.4803e+00, 7.5380e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.3699e+01, 1.0000e-05,\n",
      "        2.9392e+01, 1.0000e-05, 1.0105e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 8.4523e+00, 1.0000e-05, 1.0000e-05, 1.0951e+01,\n",
      "        5.2958e-01, 1.0000e-05, 5.5537e+01, 1.0000e-05, 1.2025e+01, 1.0000e-05,\n",
      "        1.8957e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.1951e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.3023e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.3821e+01, 1.0000e-05, 1.4241e+02, 1.0000e-05, 4.9865e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.0578e-01, 1.9553e+01, 9.4585e+00, 3.1198e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0079e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.6851e+01, 1.0000e-05, 5.6169e+00, 1.5107e+01, 1.0000e-05, 6.9918e+01,\n",
      "        1.0000e-05, 1.0000e-05, 9.7441e+00, 1.0000e-05, 1.0000e-05, 4.4223e+01,\n",
      "        9.2288e+00, 1.0000e-05, 2.3024e+00, 1.0000e-05, 4.1206e-01, 6.4005e+00,\n",
      "        1.0000e-05, 3.1871e+00, 1.0000e-05, 1.4783e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0147e+02, 1.2729e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.9493e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.8282e+01, 1.0000e-05, 2.7940e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.1241e+01, 2.8382e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.3514e+02, 4.5616e+01, 1.0000e-05, 4.8528e+01, 8.7755e+00, 1.0000e-05,\n",
      "        1.0000e-05, 3.2391e+01, 1.3230e+01, 1.0000e-05, 1.8183e+01, 1.0000e-05,\n",
      "        9.5003e+01, 1.0000e-05, 1.0000e-05, 2.4446e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.8008e+00, 2.5404e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.8465e-01, 3.0662e+00, 2.2096e+01, 1.0000e-05, 1.0000e-05, 1.2404e+01,\n",
      "        1.8444e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2265e+02, 1.0000e-05, 1.0000e-05, 2.5461e+00, 1.0000e-05,\n",
      "        1.0000e-05, 8.5055e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5874e+01,\n",
      "        3.3964e+00, 1.3022e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.0104e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2812e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.3550e+00, 1.1149e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.3821e+02, 1.0000e-05, 1.1848e+01, 9.6875e-01, 2.3702e+02, 1.0000e-05,\n",
      "        1.0000e-05, 8.4552e+01, 6.1088e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3476e+01, 2.3130e-01, 1.0000e-05,\n",
      "        1.0000e-05, 2.4396e+00, 1.0000e-05, 1.0000e-05, 3.5712e+00, 1.0000e-05,\n",
      "        9.8791e+00, 1.8217e+01, 2.3053e+00, 5.9570e+00, 1.0000e-05, 1.6211e+01,\n",
      "        1.0000e-05, 4.9341e+01, 2.3625e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.3838e+00, 6.9605e+00, 4.6373e+00,\n",
      "        1.5172e+01, 1.1170e+00, 1.0000e-05, 2.1594e+00, 1.0000e-05, 9.0309e+01,\n",
      "        9.0680e+00, 1.0000e-05, 1.1454e+02, 4.5772e+01, 1.0000e-05, 2.9116e+00,\n",
      "        1.0000e-05, 1.3469e+01, 7.7918e+00, 1.0000e-05, 3.9228e+00, 1.5343e+01,\n",
      "        1.0000e-05, 7.0728e+01, 1.0000e-05, 1.0000e-05, 1.1305e+02, 1.0000e-05,\n",
      "        7.5581e+01, 1.0000e-05, 1.6492e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 9.8855e+00, 5.2022e+01, 1.0000e-05,\n",
      "        1.1279e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8573e+00, 5.2798e+00,\n",
      "        2.0296e+01, 2.1372e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.7639e+01, 1.0000e-05, 1.0000e-05, 3.1774e+00, 1.0000e-05, 7.0213e-01,\n",
      "        9.4730e+01, 3.8892e+01, 1.5355e+02, 1.2502e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.3866e+01, 7.2368e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.6473e+01, 8.3044e+00, 1.0000e-05, 5.4066e-01, 1.0000e-05,\n",
      "        1.5924e+01, 1.5677e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 8.6051e+00, 1.1302e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2326e-01, 1.4380e+01, 6.5676e+01, 4.2903e+01, 1.0000e-05,\n",
      "        5.7951e+01, 1.1224e+02, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.2218e+01, 1.0000e-05, 2.2713e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.8015e+00, 7.6721e+01, 1.0000e-05, 4.7474e-01, 2.5471e+01, 1.0000e-05,\n",
      "        1.0000e-05, 5.1727e+01, 4.7674e+01, 5.9556e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.2339e+01, 1.5450e+00, 1.0000e-05, 1.2556e+00, 1.0000e-05, 1.0000e-05,\n",
      "        3.0957e+01, 1.0000e-05, 7.5707e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.4469e+01, 1.0000e-05, 3.6208e+01,\n",
      "        1.0000e-05, 1.9357e+00, 1.0776e+02, 1.5979e+01, 1.0000e-05, 8.4559e-02,\n",
      "        1.6120e+00, 1.0000e-05, 1.0000e-05, 1.1450e+01, 1.8154e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.1623e+00, 7.2115e-01, 1.0000e-05, 2.8012e+00,\n",
      "        1.0000e-05, 1.0000e-05, 6.6464e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.1771e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.5479e+02, 3.8645e+00, 1.0000e-05, 1.8808e+00, 3.8827e+01, 9.1075e+00,\n",
      "        6.5015e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        7.1507e-01, 1.0000e-05, 1.0000e-05, 6.2372e+00, 1.5635e+02, 6.3498e+00,\n",
      "        1.0000e-05, 1.0000e-05, 7.4888e+00, 1.0000e-05, 1.0000e-05, 1.5275e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.4317e+01, 1.0000e-05,\n",
      "        4.1032e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.4138e+00, 1.4055e+02, 1.0000e-05, 1.0000e-05, 3.8641e+00, 1.0000e-05,\n",
      "        5.8806e+00, 2.1614e+01, 1.0000e-05, 2.6149e+01, 1.0000e-05, 5.1136e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.0416e+01, 5.3407e+00, 1.0000e-05, 1.0000e-05, 2.4083e+00, 9.4902e+01,\n",
      "        1.0000e-05, 8.3517e+00, 7.7587e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 3.0978e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.6673e-01, 2.4118e+01, 1.2338e+00, 1.0000e-05, 4.0576e+00, 2.2935e+00,\n",
      "        1.0000e-05, 3.7732e+01, 1.0000e-05, 3.3884e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.5920e+01, 1.7724e+02, 7.8903e+00, 1.0000e-05,\n",
      "        1.0000e-05, 5.2995e+00, 1.0000e-05, 7.5137e+00, 1.0000e-05, 9.2116e+00,\n",
      "        7.2271e+01, 1.0000e-05, 4.2864e+00, 3.4042e+01, 1.0000e-05, 1.0000e-05,\n",
      "        9.2707e+00, 1.6012e+00, 7.3752e+00, 1.0000e-05, 1.0000e-05, 1.0698e+02,\n",
      "        2.6291e+00, 1.2479e+01, 1.0000e-05, 1.0000e-05, 8.6917e+01, 1.5292e+01,\n",
      "        3.7635e+00, 1.0000e-05, 3.3983e+01, 2.9514e+00, 3.2471e-01, 1.0000e-05,\n",
      "        2.0967e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.1894e+00, 1.4377e+01,\n",
      "        1.0000e-05, 1.0000e-05, 6.3176e+01, 2.7025e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 9.8061e+01, 1.6961e+01, 1.0000e-05, 6.5753e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.9812e+01,\n",
      "        1.0000e-05, 5.0008e+00, 1.0000e-05, 1.0000e-05, 2.6709e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0051e+01,\n",
      "        6.6557e+00, 4.1239e-01, 1.0000e-05, 1.0000e-05, 2.8866e+00, 1.0000e-05,\n",
      "        1.3786e+01, 1.0000e-05, 1.0000e-05, 9.0496e+01, 7.8513e-01, 5.3633e+00,\n",
      "        5.1730e+01, 1.0000e-05, 1.0000e-05, 3.3571e+00, 1.0000e-05, 3.7679e+00,\n",
      "        3.3632e+01, 6.6759e+00, 1.0000e-05, 7.9594e+00, 3.8840e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.4275e+01, 1.0000e-05, 2.2651e+00, 1.0000e-05,\n",
      "        1.0000e-05, 4.1026e+00, 3.8503e+00, 2.3494e+01, 3.4742e+00, 1.3511e+01,\n",
      "        1.2760e+02, 1.0000e-05, 4.4258e+01, 2.8187e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([6.2957e+01, 1.0000e-05, 1.0000e-05, 5.2106e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5816e+01, 8.4301e+00,\n",
      "        7.3015e+01, 1.0000e-05, 1.6642e+01, 1.0000e-05, 1.0000e-05, 4.3282e+01,\n",
      "        1.4071e+01, 1.1287e+01, 2.9678e+01, 1.0000e-05, 3.5300e+01, 1.0000e-05,\n",
      "        4.2568e+00, 4.8654e+00, 1.0309e+02, 1.4843e+01, 1.0000e-05, 1.0000e-05,\n",
      "        3.7864e+00, 1.0000e-05, 1.0000e-05, 8.6994e+00, 4.6624e-01, 5.2929e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1371e+01,\n",
      "        1.1258e+02, 6.6859e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.1986e+01, 2.1362e+00, 1.3331e+02, 1.1181e+00, 3.7818e+00, 1.0000e-05,\n",
      "        1.0000e-05, 6.0909e+00, 1.9902e+00, 1.5457e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1377e+01, 1.2247e+01, 1.0390e+02,\n",
      "        1.0589e+01, 1.0000e-05, 5.9098e+01, 1.3090e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.9019e+00, 1.9122e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9471e+00,\n",
      "        1.0000e-05, 1.3061e+01, 4.7019e+01, 1.0000e-05, 1.1921e+01, 1.0307e+01,\n",
      "        4.7416e+01, 1.0000e-05, 1.3248e+02, 1.4259e+01, 1.0000e-05, 7.3320e+01,\n",
      "        1.0000e-05, 8.4036e+00, 1.0000e-05, 1.0000e-05, 2.1566e+00, 1.0000e-05,\n",
      "        1.0000e-05, 6.7373e+00, 1.0000e-05, 8.1012e+01, 2.5835e+02, 6.6883e+00,\n",
      "        1.0000e-05, 8.8150e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.1682e+01,\n",
      "        1.0000e-05, 2.7209e+00, 1.0000e-05, 2.6681e+00, 1.0000e-05, 3.3152e+00,\n",
      "        6.6523e+00, 1.0000e-05, 1.8901e+00, 4.0442e-01, 1.0000e-05, 1.6376e+01,\n",
      "        7.9589e-01, 1.0000e-05, 1.0000e-05, 2.4439e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.6711e+01, 9.2124e-01, 7.1799e+00, 1.0000e-05, 1.0000e-05, 5.7506e-01,\n",
      "        3.6132e+01, 1.0000e-05, 1.9619e+00, 7.2008e-02, 1.0000e-05, 1.6031e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.2576e+00, 6.0934e-01, 1.0000e-05, 5.8312e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.8318e+00, 3.2435e+01, 1.0000e-05,\n",
      "        3.4102e+00, 1.0000e-05, 8.6397e+01, 1.0000e-05, 1.0000e-05, 2.6979e+00,\n",
      "        3.4519e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.2244e-01, 1.0000e-05, 1.7861e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.6668e+00, 1.4407e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.1372e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.5129e+01, 6.1079e+00, 1.4629e+01, 4.1239e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.1474e+02, 5.1657e+00, 9.2442e-01, 1.0000e-05,\n",
      "        4.3340e+01, 1.0000e-05, 4.1276e+01, 2.0845e+01, 2.9568e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1250e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.1784e+01, 1.0000e-05, 2.8658e-01, 6.9585e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2070e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.7051e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.7088e+01, 1.0000e-05, 3.1572e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.8718e+00, 1.6378e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8020e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3563e+01, 2.8267e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 7.2605e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 3.4865e+00, 9.3616e+00, 5.7839e+01, 9.2410e+01, 1.2310e+01,\n",
      "        1.0000e-05, 1.2250e+01, 1.0000e-05, 1.0000e-05, 6.0484e+01, 6.0068e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.0430e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 9.4233e-01, 1.0000e-05, 3.5242e+01, 9.1333e+00, 2.7092e+01,\n",
      "        1.0000e-05, 1.6819e+01, 6.9925e+00, 1.0000e-05, 1.8063e+00, 1.0000e-05,\n",
      "        9.9874e+00, 1.1899e+01, 1.0000e-05, 4.9687e+00, 1.0000e-05, 1.1468e+00,\n",
      "        1.1449e+00, 1.6319e+00, 1.0000e-05, 1.0000e-05, 3.5147e+01, 2.0487e+00,\n",
      "        1.0000e-05, 1.0029e+01, 1.0000e-05, 4.0977e+01, 1.0000e-05, 1.1249e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.2965e+01, 1.0000e-05, 1.7884e+01, 1.0000e-05,\n",
      "        1.8026e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.9197e+01, 4.3838e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([7.7416e+00, 1.0000e-05, 5.9556e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5338e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.6998e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.6956e+00, 1.0000e-05,\n",
      "        1.7216e+01, 5.3031e+00, 1.0000e-05, 4.1188e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.8362e+00, 2.0065e+00, 1.0000e-05, 1.9809e+01,\n",
      "        1.6520e+01, 2.1289e+02, 1.5454e+00, 3.8262e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.4224e+01, 1.0000e-05, 1.7947e+00, 1.1602e+01, 2.5882e+01, 1.4065e+00,\n",
      "        1.0000e-05, 1.0000e-05, 9.2434e+01, 1.0000e-05, 1.0000e-05, 1.0899e+02,\n",
      "        4.7456e+01, 1.0000e-05, 2.5003e+01, 1.7118e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.9133e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.0758e+01, 1.2539e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 3.3929e+00, 8.3463e+01, 2.5982e+00, 1.0000e-05, 2.0364e+00,\n",
      "        9.5552e+01, 1.7755e+01, 1.0000e-05, 1.0000e-05, 9.9351e+00, 1.0000e-05,\n",
      "        1.0000e-05, 7.3963e+00, 4.7977e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.2498e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 9.2228e+00, 1.0000e-05, 7.1313e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.5731e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6930e+01,\n",
      "        1.7789e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2690e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.1741e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.6144e+00, 1.0000e-05, 1.0631e+02, 1.1992e+00,\n",
      "        1.2650e+01, 1.0000e-05, 1.0000e-05, 1.7991e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3162e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.6819e+01, 1.0000e-05, 5.4481e+00, 3.2104e+01, 2.0138e+01, 1.4644e+01,\n",
      "        1.0000e-05, 5.9650e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0044e+00, 1.0000e-05, 1.0000e-05, 1.0610e+01, 1.0000e-05,\n",
      "        1.0000e-05, 8.1622e+01, 5.5959e+00, 1.0000e-05, 1.0000e-05, 1.0100e+01,\n",
      "        1.0000e-05, 9.8310e-01, 8.4198e-02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.7972e+01,\n",
      "        2.2080e+01, 4.7899e+00, 1.5793e+01, 1.7027e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1216e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.9486e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.9742e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.0153e+00, 1.0000e-05, 6.5344e+01, 1.0000e-05, 2.3507e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 6.1289e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.4195e+01, 1.0000e-05, 2.2639e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0993e+01, 4.1477e+01, 9.1295e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 8.4333e+01, 8.3025e+01, 9.5481e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.4881e+02, 8.0813e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5475e+01,\n",
      "        3.6234e-02, 6.9079e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.4298e+00, 1.0000e-05, 6.1136e+01, 3.6319e+00, 1.0000e-05, 1.2325e+02,\n",
      "        1.5765e-01, 1.0000e-05, 1.4899e+01, 2.4512e+01, 9.6558e+00, 1.0000e-05,\n",
      "        1.0000e-05, 2.5554e+01, 1.0000e-05, 1.0000e-05, 1.9592e+01, 1.0000e-05,\n",
      "        1.0000e-05, 3.9175e+00, 1.0000e-05, 3.4386e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.7050e-01, 1.0000e-05, 2.9681e+01, 8.0593e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.5063e+00, 1.8007e+01, 1.0000e-05, 1.0000e-05, 2.0884e+02,\n",
      "        1.0000e-05, 1.0000e-05, 5.8296e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.4523e+00, 1.0000e-05, 4.1591e+01, 2.0596e+01, 1.7595e+01,\n",
      "        6.4438e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.7658e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7786e+01, 1.6669e+01,\n",
      "        9.7227e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.2186e+00, 5.7458e-02,\n",
      "        3.1998e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6198e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 5.4356e-01, 2.5985e+00, 4.5198e+01,\n",
      "        1.0000e-05, 4.9317e+00, 1.0000e-05, 1.0000e-05, 1.3083e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.8512e+01, 1.3571e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 9.2988e-01, 1.3647e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.8742e+00, 1.7362e+01, 4.5864e+00,\n",
      "        1.7704e+02, 1.0000e-05, 1.0000e-05, 2.8170e+00, 9.7766e+01, 1.0000e-05,\n",
      "        3.0052e+01, 1.3797e+02, 1.0000e-05, 4.5184e+00, 4.1387e+00, 6.5075e+00,\n",
      "        1.0000e-05, 4.6276e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.5462e+00, 1.0000e-05, 1.0000e-05, 7.9652e+01, 1.4598e+00, 1.6164e+01,\n",
      "        1.2825e+00, 3.2113e+01, 4.6428e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0005e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2298e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.6961e-01,\n",
      "        1.1025e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.3476e+01, 2.1310e+01, 1.0000e-05, 1.2216e+01, 1.0000e-05,\n",
      "        2.1944e+01, 6.1457e+00, 7.9575e+00, 1.0000e-05, 1.3725e+01, 1.0000e-05,\n",
      "        8.1142e+00, 1.0000e-05, 1.1451e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.4693e+00, 1.0000e-05, 1.0000e-05, 4.0372e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.5488e+01, 1.0000e-05, 1.0000e-05, 4.1650e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9016e+01, 7.1184e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.7127e+00, 9.4128e+00, 2.6249e+01, 1.0000e-05,\n",
      "        7.8826e+01, 3.0626e+01, 7.8392e+00, 1.0000e-05, 1.2451e+01, 1.0000e-05,\n",
      "        4.0367e+00, 1.0616e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.8239e-01,\n",
      "        3.7133e+00, 8.3462e+00, 1.9987e+01, 1.0000e-05, 1.5906e+02, 1.0000e-05,\n",
      "        1.0000e-05, 6.7304e+00, 4.7341e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 2.1339e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.8162e+00, 1.0000e-05, 6.2975e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.7507e+01, 1.0000e-05, 1.0000e-05, 4.8549e+01, 9.1903e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 9.1431e+00, 2.5405e+00, 2.1313e+01, 5.8958e+01,\n",
      "        1.9566e+01, 1.0000e-05, 5.6774e+00, 1.0000e-05, 2.4215e+01, 1.0000e-05,\n",
      "        1.0000e-05, 5.6047e+00, 9.1312e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.0009e+00, 1.4035e+01, 3.0474e+01, 1.1352e+01, 1.0000e-05, 2.1285e+01,\n",
      "        1.0000e-05, 1.0000e-05, 7.7523e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.8502e+01, 1.5592e+01, 1.4719e+00, 1.0000e-05, 1.1800e+00,\n",
      "        1.0000e-05, 4.5556e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.6234e+00, 1.0000e-05, 4.4631e+00, 1.0304e+01,\n",
      "        1.1019e+01, 1.0000e-05, 3.4382e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.5482e+00, 1.0000e-05, 1.4744e+01, 1.0000e-05,\n",
      "        2.5474e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4344e+01,\n",
      "        1.6016e+01, 1.0000e-05, 3.1913e+00, 2.7638e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.4616e+01, 1.6684e+00, 1.5468e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.4911e+01, 1.0000e-05, 1.0000e-05, 2.1244e+00, 1.0000e-05, 1.2154e+01,\n",
      "        1.0000e-05, 2.7372e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.7702e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        7.5434e+00, 1.0000e-05, 1.0000e-05, 1.3046e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([6.0267e+00, 1.1789e+00, 2.2735e+01, 1.0000e-05, 8.7317e+00, 1.5065e+02,\n",
      "        8.1045e+00, 3.8746e-02, 1.0000e-05, 3.8730e+01, 1.0000e-05, 5.0305e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5687e+01, 1.5017e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7186e+01, 1.0000e-05, 5.2257e+01,\n",
      "        7.2447e+01, 1.0000e-05, 1.3181e+01, 1.0000e-05, 1.3845e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.0243e+00, 2.1775e+01,\n",
      "        5.7295e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.9737e-01,\n",
      "        1.0000e-05, 6.3432e+01, 1.0000e-05, 1.1075e+01, 1.0000e-05, 1.7213e-01,\n",
      "        1.0000e-05, 1.4987e+00, 1.0000e-05, 1.0000e-05, 3.6945e+00, 8.7737e+00,\n",
      "        1.0000e-05, 1.0000e-05, 4.8017e+00, 1.0000e-05, 1.0000e-05, 1.1023e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.3776e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 2.9676e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.0110e+01, 1.7277e+02, 5.5907e+01, 1.7788e+01, 1.8898e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.4147e+00, 5.2925e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.1363e+01, 1.0000e-05, 2.7299e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.3019e+00, 1.0000e-05, 9.2793e+00, 1.0000e-05, 1.1772e+01,\n",
      "        2.9008e+00, 1.7023e+00, 1.0000e-05, 1.0000e-05, 5.6021e+01, 9.7712e-01,\n",
      "        1.0000e-05, 3.1916e+00, 1.0000e-05, 9.2332e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 7.2128e+00, 2.4623e-01, 1.0000e-05,\n",
      "        4.9818e+01, 1.0000e-05, 1.7868e+00, 1.6882e+02, 9.7719e+00, 4.3037e+01,\n",
      "        1.0000e-05, 7.6797e+00, 1.0000e-05, 1.0000e-05, 7.1992e-01, 1.0000e-05,\n",
      "        1.0000e-05, 5.2364e+00, 1.8888e+01, 2.3354e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([3.3579e+00, 1.0000e-05, 2.3958e+01, 1.0000e-05, 2.5659e+01, 7.0254e+00,\n",
      "        4.2309e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.8073e+01,\n",
      "        1.0000e-05, 2.7099e+00, 1.0000e-05, 1.3238e+01, 3.6712e+00, 2.5259e+02,\n",
      "        5.6854e+01, 9.3048e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.3182e+01, 4.1312e-01, 1.0000e-05, 7.1764e+00, 1.0000e-05,\n",
      "        1.5744e+00, 2.1404e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.4075e+01, 4.1680e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.1736e+01, 1.0000e-05, 8.0820e+00, 1.0000e-05, 2.5125e+00, 2.7067e+00,\n",
      "        4.9321e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.8703e+01, 1.0000e-05, 1.0000e-05, 4.8704e+01, 1.0000e-05, 1.0000e-05,\n",
      "        9.2567e+01, 2.5327e+01, 2.1501e+00, 2.5285e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([9.0386e+01, 1.0000e-05, 1.5130e+00, 6.9383e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.3827e+00,\n",
      "        7.4605e+00, 4.7105e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0772e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.4214e+00, 1.0000e-05, 8.3280e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.4157e+00, 3.9830e+01, 9.7460e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.3503e+01, 4.9285e+01, 1.0000e-05, 1.5351e+02, 1.8645e+01,\n",
      "        5.9003e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.9180e+00,\n",
      "        1.0000e-05, 1.2576e+01, 1.0000e-05, 1.0000e-05, 1.6129e+00, 1.6143e+01,\n",
      "        1.0000e-05, 7.9025e+00, 1.0000e-05, 1.5623e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 3.8987e+01, 8.6386e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.4142e-01, 1.0000e-05, 5.6598e+00, 2.9065e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.3248e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.3775e+00, 1.3164e+01, 1.0000e-05, 5.9093e+00,\n",
      "        1.6599e+00, 1.0000e-05, 1.0000e-05, 9.6359e+00, 1.0000e-05, 1.6666e+01,\n",
      "        1.0000e-05, 2.8026e+01, 2.2721e+01, 1.0086e+01, 1.3186e+00, 1.0000e-05,\n",
      "        4.3261e+00, 1.0000e-05, 1.0000e-05, 4.2361e+01, 1.0000e-05, 1.8982e+00,\n",
      "        1.0000e-05, 2.1021e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1062e-01,\n",
      "        1.0000e-05, 1.0000e-05, 3.7359e+00, 5.4188e+01, 6.7253e+00, 3.9305e+01,\n",
      "        1.0000e-05, 6.1822e+00, 1.0000e-05, 1.0000e-05, 4.4523e+01, 2.0500e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4315e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.6083e+00, 1.0000e-05, 4.0296e+00, 1.0000e-05, 7.6211e+01, 1.0000e-05,\n",
      "        1.0000e-05, 8.2336e+00, 1.0000e-05, 1.4066e+01, 5.3542e+00, 5.3407e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7159e+01, 1.9568e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0770e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.0313e+00, 1.0000e-05, 1.0000e-05, 1.2377e+01, 1.0000e-05, 4.8485e+00,\n",
      "        8.5456e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.7505e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.6959e+01, 4.7528e+01, 1.4445e+00, 5.0990e-01, 7.5084e+00, 1.0000e-05,\n",
      "        2.4207e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.1234e+00,\n",
      "        1.0000e-05, 2.6379e+01, 1.0000e-05, 1.0000e-05, 1.8905e+01, 3.5663e+00,\n",
      "        1.0000e-05, 4.3475e+00, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([1.3114e+02, 1.0000e-05, 2.8383e+00, 3.8215e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1780e+01, 1.6594e+02, 2.8124e+01,\n",
      "        3.7852e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.4877e+01, 2.8342e+01, 6.0890e-01, 7.6770e-01, 1.0000e-05, 1.0000e-05,\n",
      "        3.2925e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.6102e+02, 1.0000e-05, 1.0000e-05, 4.4224e+01, 3.4084e+00,\n",
      "        3.1044e+01, 1.0000e-05, 1.0000e-05, 2.4473e+00, 5.1023e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.5014e+01, 1.0000e-05, 5.3993e+00, 2.7245e+01,\n",
      "        1.0000e-05, 2.9755e+00, 1.0000e-05, 5.3099e+00, 1.0000e-05, 4.9329e+00,\n",
      "        1.0000e-05, 9.2819e+01, 1.0000e-05, 1.8710e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.5021e+01, 7.5605e+01, 3.9480e-01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.2226e+00, 2.9219e-02, 1.2783e+02, 1.0472e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.1797e+00,\n",
      "        1.0000e-05, 8.5177e+01, 1.0000e-05, 1.6345e+01, 1.0656e+02, 1.0000e-05,\n",
      "        1.0000e-05, 6.7117e+00, 6.8794e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.1430e+00, 9.3998e-01, 1.0000e-05, 1.3975e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2550e+00, 3.4714e+00, 1.0000e-05,\n",
      "        6.9991e+00, 1.4948e+01, 1.0000e-05, 9.5533e+00, 1.0000e-05, 1.0169e+02,\n",
      "        4.3664e+01, 2.4584e+00, 1.3232e+00, 1.0000e-05, 2.7841e+01, 1.0000e-05,\n",
      "        1.0000e-05, 3.9032e+00, 1.0000e-05, 3.0364e+00, 1.0000e-05, 3.7155e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2162e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.2587e+01, 1.4105e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.8378e+01, 1.2821e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.2946e+01, 1.0000e-05, 2.6453e+01, 1.0000e-05, 1.5563e+01, 1.1103e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9589e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8981e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.1778e+02, 4.4033e+00, 1.0000e-05, 1.0000e-05,\n",
      "        3.3564e+01, 1.9116e+00, 7.5857e+01, 1.0000e-05, 1.4228e+00, 1.9110e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.8731e+01, 1.3050e+02,\n",
      "        4.6516e+01, 5.2503e+00, 6.7943e+00, 1.0000e-05, 1.0000e-05, 6.6513e+00,\n",
      "        1.0000e-05, 1.4107e+01, 3.5710e+00, 1.9661e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.5073e+01, 1.0000e-05, 3.3916e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.3641e+01, 1.0850e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.6719e+01, 5.3608e+00, 1.3257e+01, 7.4363e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4477e+01, 2.5321e+01,\n",
      "        7.0028e-01, 7.9295e+00, 1.0000e-05, 3.8796e+00, 3.2711e+01, 1.4531e+01,\n",
      "        1.0000e-05, 9.8511e+01, 1.5630e-01, 3.1432e+00, 4.7159e+00, 1.0000e-05,\n",
      "        1.0000e-05, 3.4085e+01, 1.0000e-05, 7.5477e+00, 3.3285e+01, 1.7530e+01,\n",
      "        4.3910e+00, 7.5338e+00, 1.2241e+02, 1.0601e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.5209e+00, 3.0056e-02, 5.6763e+01, 7.2267e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0166e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.2330e+01, 1.2646e+01, 6.8422e+00, 1.0000e-05, 1.7486e+01,\n",
      "        3.3900e+01, 1.5550e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5345e+00,\n",
      "        9.9045e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.3128e+01,\n",
      "        1.0000e-05, 3.2738e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        8.8906e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.9175e+01, 2.8968e+00, 5.8128e+01, 5.7767e+00,\n",
      "        5.8412e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.2206e+01, 1.0000e-05,\n",
      "        1.8650e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.9379e+02, 1.3549e+01, 9.2267e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.0219e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 8.5352e+01, 7.9661e+01, 1.0000e-05, 1.0000e-05,\n",
      "        6.0078e+00, 1.4204e+01, 2.0767e+00, 1.4120e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.3411e+01, 1.6071e+02, 3.0937e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        8.0302e+00, 5.8082e+01, 1.0000e-05, 4.1214e+00, 1.0000e-05, 8.1504e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2604e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.7184e+01, 3.7450e+00, 1.0000e-05, 5.8914e+01,\n",
      "        4.5540e+00, 2.1987e+01, 1.0000e-05, 6.7674e+00, 6.2380e+00, 4.1593e+01,\n",
      "        7.6898e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.1226e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.4223e+01, 4.6159e+01, 3.2555e+00, 1.0000e-05,\n",
      "        9.0214e+01, 1.0000e-05, 1.0000e-05, 4.8491e+00, 1.0000e-05, 7.9923e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.7594e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 6.6511e+00, 3.2895e+01, 1.0000e-05, 1.0000e-05, 1.4876e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.6811e+00, 2.6650e+00, 1.0000e-05, 1.8825e+00,\n",
      "        1.4049e+02, 1.0000e-05, 1.0576e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.5577e+00, 1.1363e+00, 6.9649e+01, 9.4972e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.3810e+01, 1.0000e-05, 1.0000e-05, 7.1215e+00,\n",
      "        1.0000e-05, 1.0000e-05, 2.9350e+02, 6.9949e+01, 2.2532e+01, 4.4796e+00,\n",
      "        1.1647e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.6835e+00, 1.0000e-05,\n",
      "        2.0845e+01, 1.0000e-05, 1.0000e-05, 2.9068e+00, 1.0000e-05, 4.3012e+00,\n",
      "        1.9453e+01, 1.0000e-05, 1.4483e+01, 1.0000e-05, 3.1848e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.6388e+01, 1.0000e-05, 1.1866e+00, 1.4800e-01,\n",
      "        1.7916e+01, 5.0631e+00, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.3836e+00, 1.6311e+02, 1.0000e-05, 8.1778e+00, 1.0000e-05, 1.3971e+02,\n",
      "        6.3183e+00, 1.4453e+02, 2.0720e+01, 3.2711e+01, 8.0036e-01, 1.0000e-05,\n",
      "        1.0000e-05, 6.6618e+00, 1.0000e-05, 1.0000e-05, 2.5360e+02, 2.8917e+00,\n",
      "        1.1369e+01, 2.8285e+01, 1.4271e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.9001e+00, 2.4565e+01, 1.0000e-05, 1.4596e+01,\n",
      "        1.0000e-05, 2.7683e+02, 1.0000e-05, 5.9877e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.9892e+01, 1.0000e-05, 1.1471e+01, 2.7204e+00, 1.0000e-05, 7.2305e+01,\n",
      "        1.0000e-05, 1.5338e+00, 1.0000e-05, 1.3466e+01, 1.0000e-05, 1.0000e-05,\n",
      "        4.1640e+00, 1.0000e-05, 2.3969e+01, 1.0000e-05, 1.0000e-05, 1.2749e+01,\n",
      "        1.0000e-05, 9.0776e+00, 1.0513e+01, 1.0000e-05, 1.0000e-05, 4.4860e+01,\n",
      "        5.6057e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.9793e+00, 1.0000e-05,\n",
      "        1.0000e-05, 4.6530e+00, 1.6677e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.5060e+00, 7.6107e+01, 1.0000e-05, 1.0000e-05, 2.0119e+00,\n",
      "        9.7526e-01, 1.0000e-05, 1.0000e-05, 2.5130e+00, 1.0000e-05, 8.5509e+00,\n",
      "        2.5300e+00, 2.7916e+01, 1.0000e-05, 4.5823e+00, 5.9502e+00, 1.0000e-05,\n",
      "        7.1597e+00, 1.7752e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.1454e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0687e+01, 1.7203e+01, 1.0000e-05, 1.8623e+00,\n",
      "        1.0000e-05, 8.8451e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3069e+00,\n",
      "        1.9394e+01, 4.9886e+00, 4.7026e+00, 1.0000e-05, 1.2711e+00, 1.0000e-05,\n",
      "        1.2589e+01, 1.0000e-05, 1.0000e-05, 1.0814e+01, 4.3101e+00, 3.1781e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.2172e+00, 4.0610e+00, 1.0000e-05, 1.0000e-05, 1.1400e+01, 7.4952e+01,\n",
      "        1.0000e-05, 9.9506e+00, 5.6030e+00, 1.0000e-05, 1.9481e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.2619e+01, 1.6356e+01, 3.5979e+00, 1.0000e-05, 1.0000e-05,\n",
      "        3.9500e+00, 2.7476e+01, 1.0000e-05, 6.0160e+00, 3.3287e+00, 1.0000e-05,\n",
      "        2.8665e+01, 2.7879e+01, 3.9262e+02, 1.0000e-05, 7.8957e+01, 4.6822e+00,\n",
      "        1.0000e-05, 6.9107e+00, 1.6514e+01, 4.1361e+00, 1.0000e-05, 1.0000e-05,\n",
      "        5.4653e+00, 3.3398e+00, 1.0000e-05, 1.0000e-05, 6.8833e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.8108e+02, 1.0000e-05, 1.0148e+01, 1.9362e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2534e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6663e+01, 1.0000e-05, 1.0000e-05,\n",
      "        8.9282e+00, 8.8863e+00, 5.3544e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([3.5559e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.8519e-01, 2.9362e+02,\n",
      "        2.4697e+00, 1.0000e-05, 1.6041e+00, 3.9376e+00, 1.0000e-05, 1.0000e-05,\n",
      "        5.0284e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.4763e+01, 1.0431e+01,\n",
      "        1.0000e-05, 1.0000e-05, 4.9373e+00, 1.0000e-05, 7.8920e+00, 3.7608e+01,\n",
      "        5.7601e+00, 6.4839e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.9369e-01,\n",
      "        8.4661e+00, 4.4620e+01, 1.0000e-05, 2.4463e+00, 1.8936e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.7531e+00, 1.0000e-05,\n",
      "        1.1793e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.7862e+01, 1.0000e-05, 1.1028e+01, 9.3143e-02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2960e+01,\n",
      "        2.7434e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3054e+01, 5.4040e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.4553e+02, 1.0000e-05, 2.4596e+00, 2.7429e+00,\n",
      "        9.2592e+00, 3.1208e+00, 1.0000e-05, 1.3068e+01, 5.1871e-01, 1.0000e-05,\n",
      "        1.0000e-05, 2.1348e+01, 1.0000e-05, 3.7446e+01, 1.0000e-05, 1.7089e+00,\n",
      "        1.3036e+01, 2.4117e+01, 2.1170e+00, 1.1759e+01, 2.1280e+01, 1.2290e+01,\n",
      "        3.6479e+00, 1.0000e-05, 2.4854e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.6452e+01, 2.1273e+01, 2.8351e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.1533e+01, 1.7843e+01, 3.9970e+01, 1.0000e-05, 1.0000e-05, 2.3313e+02,\n",
      "        1.0000e-05, 2.3256e+01, 1.0000e-05, 1.0000e-05, 3.7014e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.7519e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.1078e+00, 1.0000e-05, 1.5720e+01, 1.0000e-05, 2.9162e+00, 1.0000e-05,\n",
      "        2.2319e+00, 2.7692e+01, 2.4353e+01, 3.0617e+01, 8.8836e+00, 3.3843e+01,\n",
      "        1.0000e-05, 7.5416e+00, 1.9099e+00, 8.1982e+00, 1.0000e-05, 2.0842e+01,\n",
      "        1.0000e-05, 1.5534e+02, 1.0000e-05, 1.0000e-05, 2.6724e+00, 1.0000e-05,\n",
      "        8.3569e+00, 1.0000e-05, 1.0000e-05, 5.1100e+00, 1.0000e-05, 8.5719e+00,\n",
      "        1.0000e-05, 1.0000e-05, 2.6358e+00, 4.3888e+01, 3.6814e+00, 1.0000e-05,\n",
      "        1.0000e-05, 5.8046e+00, 5.7452e+00, 2.5071e+01, 3.9149e+00, 1.0000e-05,\n",
      "        5.7864e+00, 1.1119e+02, 2.1051e+01, 1.0000e-05, 1.0000e-05, 1.0328e+01,\n",
      "        1.0000e-05, 1.0000e-05, 3.1816e+01, 9.1881e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2380e+00, 1.0335e+01, 1.0000e-05, 7.4496e-01, 1.0000e-05,\n",
      "        2.9540e+01, 9.2352e+00, 8.4176e+00, 1.3662e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([6.8311e+01, 3.7659e+00, 1.0000e-05, 4.5459e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.9091e+00, 1.0000e-05, 1.0000e-05, 1.7257e+02,\n",
      "        1.5190e+00, 1.6238e+01, 2.1406e+01, 7.6400e+01, 8.5456e+00, 1.1389e+00,\n",
      "        2.6898e+01, 4.7631e+00, 3.3683e+00, 1.0000e-05, 5.6935e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6412e+02, 1.0000e-05,\n",
      "        1.0000e-05, 5.3079e+00, 1.5184e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2030e+01, 1.0000e-05, 1.0000e-05, 8.9263e+00, 1.0175e+00,\n",
      "        1.0000e-05, 4.0115e+01, 1.0000e-05, 1.3226e+01, 5.1922e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.1857e+01, 4.0748e+00, 4.9272e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0975e+01, 1.0000e-05,\n",
      "        1.0000e-05, 5.9895e+00, 1.0000e-05, 8.9344e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 3.9141e+01, 5.5633e+00, 2.6476e+01, 2.3202e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 7.6547e+01, 1.0000e-05, 3.1785e+00,\n",
      "        4.2583e+01, 7.1737e+00, 4.2971e+00, 1.0000e-05, 1.5482e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.7393e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.3162e+01, 1.6492e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.5147e+00, 2.5342e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        9.4835e+01, 7.8902e+00, 1.0000e-05, 3.4662e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.8047e+00, 1.0000e-05, 1.2401e+01, 1.6929e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 6.5452e+00, 6.2149e+01, 4.4920e+01,\n",
      "        1.0000e-05, 1.9896e+00, 1.0000e-05, 8.7468e+00, 6.2770e-01, 1.8288e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2212e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.4606e+01, 2.1881e+01, 1.0000e-05, 5.6549e+01, 3.5652e+00,\n",
      "        1.0000e-05, 1.0000e-05, 8.7593e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.1421e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.0528e+00, 1.7607e+01, 2.3724e+01, 1.0000e-05, 4.7553e+01,\n",
      "        1.0000e-05, 1.2383e+01, 1.0000e-05, 3.0161e+01, 6.2823e+01, 1.0191e+01,\n",
      "        2.1136e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8229e+01, 6.5744e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.1292e-03, 8.2092e-01, 1.0000e-05, 1.9952e+01,\n",
      "        1.0000e-05, 1.0000e-05, 9.4965e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 9.0089e-02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([7.2957e+01, 1.0000e-05, 1.6105e+02, 2.3796e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.3985e+00, 1.0000e-05, 1.0000e-05, 3.6993e+00,\n",
      "        3.9146e+00, 1.0000e-05, 1.0000e-05, 1.3439e+02, 1.0000e-05, 2.4601e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0886e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.2905e+01, 1.0989e+02, 1.0000e-05, 1.6282e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.4447e+00, 8.6620e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.3044e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.8791e+01, 1.8345e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.7742e+02, 1.0000e-05, 9.1899e+01, 1.0000e-05, 1.0000e-05, 1.7598e+00,\n",
      "        1.0000e-05, 5.2887e+01, 4.5957e+01, 1.0000e-05, 1.5781e+00, 9.2886e+00,\n",
      "        1.0000e-05, 1.0000e-05, 3.3104e+00, 1.0725e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 7.5746e+00, 1.0000e-05, 3.5647e+01, 1.7847e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5223e+01, 4.5654e+01, 1.3114e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3345e+01,\n",
      "        5.3737e+00, 1.0000e-05, 1.1919e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.2129e+01, 7.0229e+00, 5.7389e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.6052e+00, 2.4170e+01, 1.0000e-05, 1.0000e-05, 8.3836e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.1437e-01, 7.2422e+00, 7.4553e+00,\n",
      "        6.4043e+01, 1.0000e-05, 1.6605e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.6756e+01, 1.0000e-05, 1.6343e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.1975e+00, 1.0000e-05, 1.4247e+02, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 7.6914e+01, 5.9512e+00, 1.0000e-05,\n",
      "        1.2560e+02, 4.0822e+00, 1.0000e-05, 9.9743e+01, 1.0000e-05, 2.7301e+02,\n",
      "        7.9193e+00, 4.8434e+00, 1.0000e-05, 3.6387e+02, 1.0000e-05, 1.0000e-05,\n",
      "        8.2332e+00, 1.1744e+02, 1.0000e-05, 1.0000e-05, 3.0188e+01, 1.0000e-05,\n",
      "        1.0000e-05, 2.9064e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.3194e+01,\n",
      "        4.8654e-01, 1.0000e-05, 1.0687e+01, 1.0000e-05, 8.0914e+00, 1.0000e-05,\n",
      "        3.0578e+00, 6.4847e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.2785e+01,\n",
      "        5.5767e+00, 1.0000e-05, 5.9243e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.2136e+01, 1.0000e-05, 1.0000e-05, 8.2624e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.9734e+01, 2.2385e+01, 1.0000e-05, 3.3324e+02, 1.2301e+01, 1.5163e+00,\n",
      "        1.0000e-05, 1.2445e+00, 2.6751e+01, 1.0000e-05, 1.0000e-05, 7.0577e+00,\n",
      "        1.0000e-05, 1.0768e+01, 1.0000e-05, 1.0000e-05, 2.0106e+00, 9.9020e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.2908e+01, 2.8194e+00, 1.0000e-05, 4.8470e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.9140e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 9.1901e+01, 7.3892e+01, 1.0000e-05, 1.2507e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.6455e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0280e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.8106e+00, 2.5501e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.4692e+00,\n",
      "        1.0000e-05, 7.2888e-01, 3.2819e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.2394e+01, 1.0000e-05, 1.0000e-05, 7.9297e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([4.7399e+01, 3.7551e+00, 4.0964e+02, 5.4642e+00, 1.6723e+00, 1.8553e+02,\n",
      "        1.0000e-05, 1.3158e+02, 3.0866e+01, 1.0000e-05, 3.3094e+01, 1.0000e-05,\n",
      "        4.1827e+01, 9.3571e+01, 1.5994e+00, 1.0000e-05, 1.2590e+02, 1.0000e-05,\n",
      "        6.6315e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.5046e+01, 5.1913e+01, 1.0000e-05, 1.0000e-05, 4.3779e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.0535e+00, 1.0000e-05, 5.1981e+01, 6.1463e+00,\n",
      "        1.0000e-05, 4.7935e+00, 1.5304e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7038e+01, 2.3217e+02, 1.0000e-05,\n",
      "        1.0000e-05, 5.1166e+00, 1.0853e+01, 7.9116e+00, 1.0000e-05, 1.6826e+00,\n",
      "        2.0102e+00, 2.3248e+00, 5.7142e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.5377e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([3.9816e+00, 1.0000e-05, 1.3854e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.6098e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.9447e+00, 5.1301e+00,\n",
      "        1.0000e-05, 1.0269e+00, 1.0000e-05, 1.0000e-05, 4.9202e+01, 3.3073e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.5971e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.1791e+02, 4.7442e+00, 1.0000e-05, 1.0000e-05, 1.2664e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.1981e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9076e+02,\n",
      "        1.0000e-05, 8.5852e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.3239e+00, 3.6271e+00, 2.6411e+00, 1.0612e+00, 1.0000e-05, 6.3830e+01,\n",
      "        1.0000e-05, 4.0361e+01, 1.0000e-05, 6.2665e+01, 1.0932e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.9855e+01, 8.0019e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([5.7005e+00, 2.5122e+01, 1.6432e+00, 6.7357e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.4656e+02, 1.0000e-05, 6.0520e+00, 6.3436e+01, 8.0699e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.5628e+02, 1.9216e+01, 1.1905e+00, 8.7955e+01, 1.0000e-05,\n",
      "        1.0000e-05, 3.9807e+00, 1.1745e+01, 7.9786e-01, 1.0000e-05, 1.2567e+02,\n",
      "        1.0000e-05, 3.4976e+00, 1.5098e+01, 1.0000e-05, 3.1739e+00, 1.5685e+00,\n",
      "        2.9245e-01, 2.3057e+01, 1.0000e-05, 8.7074e+00, 1.1198e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.2391e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.3573e+00, 1.0000e-05, 2.2921e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3284e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8714e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 9.7051e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.2634e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.5872e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.7330e+00,\n",
      "        1.0000e-05, 7.1858e+01, 1.0000e-05, 1.5708e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 3.2954e+01, 1.8883e+00, 1.0000e-05, 1.0000e-05, 1.4866e+02,\n",
      "        9.1206e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.7620e+00, 1.0000e-05,\n",
      "        1.2641e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4638e+02, 7.7074e-01,\n",
      "        1.3759e+02, 5.5121e+00, 3.8343e+01, 1.0000e-05, 1.0000e-05, 6.1323e+00,\n",
      "        6.0745e-02, 2.4691e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2876e+01, 1.0000e-05, 5.2016e+00, 5.6045e+01, 1.0000e-05,\n",
      "        1.0000e-05, 6.0455e+00, 1.0000e-05, 5.2322e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.4122e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.5844e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.7095e+01, 1.0000e-05, 3.7814e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.4164e+01, 1.0000e-05, 1.3299e+02,\n",
      "        1.0000e-05, 6.1871e+00, 1.0000e-05, 2.0932e+01, 1.0000e-05, 1.0000e-05,\n",
      "        7.5474e+00, 2.1888e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.1255e+01,\n",
      "        5.3159e+00, 2.1888e+01, 1.9402e+00, 8.4240e+00, 4.2128e+00, 7.7989e+01,\n",
      "        1.0000e-05, 1.0000e-05, 4.5344e+01, 6.9254e+00, 1.0000e-05, 7.9458e+00,\n",
      "        2.3298e+02, 1.0000e-05, 6.7549e-01, 6.7597e-01, 2.5388e+01, 1.0000e-05,\n",
      "        2.3042e+00, 1.0000e-05, 2.5728e+01, 1.3071e+01, 1.0000e-05, 1.3243e+02,\n",
      "        7.9511e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.5110e+01, 1.0000e-05, 5.5285e+01, 7.9008e-01, 1.0000e-05, 1.0000e-05,\n",
      "        5.4937e+01, 1.0000e-05, 1.0000e-05, 2.5878e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.1261e+00, 1.0000e-05, 1.0000e-05, 1.2964e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.1396e+02, 1.0000e-05, 1.0000e-05, 9.6761e+00, 1.3693e-01,\n",
      "        1.0000e-05, 1.0000e-05, 3.2328e-01, 1.0000e-05, 1.0000e-05, 1.1829e+01,\n",
      "        1.7142e+01, 7.3493e+00, 3.5983e+01, 6.0144e+00, 1.7808e+01, 1.0000e-05,\n",
      "        3.6508e+01, 9.1752e+00, 8.1014e+00, 1.0000e-05, 4.0477e+00, 1.0000e-05,\n",
      "        1.0000e-05, 4.4740e+00, 1.4697e+01, 3.0613e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.4530e+02, 1.0000e-05, 1.2802e+01, 1.0000e-05, 1.0505e+00,\n",
      "        2.1299e+00, 1.0000e-05, 1.0718e+01, 1.0000e-05, 3.3835e-01, 1.0000e-05,\n",
      "        5.5199e+01, 1.0000e-05, 2.7050e+00, 1.6656e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([9.3646e+01, 2.7068e+01, 7.5748e+01, 1.0000e-05, 1.0000e-05, 4.6771e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7486e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.6116e-01, 1.0000e-05, 6.6705e+00, 7.7544e+01,\n",
      "        1.0000e-05, 2.2950e+01, 3.3652e+00, 1.0789e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.4591e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5138e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.5357e+01, 2.1685e+01, 2.4202e+00, 4.9237e+00,\n",
      "        1.0000e-05, 1.0000e-05, 5.5587e+01, 1.0000e-05, 7.3537e+01, 2.8631e+01,\n",
      "        1.0000e-05, 1.0000e-05, 6.4268e+01, 1.0000e-05, 2.5960e+01, 1.0000e-05,\n",
      "        9.9358e+00, 1.0000e-05, 4.3344e+00, 1.0000e-05, 1.0000e-05, 6.6905e+01,\n",
      "        1.9806e+01, 1.0000e-05, 1.6153e+00, 6.4640e+01, 5.9607e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.0983e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 7.6592e-01, 3.3246e+00, 1.0000e-05, 2.1045e+00,\n",
      "        6.4313e+00, 1.5650e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1681e+01,\n",
      "        1.0000e-05, 2.7923e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.0662e+01, 2.8125e+01, 1.0000e-05, 9.6723e+00, 2.4478e+02,\n",
      "        8.3333e+01, 1.0335e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.9766e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.4741e+00, 1.0000e-05,\n",
      "        2.7628e+02, 3.9833e+01, 4.6823e+00, 1.0453e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.9539e+00, 1.7278e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.6660e+00, 1.0000e-05, 3.0883e+00, 2.0739e-01,\n",
      "        1.0000e-05, 6.6272e+00, 1.0000e-05, 1.0000e-05, 6.1396e+00, 1.0000e-05,\n",
      "        1.2567e+01, 8.1325e+00, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 4.0404e+01, 1.4072e+01, 2.3715e+00, 1.9712e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.0999e+00, 1.2400e+01,\n",
      "        1.1283e+00, 5.6778e+01, 3.0514e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 8.4201e+00, 1.0000e-05, 5.2625e+00, 1.0000e-05, 7.6094e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.3421e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.9966e+00, 1.0000e-05, 1.0000e-05, 9.8555e+00, 2.9067e+01,\n",
      "        1.0000e-05, 2.1544e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1156e+00,\n",
      "        7.2272e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4678e+00, 1.0000e-05,\n",
      "        1.0000e-05, 3.2993e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2894e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4992e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.4422e+01, 2.8720e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([3.7988e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2451e+00,\n",
      "        1.0000e-05, 5.2611e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.2008e-01, 1.0000e-05, 8.8082e+00, 1.1552e+01, 1.0737e+00, 1.5428e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0921e+00, 6.3478e+01, 4.9823e-01, 5.1105e+00,\n",
      "        2.4034e+01, 5.2192e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.9302e+01,\n",
      "        1.0000e-05, 1.0000e-05, 4.9584e+01, 1.0000e-05, 6.4124e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.1286e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1743e+01,\n",
      "        1.0000e-05, 2.1335e+00, 2.2917e+01, 2.7752e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.8483e+00, 2.1880e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([1.0898e+01, 1.0000e-05, 3.1477e+01, 1.0000e-05, 1.0000e-05, 1.6543e+01,\n",
      "        5.1291e+00, 8.4552e-01, 1.0000e-05, 9.0497e+01, 1.0000e-05, 1.2182e+02,\n",
      "        1.0000e-05, 8.3125e+00, 2.6412e+01, 3.0136e+01, 1.0000e-05, 7.6907e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1242e+00, 1.0000e-05, 1.0000e-05,\n",
      "        6.3705e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.9548e+02, 7.4929e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.3026e+02, 1.0000e-05, 1.6138e-01, 2.0665e+00, 8.5308e+01, 1.0000e-05,\n",
      "        2.6639e+01, 1.0000e-05, 1.0000e-05, 5.2334e+01, 1.0000e-05, 1.0000e-05,\n",
      "        3.9900e+00, 1.0000e-05, 9.3299e+00, 1.0000e-05, 1.3057e+00, 4.5736e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0112e+01, 1.0658e+00,\n",
      "        1.2689e+01, 1.0000e-05, 1.0000e-05, 4.4302e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.9483e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2626e+01, 1.0000e-05,\n",
      "        2.0777e+00, 1.0000e-05, 1.0195e+02, 9.3278e+01, 1.0000e-05, 8.4376e+00,\n",
      "        1.0000e-05, 4.8974e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.1870e+01,\n",
      "        5.7198e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.1755e+01, 7.3693e+00, 1.2558e+01, 3.0502e+00,\n",
      "        1.5115e+01, 1.1026e+00, 1.0000e-05, 1.9323e+02, 9.6114e-01, 1.0000e-05,\n",
      "        1.0000e-05, 8.8049e+00, 1.0955e+01, 1.0590e+02, 2.2885e+00, 8.3926e+00,\n",
      "        1.0000e-05, 1.1671e+01, 1.7773e+01, 1.0000e-05, 3.0456e+01, 1.0000e-05,\n",
      "        4.0883e-01, 1.0000e-05, 3.6889e+00, 1.0000e-05, 2.3114e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 5.2037e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.9224e+00, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([3.3967e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1073e+01, 5.7444e+00,\n",
      "        1.0000e-05, 1.7472e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.9899e+01, 1.0000e-05, 1.0000e-05, 7.5031e+00, 2.8707e+00, 1.0000e-05,\n",
      "        2.4579e+00, 1.0000e-05, 1.8121e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2805e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.4052e+01,\n",
      "        6.2580e+00, 3.3124e+00, 1.0000e-05, 1.0000e-05, 4.5965e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.0956e+00, 1.7773e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0536e+02, 8.4668e+00, 1.0000e-05, 3.0557e+00, 6.7684e+00, 2.8950e+00,\n",
      "        1.0000e-05, 1.0000e-05, 3.9079e+01, 3.4657e+00, 1.0000e-05, 8.0649e+01,\n",
      "        1.0000e-05, 1.6891e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.1470e+02, 1.0000e-05, 6.5920e+00, 1.3494e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6593e+00, 3.5507e+00,\n",
      "        1.0939e+02, 1.1187e+02, 1.6321e+01, 3.4934e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3015e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.9973e+00, 5.5553e+00, 6.5885e+00,\n",
      "        3.0763e+00, 8.5782e+00, 1.0000e-05, 2.2367e+00, 9.5936e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.0579e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 6.7710e+00, 1.0000e-05, 1.0000e-05, 1.1904e+02,\n",
      "        3.3125e+01, 1.0000e-05, 1.0000e-05, 1.7716e+01, 1.2947e+01, 4.6589e+01,\n",
      "        3.4059e+00, 1.0000e-05, 1.0000e-05, 9.6798e+00, 1.0000e-05, 1.0386e+01,\n",
      "        1.9397e+02, 9.4796e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.1727e+00,\n",
      "        4.4675e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.4481e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.7842e+01, 2.8214e+00, 1.5095e+01, 1.0000e-05, 6.2618e+00,\n",
      "        1.0000e-05, 1.0000e-05, 5.8089e+00, 1.0000e-05, 1.1232e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.5891e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.3313e+01, 7.8757e+00, 1.7096e+02, 4.6970e+00,\n",
      "        5.9010e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.8847e+00, 2.1932e+00, 6.0681e+00, 1.0000e-05,\n",
      "        3.4805e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1525e+02,\n",
      "        1.0000e-05, 6.8143e+00, 9.4751e-01, 1.0000e-05, 9.3252e+00, 1.3432e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5951e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.1148e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.2411e+01, 3.4706e+00, 5.3422e+00, 1.0000e-05, 3.0537e-01, 1.0000e-05,\n",
      "        3.3402e+01, 2.6204e+02, 1.0746e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        6.5585e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.0235e+00,\n",
      "        7.1021e+00, 1.2809e+00, 1.4234e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.1636e+01, 1.0000e-05, 1.7689e+01, 1.0000e-05, 4.7234e+00, 5.4067e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.3557e+00, 5.2947e+01, 1.5490e+00,\n",
      "        1.0000e-05, 5.6742e+00, 3.8161e+01, 1.8635e+00, 1.0000e-05, 9.6011e+00,\n",
      "        1.8460e+00, 1.0490e+01, 1.0000e-05, 1.4106e+01, 1.0000e-05, 2.2616e+01,\n",
      "        2.3014e+01, 1.0000e-05, 1.4833e+01, 1.8494e+01, 3.1219e+00, 1.8690e+01,\n",
      "        1.0000e-05, 1.0000e-05, 9.0331e-02, 1.0000e-05, 2.1334e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 5.3859e-01, 8.8666e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.4763e+00, 1.0000e-05, 1.3233e+01, 1.0760e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.0361e+01, 6.0845e+00, 1.0000e-05, 6.2768e+00, 3.6628e+01, 1.0000e-05,\n",
      "        1.3619e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.9281e+01, 3.4310e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8713e+01, 1.2209e+02,\n",
      "        1.4213e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.1218e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 3.7571e+01, 1.0000e-05, 8.6977e+00,\n",
      "        1.0000e-05, 4.4581e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.6046e+00,\n",
      "        1.5645e+02, 3.1048e+02, 1.0000e-05, 1.0589e+00, 9.9235e+00, 1.0000e-05,\n",
      "        1.4402e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.1722e+01, 1.0000e-05, 1.0000e-05, 1.0936e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 2.1752e+02, 1.0000e-05, 1.2031e+01, 5.7010e-01,\n",
      "        1.7713e+00, 1.0000e-05, 1.0000e-05, 4.6383e+01, 1.4829e+01, 1.5860e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1491e+02, 1.0000e-05,\n",
      "        6.2638e+00, 1.0000e-05, 1.9655e+00, 1.6977e+01, 1.0000e-05, 3.7064e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.3194e+01, 1.2005e+01,\n",
      "        1.0000e-05, 1.0000e-05, 2.1208e+02, 1.8876e+00, 1.0000e-05, 4.6634e+01,\n",
      "        1.0000e-05, 8.2160e+00, 1.4061e+01, 1.0000e-05, 8.3431e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.1066e+00, 1.6004e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.3748e+00, 9.4655e+00, 1.0000e-05, 1.0000e-05, 2.8430e+02, 1.0000e-05,\n",
      "        3.7420e+00, 1.2135e+00, 7.7586e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.8356e+01, 1.0000e-05, 1.0000e-05, 3.2915e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6668e+00, 1.9542e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.2363e+02, 1.0000e-05, 1.0000e-05, 1.0079e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        8.6461e+00, 8.1373e+01, 2.3378e+01, 2.2079e+00, 1.0000e-05, 5.5881e+00,\n",
      "        1.0000e-05, 9.2772e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.2802e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.5058e+02, 4.1554e+01, 1.0000e-05,\n",
      "        3.2798e+01, 8.5664e+00, 1.0000e-05, 7.3879e+00, 1.0000e-05, 3.0105e+00,\n",
      "        1.0000e-05, 1.0000e-05, 9.7048e+00, 7.6761e-01, 1.0000e-05, 1.0000e-05,\n",
      "        2.4361e+00, 1.0000e-05, 1.9498e+01, 2.2920e+00, 1.1363e-01, 7.3056e+01,\n",
      "        5.9980e+00, 1.0000e-05, 1.3573e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 8.3344e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 5.0354e+00, 2.0579e+01, 1.0000e-05, 2.6947e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0721e+01, 3.9290e+00, 1.5559e+00, 1.0000e-05, 1.0000e-05,\n",
      "        5.8150e+00, 1.5780e+00, 1.0000e-05, 1.0000e-05, 2.0806e+01, 1.5118e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.3448e-01, 1.0000e-05, 1.0000e-05,\n",
      "        4.4431e+00, 1.0000e-05, 2.3481e+01, 1.0000e-05, 1.4207e+02, 1.0000e-05,\n",
      "        5.7146e+00, 1.0000e-05, 1.6779e+00, 4.1960e+01, 1.5169e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.3307e+01, 1.4561e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.6392e+01, 6.9723e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.6499e+01, 4.8713e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.2896e+01, 1.0000e-05, 6.4859e+00, 8.6127e+01, 4.0199e+01, 4.6713e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6658e+02, 1.0000e-05,\n",
      "        1.0000e-05, 5.0047e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.5518e+00, 1.0000e-05, 1.0000e-05, 1.4867e+00, 1.0000e-05, 1.2597e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2010e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.3041e-01, 8.1967e+00, 1.0000e-05, 5.4478e+00, 1.6148e-01, 4.4258e+00,\n",
      "        8.8405e+01, 1.1068e+00, 1.4040e+02, 1.0000e-05, 1.0000e-05, 9.0585e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8970e+01, 7.1505e-01, 1.0000e-05,\n",
      "        1.5222e+02, 1.0000e-05, 1.7321e+00, 1.0000e-05, 2.7426e+00, 9.6648e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 6.2731e+00, 1.0000e-05, 1.4654e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 8.0311e+01, 1.0000e-05,\n",
      "        1.2670e+01, 1.0000e-05, 1.0000e-05, 4.6350e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([5.8289e+00, 1.0000e-05, 1.0000e-05, 2.8580e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 2.0079e+00, 2.1275e+00, 1.0000e-05, 1.0000e-05,\n",
      "        4.3033e+01, 7.4919e-01, 1.0000e-05, 2.7836e+01, 2.4359e+01, 1.0000e-05,\n",
      "        1.2296e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.9946e+00,\n",
      "        3.6318e+01, 1.0000e-05, 1.0000e-05, 4.1951e+00, 2.8090e+00, 1.0000e-05,\n",
      "        1.9243e+00, 1.1971e+00, 1.0000e-05, 5.3540e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2537e+01, 9.2674e+01, 1.0000e-05, 8.8826e+00, 2.0657e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.5927e-01, 1.3172e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.5735e+01, 1.0000e-05, 2.6279e+01, 1.5404e+01,\n",
      "        1.7860e+01, 7.4034e+00, 7.2934e+00, 1.3190e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.1162e+00, 8.2692e+00, 3.4450e+00, 1.0000e-05, 1.2332e+01, 1.5290e+00,\n",
      "        2.4308e+02, 1.0000e-05, 1.0000e-05, 2.4340e+02, 1.0000e-05, 1.0000e-05,\n",
      "        8.5392e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.2570e+02, 3.4871e+00, 1.4938e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.4241e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 4.0559e+01, 1.0000e-05, 9.8597e+00, 2.4924e+01, 3.0338e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1948e+02,\n",
      "        3.3448e+02, 1.0000e-05, 1.5923e+00, 1.0000e-05, 1.6733e+01, 1.0000e-05,\n",
      "        1.0000e-05, 9.0053e+00, 1.0000e-05, 3.4092e+01, 1.0000e-05, 1.0000e-05,\n",
      "        3.5089e+00, 1.0000e-05, 1.0000e-05, 1.2973e+02, 1.2055e+01, 1.0000e-05,\n",
      "        3.1888e+00, 6.5231e+00, 5.2969e+01, 8.8791e+00],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 2.4229e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.9463e+01, 5.3497e+01, 8.1736e+00, 1.0530e+02, 1.0000e-05, 1.0000e-05,\n",
      "        9.5193e+00, 5.7864e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 7.2723e+00, 1.0000e-05, 1.0000e-05, 5.7232e+00,\n",
      "        1.0000e-05, 2.6920e+01, 3.5792e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.4522e+00, 1.0000e-05, 1.0000e-05, 1.9378e+01, 4.4807e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.6712e+00, 4.1172e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.2862e+02, 4.2762e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        4.6274e+00, 1.0000e-05, 1.4221e+02, 1.0000e-05, 1.2304e+00, 2.9669e+00,\n",
      "        3.6108e+00, 1.0000e-05, 1.9098e+00, 2.8035e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.4189e+01, 1.0000e-05, 1.4226e+01, 2.7554e+00, 1.0000e-05, 8.3665e+01,\n",
      "        1.0000e-05, 1.2183e+00, 1.0000e-05, 3.8368e+02, 1.0660e+00, 7.2962e+00,\n",
      "        7.5210e-01, 1.0000e-05, 1.0000e-05, 2.1782e+01, 1.0000e-05, 1.3089e+02,\n",
      "        1.0000e-05, 1.4618e+01, 1.0000e-05, 2.1313e+01, 1.3232e+02, 3.0149e+00,\n",
      "        3.6640e+00, 1.1847e+01, 2.3985e+01, 2.6270e+01, 1.5034e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.5909e-01, 4.5230e+00, 1.0000e-05, 1.0000e-05, 9.3754e+00,\n",
      "        1.0000e-05, 1.0000e-05, 6.8361e+00, 1.0000e-05, 1.0000e-05, 1.7747e+01,\n",
      "        6.7044e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.6555e+01,\n",
      "        1.2288e+01, 1.0000e-05, 7.8210e+01, 1.0000e-05, 1.6804e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 9.6270e+01, 2.3299e+02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 3.0486e+01, 1.3536e+02],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.9479e+02, 1.0000e-05, 1.0000e-05, 1.9954e+01, 2.8317e+02, 1.0000e-05,\n",
      "        4.3190e+00, 7.7559e+00, 1.0000e-05, 8.7569e+01, 2.6652e+00, 4.6960e+01,\n",
      "        1.0000e-05, 4.9136e-01, 1.1587e+01, 5.6035e+01, 1.0000e-05, 9.9252e+00,\n",
      "        4.1269e+00, 1.0000e-05, 1.1117e+01, 5.2745e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5270e+01, 3.7188e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0138e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        9.0046e+00, 1.0000e-05, 3.7998e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.1796e-01, 3.1246e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 5.1293e+00,\n",
      "        6.3047e+00, 1.1643e+01, 6.0583e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        5.9196e+01, 3.9234e+01, 1.0000e-05, 2.9033e+00, 1.2397e+00, 2.6072e+01,\n",
      "        1.0000e-05, 3.8657e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.3257e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.7495e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.3711e+00, 1.0000e-05,\n",
      "        1.0000e-05, 2.0804e+00, 1.0000e-05, 1.3323e+01, 1.0000e-05, 2.6149e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.8890e+02, 2.4416e+02, 1.5521e+01,\n",
      "        1.0000e-05, 1.7438e+01, 1.0000e-05, 1.7775e+01, 1.0000e-05, 1.0000e-05,\n",
      "        2.7918e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1115e+00, 1.1188e+00,\n",
      "        1.0069e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.9667e-01, 3.9801e+01,\n",
      "        1.2453e+00, 1.0000e-05, 1.0823e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.1427e+01, 1.0000e-05, 1.3983e+01, 1.0000e-05, 2.6421e-01, 4.9605e+01,\n",
      "        1.0000e-05, 1.9348e+02, 1.5952e+02, 1.5632e+01, 2.0043e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0929e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.8875e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.3712e+01, 6.2381e+00,\n",
      "        1.0000e-05, 6.5611e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.1444e+01,\n",
      "        1.0000e-05, 4.0716e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.2461e+01,\n",
      "        4.7019e+01, 1.0000e-05, 5.7723e-01, 1.0000e-05, 2.9616e+01, 1.0000e-05,\n",
      "        3.3811e+00, 4.3623e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.2655e+01,\n",
      "        4.6129e+01, 4.5070e+01, 2.4345e+01, 4.2908e+01, 1.0000e-05, 1.0000e-05,\n",
      "        9.2021e+00, 1.0000e-05, 1.0000e-05, 1.1333e+01, 5.6257e-01, 1.0000e-05,\n",
      "        2.4066e+00, 1.0000e-05, 1.0000e-05, 8.1495e+00, 2.4107e+00, 1.0000e-05,\n",
      "        1.0000e-05, 9.7077e-01, 1.0000e-05, 4.8496e+00, 2.1492e+01, 1.0000e-05,\n",
      "        8.1438e+01, 1.0000e-05, 5.2366e+00, 1.0000e-05, 1.1243e+02, 1.0000e-05,\n",
      "        1.0000e-05, 3.1592e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.0000e-05, 5.6182e-01, 1.0000e-05, 8.7838e+00,\n",
      "        2.7670e+00, 4.0749e+00, 1.9254e+00, 1.0000e-05, 9.9823e+00, 1.0000e-05,\n",
      "        8.6745e+00, 4.0238e+01, 3.6921e+00, 1.7816e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.1844e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.0815e+01, 1.0000e-05, 6.9342e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.2699e+02, 1.0000e-05, 2.7605e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.5491e+01, 3.4055e+00, 7.5691e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 9.1252e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.3029e+00, 5.2754e+00, 4.6512e-01, 9.8813e+00, 1.0000e-05, 1.2686e+01,\n",
      "        1.0297e+01, 2.4456e+01, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([8.2743e+01, 1.0000e-05, 2.3789e+01, 1.9058e+00, 1.0000e-05, 1.0000e-05,\n",
      "        2.4287e+00, 5.3414e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        3.4997e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 7.9184e+00, 1.0000e-05, 2.3223e+02,\n",
      "        4.7297e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.5554e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.8407e+01, 1.0000e-05, 1.7602e+01, 1.3799e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.9422e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 8.4401e+01, 1.0000e-05, 3.9689e+02, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 4.7464e+01, 1.0000e-05, 1.5135e+02, 5.7737e+00,\n",
      "        1.0000e-05, 1.0000e-05, 5.0561e+01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.3485e+01, 6.5631e-01, 1.0000e-05, 1.8973e+01,\n",
      "        1.3472e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 6.5865e+01,\n",
      "        1.0000e-05, 4.9762e-01, 1.0000e-05, 1.6417e+01, 5.5256e+01, 1.4918e+01,\n",
      "        1.0000e-05, 1.0000e-05, 6.1906e+00, 1.0000e-05, 4.2704e+00, 3.6152e+02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.3988e+01, 1.0000e-05, 1.0000e-05, 3.0659e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.5537e-01, 3.7396e+01, 9.5129e+01, 1.0000e-05, 1.9649e+00,\n",
      "        9.6326e+00, 7.5218e+01, 1.7888e+01, 2.3924e+00, 1.0000e-05, 1.0000e-05,\n",
      "        3.6531e+01, 1.0294e+01, 1.0000e-05, 1.0000e-05, 1.6130e+02, 1.0000e-05,\n",
      "        1.4830e+01, 2.6506e+00, 1.0000e-05, 1.0000e-05, 4.5034e+00, 4.7368e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss Calculation\n",
      "tensor([8.8794e+00, 1.0000e-05, 1.0000e-05, 9.5122e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 2.1000e+02, 1.0000e-05, 1.0000e-05, 1.5998e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.9132e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.8608e+01, 3.1843e+01, 1.0000e-05,\n",
      "        3.1778e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.3716e-01, 1.3948e+02, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        7.8348e+01, 1.0000e-05, 1.0000e-05, 2.2640e+00, 3.3527e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        7.0931e+01, 1.0000e-05, 7.6804e+00, 3.8084e-02, 4.2671e+01, 1.4026e-01,\n",
      "        4.3345e+01, 1.0000e-05, 7.1119e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.1352e+01, 1.8283e+01, 5.9960e-01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([4.0058e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.8495e+01, 2.8472e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.5384e+02, 1.0000e-05, 1.0000e-05, 1.6883e+00,\n",
      "        1.3445e+01, 9.9205e+00, 1.0000e-05, 1.0784e+00, 1.0000e-05, 3.2460e+01,\n",
      "        7.7235e-02, 1.0000e-05, 1.0000e-05, 2.9361e+01, 6.6287e+00, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0664e+01, 1.0000e-05,\n",
      "        4.1058e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.8685e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 3.3436e+01, 1.0000e-05,\n",
      "        3.7562e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 4.5884e+00, 1.0000e-05,\n",
      "        1.0000e-05, 9.8385e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.3322e+00, 2.5361e+00, 1.0000e-05, 1.4169e+02, 3.1685e+00,\n",
      "        1.0000e-05, 1.0000e-05, 9.4903e+01, 1.6099e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.7464e+02, 1.0000e-05, 5.0170e+00, 1.0000e-05, 4.5229e+01, 1.0000e-05,\n",
      "        3.8341e+00, 1.0000e-05, 1.0815e+01, 8.1455e+00, 1.0000e-05, 8.5033e+00,\n",
      "        8.8633e+01, 1.7481e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5270e+01,\n",
      "        1.0000e-05, 4.7016e+00, 1.0358e+01, 3.2371e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 9.9430e+01, 1.0000e-05, 1.0000e-05,\n",
      "        5.4714e+01, 1.0000e-05, 1.0000e-05, 2.2487e+01, 7.0566e+00, 1.0000e-05,\n",
      "        1.9611e+01, 1.0000e-05, 6.7174e-01, 1.0000e-05, 1.0000e-05, 4.6911e-01,\n",
      "        1.0000e-05, 1.0000e-05, 1.3210e+00, 1.5868e+02, 1.0000e-05, 1.0000e-05,\n",
      "        4.1232e+00, 2.5155e+01, 1.3143e+01, 8.6943e+00, 4.7637e+00, 1.0709e+02,\n",
      "        1.0000e-05, 1.3349e+01, 3.1230e+01, 1.1436e+01, 2.7226e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.0000e-05, 1.0000e-05, 1.6631e+01, 1.0000e-05, 1.1753e+02, 1.0000e-05,\n",
      "        1.0000e-05, 4.3158e+01, 1.0000e-05, 5.2858e-01, 5.8984e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 4.4692e+00, 1.0000e-05, 2.8510e+01,\n",
      "        8.2961e-01, 1.0000e-05, 1.0000e-05, 2.7386e+01, 4.0174e+01, 7.7390e-02,\n",
      "        1.4929e+00, 4.5256e+01, 1.0000e-05, 1.0869e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.4803e-01, 1.0000e-05, 1.0000e-05, 7.6122e+01, 1.0000e-05,\n",
      "        5.6285e+01, 1.0000e-05, 8.9665e+01, 3.7538e+00, 5.0539e+00, 1.0000e-05,\n",
      "        3.2707e+01, 1.4235e+01, 3.5707e+01, 2.1494e+01, 1.6367e+00, 8.3232e+00,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 8.6469e+00, 1.0000e-05, 1.0000e-05,\n",
      "        8.2793e+01, 1.0000e-05, 1.2746e-01, 1.0000e-05],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([1.3674e+02, 1.5390e+01, 1.8496e+01, 1.7380e+02, 1.6252e-01, 1.2889e+01,\n",
      "        1.0000e-05, 4.5393e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 6.1491e+01, 1.0000e-05, 1.4588e+02, 7.6464e+00, 7.1973e+01,\n",
      "        1.0000e-05, 1.0000e-05, 7.0814e+01, 6.0776e+00, 1.0000e-05, 1.0000e-05,\n",
      "        9.9350e+01, 1.0000e-05, 1.0000e-05, 1.3889e+01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0474e+01, 3.3245e-01, 4.4966e+01, 1.0000e-05, 1.1410e-01, 1.0000e-05,\n",
      "        1.0213e+01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.0812e+00, 1.0000e-05,\n",
      "        3.9189e+01, 2.8813e+01, 9.4744e+00, 2.3576e+02, 2.4050e+01, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 5.5917e-01, 1.0000e-05, 5.9638e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.5657e+01, 1.0000e-05, 1.0990e+02, 2.2187e+01,\n",
      "        1.0000e-05, 1.0000e-05, 1.8479e+01, 5.6336e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n",
      "Triplet Loss Calculation\n",
      "tensor([2.0363e+00, 1.9246e+00, 1.0000e-05, 1.0000e-05, 7.6793e+00, 4.2962e+01,\n",
      "        1.9476e+01, 1.0000e-05, 1.0000e-05, 5.8280e+01, 1.0000e-05, 1.1824e+02,\n",
      "        1.0000e-05, 4.9237e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        2.4450e+01, 1.0202e+02, 5.3158e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 5.9439e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 2.5420e+01,\n",
      "        1.0000e-05, 6.8667e+00, 1.0000e-05, 1.0000e-05, 1.8428e+01, 1.0000e-05,\n",
      "        1.0000e-05, 2.3169e+00, 1.0000e-05, 1.0000e-05, 1.0000e-05, 9.0818e+00,\n",
      "        1.0000e-05, 3.7124e+00, 1.0000e-05, 4.6313e-01, 7.8679e+01, 1.0000e-05,\n",
      "        1.9976e+01, 5.9956e+00, 1.0000e-05, 9.4561e+00, 1.0000e-05, 1.0000e-05,\n",
      "        1.8440e+02, 1.0000e-05, 1.0000e-05, 5.0751e+01, 5.4801e+01, 1.0000e-05,\n",
      "        2.8620e+01, 1.0000e-05, 1.0000e-05, 3.8222e+01],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "CAP ARRAY\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     33\u001b[0m     loss_fn_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_cap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_cap\n\u001b[0;32m---> 34\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_triplet_capped_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tripletloss_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcap_calc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTripletLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCappedBCELoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     36\u001b[0m         _, auc \u001b[38;5;241m=\u001b[39m metric_utils\u001b[38;5;241m.\u001b[39mauc_sigmoid(test_loader_reduced, network, embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/train.py:233\u001b[0m, in \u001b[0;36mtrain_triplet_capped_loss\u001b[0;34m(epoch, train_loader, network, optimizer, directory, verbose, cap_calc, loss_fn, loss_fn_args, print_dist)\u001b[0m\n\u001b[1;32m    231\u001b[0m anchor_output, anchor_embeds \u001b[38;5;241m=\u001b[39m network(anchor_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    232\u001b[0m _, pos_embeds \u001b[38;5;241m=\u001b[39m network(pos_data\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m--> 233\u001b[0m _, neg_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m cap \u001b[38;5;241m=\u001b[39m cap_calc(anchor_embeds, pos_embeds, neg_embeds) \n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_dist:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/class-sample-research/models.py:42\u001b[0m, in \u001b[0;36mConvNetWithEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1_drop(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m) \n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "loss_fn_args['print_loss']=False\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 5\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    \n",
    "    learning_rate_aucs = []\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=loss_fns.TripletLoss,loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args, print_dist=True)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "    \n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm, \"margin=0.5\"]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25ce4afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006964887678623199, AUC: 0.3482715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00073517307639122, AUC: 0.841398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010314007997512817, AUC: 0.826527\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009717654287815094, AUC: 0.8537300000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006961984634399415, AUC: 0.47539049999999994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007382735610008239, AUC: 0.825611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009567088186740875, AUC: 0.831588\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015623956918716432, AUC: 0.8068925\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931769251823425, AUC: 0.509471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006451610028743743, AUC: 0.8464630000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012270017266273498, AUC: 0.8370259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013078323006629944, AUC: 0.864949\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006944571435451508, AUC: 0.445826\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008669923841953278, AUC: 0.8259850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009103761315345764, AUC: 0.8520369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009062850177288056, AUC: 0.8787030000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006883836388587952, AUC: 0.6776545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007697112262248993, AUC: 0.839264\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011312421560287476, AUC: 0.836353\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015704461336135864, AUC: 0.812537\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000699282020330429, AUC: 0.30072699999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007561590373516083, AUC: 0.8454740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398472785949707, AUC: 0.782464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001031688392162323, AUC: 0.865548\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006867203414440155, AUC: 0.7741625000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007571307122707367, AUC: 0.8070809999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008658646643161774, AUC: 0.851773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001038480818271637, AUC: 0.8632489999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006985587179660798, AUC: 0.4241545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007187469601631165, AUC: 0.8148850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008026745319366455, AUC: 0.847086\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012267091274261474, AUC: 0.839636\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006906501948833466, AUC: 0.6274955\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006881701350212097, AUC: 0.8297220000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000837628573179245, AUC: 0.8645340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010912498831748961, AUC: 0.8570349999999998\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931628286838532, AUC: 0.517671\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000721651703119278, AUC: 0.84447\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012107672691345215, AUC: 0.854232\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001070845365524292, AUC: 0.8673435\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006956841349601746, AUC: 0.4125635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00058995121717453, AUC: 0.7727959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006098356246948242, AUC: 0.80277\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006457704603672027, AUC: 0.810598\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006939020454883576, AUC: 0.519663\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275418400764466, AUC: 0.793617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005780983865261078, AUC: 0.807573\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005679846704006195, AUC: 0.817343\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006914187371730805, AUC: 0.576268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006320301592350006, AUC: 0.7865439999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005776986479759216, AUC: 0.799199\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006131931841373443, AUC: 0.8158160000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007014695405960083, AUC: 0.2262525\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006065096855163574, AUC: 0.7679549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006076382398605346, AUC: 0.782258\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623035877943039, AUC: 0.7969729999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007048844397068023, AUC: 0.2643715\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006356420814990997, AUC: 0.7727170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00057607901096344, AUC: 0.7863129999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005922649502754212, AUC: 0.805336\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006923337280750275, AUC: 0.5283145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00060067218542099, AUC: 0.7647740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006089491248130798, AUC: 0.7924229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006275667548179627, AUC: 0.8096549999999998\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007001697123050689, AUC: 0.291281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005993172526359558, AUC: 0.768286\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005809304416179657, AUC: 0.796477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005988768637180329, AUC: 0.8078699999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006949147582054139, AUC: 0.418945\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006285334825515747, AUC: 0.768832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005816346108913422, AUC: 0.794821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006180701851844787, AUC: 0.807817\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006933906674385071, AUC: 0.5035585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005996799767017364, AUC: 0.8007094999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005933422446250915, AUC: 0.804215\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006146812736988067, AUC: 0.8228629999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006925413310527801, AUC: 0.5801085\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005885217487812042, AUC: 0.7805460000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005701355338096619, AUC: 0.8124170000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005963357090950012, AUC: 0.8258099999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931113302707672, AUC: 0.5052620000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006038292646408081, AUC: 0.8442369999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009694813191890717, AUC: 0.871629\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016370036602020265, AUC: 0.834625\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000694307565689087, AUC: 0.5696005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012186483740806579, AUC: 0.832325\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00117341548204422, AUC: 0.863417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014034479260444641, AUC: 0.851683\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006916224658489227, AUC: 0.5674680000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010546446442604065, AUC: 0.846184\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010310733914375306, AUC: 0.8756970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012629610896110535, AUC: 0.8824119999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000692455530166626, AUC: 0.542134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010450857281684876, AUC: 0.844635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011799923181533814, AUC: 0.8730639999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001465002477169037, AUC: 0.872712\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000696535885334015, AUC: 0.43334100000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011155704259872437, AUC: 0.830578\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008758077919483185, AUC: 0.85151\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014336732625961303, AUC: 0.872911\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006861005127429962, AUC: 0.753309\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008633033633232116, AUC: 0.867355\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012579433917999268, AUC: 0.8600350000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011751952171325684, AUC: 0.863558\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006959941685199737, AUC: 0.4150395\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009759220480918884, AUC: 0.849207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010458824634552001, AUC: 0.8547909999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008734932541847229, AUC: 0.774424\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006905612051486969, AUC: 0.618195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010072756111621857, AUC: 0.8289580000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012035175561904907, AUC: 0.8549620000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011508205533027648, AUC: 0.8504689999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006850793659687042, AUC: 0.7759304999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107933908700943, AUC: 0.8351950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012307174801826476, AUC: 0.8374840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001549645185470581, AUC: 0.8352599999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006924456655979156, AUC: 0.5842160000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006634254157543183, AUC: 0.874015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000829733669757843, AUC: 0.874533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013509661555290223, AUC: 0.861971\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006873879432678223, AUC: 0.668617\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007288475334644317, AUC: 0.83084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000798613578081131, AUC: 0.8603899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010380207896232604, AUC: 0.861265\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006963967382907867, AUC: 0.34070750000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007012554407119751, AUC: 0.813416\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008462599813938141, AUC: 0.8528180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006368292272090912, AUC: 0.868417\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006908078789710999, AUC: 0.6734819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006440731883049011, AUC: 0.8425530000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001008029878139496, AUC: 0.8353280000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021165251731873, AUC: 0.8578000000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006938815414905548, AUC: 0.4701365\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006398897767066955, AUC: 0.823039\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009389999508857727, AUC: 0.8418840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012309846878051757, AUC: 0.839351\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006928956806659699, AUC: 0.5152815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006695924997329711, AUC: 0.836996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000713811069726944, AUC: 0.869234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009120012223720551, AUC: 0.884378\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006952646076679229, AUC: 0.454125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007137651145458221, AUC: 0.819804\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005585785508155823, AUC: 0.794235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010339073836803437, AUC: 0.8582479999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006922257542610168, AUC: 0.593245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006762655973434448, AUC: 0.840071\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009531191289424896, AUC: 0.8400770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009395413696765899, AUC: 0.8632420000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006843304336071014, AUC: 0.754032\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007121754586696625, AUC: 0.8400759999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007208648920059204, AUC: 0.879004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008298051655292511, AUC: 0.8793030000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006933661103248597, AUC: 0.539899\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007351889312267304, AUC: 0.838316\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008324334919452667, AUC: 0.855024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009299099445343017, AUC: 0.856663\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007029947936534882, AUC: 0.2404185\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007611706256866455, AUC: 0.838189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000869436115026474, AUC: 0.852241\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012457705140113831, AUC: 0.848675\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006937456727027893, AUC: 0.48097750000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006296309232711792, AUC: 0.8038609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005941517353057861, AUC: 0.827549\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005852521359920502, AUC: 0.8455324999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006981613039970398, AUC: 0.35741700000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006273975074291229, AUC: 0.731024\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006015610098838806, AUC: 0.759389\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006258268058300018, AUC: 0.7851140000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006868114173412323, AUC: 0.6472709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006055816113948822, AUC: 0.7690710000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006437261402606965, AUC: 0.7876675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006609744429588318, AUC: 0.796758\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006866777837276458, AUC: 0.678727\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000609272301197052, AUC: 0.7902450000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005601440370082855, AUC: 0.8025740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005625659823417663, AUC: 0.8274110000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.000690476804971695, AUC: 0.639091\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006030465364456176, AUC: 0.76774\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005807923972606659, AUC: 0.8022050000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006262666881084442, AUC: 0.8161539999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007070432305335999, AUC: 0.2567175\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006206088662147522, AUC: 0.770519\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005693963170051575, AUC: 0.794623\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005815098285675049, AUC: 0.8173400000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006920393109321594, AUC: 0.585516\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006010463535785675, AUC: 0.7736589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005691076517105102, AUC: 0.7999640000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005890714228153228, AUC: 0.8220719999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006970245838165283, AUC: 0.39544100000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006008437871932984, AUC: 0.7631329999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005794459283351898, AUC: 0.790374\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005711245238780975, AUC: 0.8116605\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006910800039768219, AUC: 0.584476\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005670577585697174, AUC: 0.800147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005715482532978058, AUC: 0.82222\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006515784859657288, AUC: 0.8217369999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006903544068336487, AUC: 0.6095495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006156922280788422, AUC: 0.7991219999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000538288563489914, AUC: 0.8264360000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005560230016708374, AUC: 0.842511\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006966787576675415, AUC: 0.3983555\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007222783267498016, AUC: 0.869363\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010071758329868318, AUC: 0.8470485000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012444838881492614, AUC: 0.867406\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006997887194156646, AUC: 0.35265399999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009627985954284668, AUC: 0.836147\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008971806466579437, AUC: 0.853533\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018746265172958374, AUC: 0.829051\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006894789934158325, AUC: 0.6815665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009019197821617126, AUC: 0.855526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008880127966403961, AUC: 0.875398\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011336479783058166, AUC: 0.887485\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006959426403045654, AUC: 0.37305950000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008238074481487274, AUC: 0.8587229999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009622399508953095, AUC: 0.867203\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014156073331832886, AUC: 0.8529399999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006926114559173584, AUC: 0.5274565\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008006506264209747, AUC: 0.837974\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010012348294258118, AUC: 0.8614569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001378297448158264, AUC: 0.868375\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007036321759223938, AUC: 0.276433\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006420859694480897, AUC: 0.8778259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007337818443775177, AUC: 0.8765740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013255744576454163, AUC: 0.847181\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006944135427474975, AUC: 0.474867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009478649497032165, AUC: 0.805874\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001002079963684082, AUC: 0.872723\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017056990265846253, AUC: 0.824271\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006942294836044311, AUC: 0.467659\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006910409927368164, AUC: 0.8614120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001030386745929718, AUC: 0.8703960000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001048705518245697, AUC: 0.8718370000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006894182562828064, AUC: 0.66474\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005513716340065002, AUC: 0.8781209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007959308028221131, AUC: 0.8807670000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012150654196739197, AUC: 0.872276\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006941581666469574, AUC: 0.4895235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009898362159729004, AUC: 0.8593590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012273207306861878, AUC: 0.8441259999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014980888962745666, AUC: 0.8405285\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006946149468421936, AUC: 0.431736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007163270711898804, AUC: 0.8414879999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001023943543434143, AUC: 0.852204\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000873123973608017, AUC: 0.8733000000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006913607120513916, AUC: 0.5953550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006977505087852478, AUC: 0.830875\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009022049903869629, AUC: 0.8551\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008309317231178283, AUC: 0.869514\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006998177468776703, AUC: 0.32885600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006329614818096161, AUC: 0.844735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007288149893283845, AUC: 0.8794120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008934969007968902, AUC: 0.880472\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006871189177036285, AUC: 0.647358\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006863475143909454, AUC: 0.836021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008715020120143891, AUC: 0.861773\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008677043914794922, AUC: 0.873766\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006948215365409851, AUC: 0.44974300000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006209433376789094, AUC: 0.859944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011180386543273926, AUC: 0.8403849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008133378028869629, AUC: 0.876189\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006891251504421235, AUC: 0.614881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007206940054893493, AUC: 0.822441\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000845925122499466, AUC: 0.8559945000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010710387229919433, AUC: 0.867974\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006960428357124329, AUC: 0.439736\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005857323706150055, AUC: 0.841833\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009211321473121643, AUC: 0.846067\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010535757541656494, AUC: 0.8533420000000002\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006958933174610138, AUC: 0.37558400000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007239218056201935, AUC: 0.830268\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007279396951198578, AUC: 0.863995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001042657732963562, AUC: 0.8694850000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006966108083724976, AUC: 0.4870975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007084191739559173, AUC: 0.813611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001110625147819519, AUC: 0.820113\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008412255644798279, AUC: 0.878032\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006922284066677093, AUC: 0.5504545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006489351093769074, AUC: 0.831129\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011578829884529114, AUC: 0.8231729999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010449389815330506, AUC: 0.857415\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006993962824344635, AUC: 0.307477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005656020939350128, AUC: 0.8104359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005565700232982635, AUC: 0.817501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005500772297382354, AUC: 0.8309770000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006847118139266968, AUC: 0.717801\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006017836928367614, AUC: 0.755253\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006175267100334168, AUC: 0.784343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006482227146625518, AUC: 0.799193\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006961579024791718, AUC: 0.41039400000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006349701881408691, AUC: 0.7525470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005696538984775543, AUC: 0.798821\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005563447773456573, AUC: 0.827493\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007002742290496826, AUC: 0.36772\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005938598811626435, AUC: 0.7970849999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005595190823078156, AUC: 0.8116250000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005830189883708954, AUC: 0.823024\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006892826557159424, AUC: 0.685903\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006198147535324096, AUC: 0.7771329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005677554905414581, AUC: 0.791501\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005808756649494172, AUC: 0.8144760000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006931638419628144, AUC: 0.506908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005789222419261932, AUC: 0.8014009999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005544031262397766, AUC: 0.8228660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005623725652694702, AUC: 0.83096\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006979638040065765, AUC: 0.33687649999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005897727608680726, AUC: 0.8049669999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005656885504722595, AUC: 0.8137570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005767394602298737, AUC: 0.82622\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000693691611289978, AUC: 0.49163749999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000623263955116272, AUC: 0.729361\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006053521037101745, AUC: 0.777497\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006119861006736756, AUC: 0.8077430000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006916729807853699, AUC: 0.665225\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000604123443365097, AUC: 0.7826569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000572631984949112, AUC: 0.804999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838558971881867, AUC: 0.8272949999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000702115386724472, AUC: 0.256207\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006290761232376099, AUC: 0.772369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005777866840362549, AUC: 0.7912410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000571781724691391, AUC: 0.817149\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000689954161643982, AUC: 0.6224354999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006696927547454834, AUC: 0.8391649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010561627745628358, AUC: 0.8624040000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011525577306747436, AUC: 0.8789560000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000687600165605545, AUC: 0.6463235\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001127226710319519, AUC: 0.830285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011954184770584107, AUC: 0.8479339999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013237202167510986, AUC: 0.8752679999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006991429030895233, AUC: 0.27942599999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008747574388980866, AUC: 0.847294\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010428960919380188, AUC: 0.8798949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001121375322341919, AUC: 0.8715649999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006881274878978729, AUC: 0.7000775\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008347879946231842, AUC: 0.8535305000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009024002850055694, AUC: 0.8643719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001316437244415283, AUC: 0.8608529999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006976059079170227, AUC: 0.4164485\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000802900105714798, AUC: 0.859806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010028988122940063, AUC: 0.871806\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001041642963886261, AUC: 0.869907\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006893242597579956, AUC: 0.5960265\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007124507427215576, AUC: 0.847674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001251802146434784, AUC: 0.8570409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013170126080513, AUC: 0.8638170000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006925864517688752, AUC: 0.5448594999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009432995319366455, AUC: 0.8400119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012971652150154114, AUC: 0.8591899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013174834847450257, AUC: 0.8662359999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006914799213409424, AUC: 0.601997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009482887387275696, AUC: 0.8457029999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013148759603500365, AUC: 0.8533959999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011871318817138672, AUC: 0.869723\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006934074759483337, AUC: 0.49730749999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009023742377758026, AUC: 0.849256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006607531905174255, AUC: 0.8573689999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001261540174484253, AUC: 0.8524685000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690977931022644, AUC: 0.595788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001017330527305603, AUC: 0.828994\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014169846773147584, AUC: 0.815164\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00099807870388031, AUC: 0.8697720000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss w/ average \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "cap_aucs = []\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "cap_calc = loss_fns.TripletLossWithAverage\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    learning_rate_aucs = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=cap_calc, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "        \n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "        \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote_average\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7278387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a37a386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006953137814998627, AUC: 0.41351000000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012100113034248352, AUC: 0.751849\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001347662389278412, AUC: 0.779907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013431642651557923, AUC: 0.8068489999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000698688566684723, AUC: 0.35026250000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011922909617424011, AUC: 0.7699495000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001296019196510315, AUC: 0.8160130000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014232771992683411, AUC: 0.8320620000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006938562393188477, AUC: 0.47384050000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011157945990562439, AUC: 0.7417819999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013826609253883362, AUC: 0.776867\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012448267340660095, AUC: 0.806332\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000687132328748703, AUC: 0.732907\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013641536831855774, AUC: 0.7088535000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014020829200744628, AUC: 0.7531479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014276145100593567, AUC: 0.771426\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007051105201244354, AUC: 0.322938\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012302497029304505, AUC: 0.7643880000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013913211822509765, AUC: 0.8029550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013240280747413636, AUC: 0.8284419999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006952855587005615, AUC: 0.485296\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012617493271827698, AUC: 0.7613989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001308695673942566, AUC: 0.806971\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014748266935348511, AUC: 0.815221\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006956429183483123, AUC: 0.476747\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011212791800498962, AUC: 0.774918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011230679154396058, AUC: 0.825895\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001202311098575592, AUC: 0.838972\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006950486600399017, AUC: 0.48097199999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011566267609596252, AUC: 0.79585\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001257402002811432, AUC: 0.818791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012450679540634156, AUC: 0.844643\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007013601660728454, AUC: 0.330824\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011272501945495605, AUC: 0.7583740000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013851812481880188, AUC: 0.788767\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013489682078361512, AUC: 0.8201449999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007009153962135315, AUC: 0.24535099999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012266036868095397, AUC: 0.7551239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014567877650260926, AUC: 0.787479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014886544346809387, AUC: 0.8071490000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006931928098201751, AUC: 0.5002415\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014012029767036438, AUC: 0.649479\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013114124536514281, AUC: 0.693584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012299007773399354, AUC: 0.7121620000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0007101229131221771, AUC: 0.2119795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014262165427207946, AUC: 0.616976\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014142146110534667, AUC: 0.6947409999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011929526925086975, AUC: 0.720495\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006943486928939819, AUC: 0.47322600000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001498607873916626, AUC: 0.60515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013810985684394837, AUC: 0.6721889999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012916117310523988, AUC: 0.6959690000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006934674978256225, AUC: 0.49733550000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015681596398353576, AUC: 0.549493\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001625352680683136, AUC: 0.65735\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001388828694820404, AUC: 0.699155\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006907029747962952, AUC: 0.5882499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014448300004005431, AUC: 0.607761\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013494152426719666, AUC: 0.6913860000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011988258361816406, AUC: 0.715993\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006981097161769867, AUC: 0.360881\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014119338393211364, AUC: 0.656449\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013790208697319031, AUC: 0.687104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013106749653816223, AUC: 0.7021760000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006989958882331848, AUC: 0.4668105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001303554654121399, AUC: 0.682885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012563939094543458, AUC: 0.707837\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012131325006484986, AUC: 0.7160259999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006810896396636963, AUC: 0.7282065\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017621628642082215, AUC: 0.254372\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019957852959632872, AUC: 0.526392\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016850491166114808, AUC: 0.6527689999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000695532888174057, AUC: 0.4225245\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001594462215900421, AUC: 0.567238\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001725798487663269, AUC: 0.651816\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001516685664653778, AUC: 0.686813\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006976980865001679, AUC: 0.3722295\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001732854187488556, AUC: 0.531613\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018189577460289, AUC: 0.639831\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015425624847412109, AUC: 0.685395\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006946506798267365, AUC: 0.45809099999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010704345107078552, AUC: 0.827189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012367932200431824, AUC: 0.8512869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011861377358436584, AUC: 0.8723659999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006960716247558594, AUC: 0.40448150000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010685625076293946, AUC: 0.821029\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015051649808883666, AUC: 0.8280540000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012095825672149658, AUC: 0.859088\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006914547681808471, AUC: 0.5743914999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013932805061340331, AUC: 0.806651\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013861716389656066, AUC: 0.843584\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011066436171531678, AUC: 0.8695819999999999\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006953510642051697, AUC: 0.44617850000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001377259373664856, AUC: 0.813183\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011593254208564759, AUC: 0.8457060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015679771900177003, AUC: 0.840825\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006900211274623871, AUC: 0.6015820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014498870372772216, AUC: 0.80487\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016269005537033082, AUC: 0.8309340000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001314681053161621, AUC: 0.862953\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.000698762983083725, AUC: 0.298201\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010476409792900085, AUC: 0.8333949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015019388198852538, AUC: 0.857112\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012248939275741578, AUC: 0.8620990000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006974970698356628, AUC: 0.30638350000000003\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012795483469963073, AUC: 0.8072629999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016676595211029053, AUC: 0.8269390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014754130244255066, AUC: 0.8483880000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006972302198410034, AUC: 0.3896045\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013244966864585875, AUC: 0.810084\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015768024921417236, AUC: 0.822819\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010522279143333436, AUC: 0.865938\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006987023949623108, AUC: 0.3143995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010556061267852784, AUC: 0.829197\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001492241322994232, AUC: 0.8176049999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001694005012512207, AUC: 0.8349770000000001\n",
      "\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.0006853301227092743, AUC: 0.7056475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014562657475471496, AUC: 0.7940020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001479567587375641, AUC: 0.808318\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013639997839927673, AUC: 0.8473469999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006961835324764252, AUC: 0.38450400000000007\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012881333231925965, AUC: 0.775539\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014020306468009948, AUC: 0.789855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013349701166152953, AUC: 0.805526\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006926476657390595, AUC: 0.5199125\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001334111452102661, AUC: 0.740914\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001407979667186737, AUC: 0.787825\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0012890581488609314, AUC: 0.818852\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006957947611808777, AUC: 0.4403390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013087846636772155, AUC: 0.7569419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014033986926078796, AUC: 0.7918499999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011560410261154176, AUC: 0.82701\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006878388524055481, AUC: 0.7391975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001277011215686798, AUC: 0.733312\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013133590817451476, AUC: 0.7746850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013910502791404725, AUC: 0.7918069999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006964699029922485, AUC: 0.3996565000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001165420174598694, AUC: 0.743013\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013641301393508911, AUC: 0.789986\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014104530215263366, AUC: 0.8106280000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006948442161083221, AUC: 0.46871799999999997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012675276398658752, AUC: 0.740739\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013235929608345033, AUC: 0.790343\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00137513667345047, AUC: 0.805365\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006947003304958344, AUC: 0.424541\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00122288978099823, AUC: 0.748015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013375158905982972, AUC: 0.782779\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001588438630104065, AUC: 0.798579\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006849124431610108, AUC: 0.694234\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012758437395095825, AUC: 0.774427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012920546531677247, AUC: 0.8121820000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012923317551612854, AUC: 0.8296669999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006977393925189972, AUC: 0.36523649999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001291788637638092, AUC: 0.737293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013498828411102296, AUC: 0.7949120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014673247933387756, AUC: 0.8174250000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006943438947200775, AUC: 0.4504785\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011255612969398498, AUC: 0.768319\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012107766270637512, AUC: 0.80548\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013772826790809632, AUC: 0.813829\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006917154788970947, AUC: 0.5690235000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013541932702064515, AUC: 0.598478\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012371926307678223, AUC: 0.688711\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010923356413841247, AUC: 0.723258\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006968607604503632, AUC: 0.3831825\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015744782090187073, AUC: 0.5828990000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014835953712463378, AUC: 0.665832\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012981435656547547, AUC: 0.6980459999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006955974400043488, AUC: 0.43256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014091846942901611, AUC: 0.645457\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001369967758655548, AUC: 0.683918\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013020054697990416, AUC: 0.7027850000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007000738978385925, AUC: 0.3881845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016745031476020813, AUC: 0.57461\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0016251876950263977, AUC: 0.659213\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013794299364089965, AUC: 0.6950369999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006960032284259796, AUC: 0.4766595\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001597978949546814, AUC: 0.5385105\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015363540053367614, AUC: 0.6627500000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012637528777122498, AUC: 0.704556\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006979741156101227, AUC: 0.32655\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001522919237613678, AUC: 0.553407\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015850431323051453, AUC: 0.659846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013161882162094115, AUC: 0.6960270000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006918782293796539, AUC: 0.559968\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012581725716590882, AUC: 0.679128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001229236662387848, AUC: 0.710427\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011915088295936585, AUC: 0.724267\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006963012814521789, AUC: 0.36674\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014103291034698487, AUC: 0.610051\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013099029064178468, AUC: 0.704035\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011947248578071594, AUC: 0.7228699999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006930817067623138, AUC: 0.55144\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00126514995098114, AUC: 0.663859\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001268758773803711, AUC: 0.6927850000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011619875431060792, AUC: 0.7084585000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006888798475265503, AUC: 0.6461855000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012820826768875121, AUC: 0.685115\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012846903204917907, AUC: 0.709021\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012085999846458435, AUC: 0.723304\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006979137063026429, AUC: 0.32278750000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00133148592710495, AUC: 0.820056\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013782886266708375, AUC: 0.858347\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00192118239402771, AUC: 0.85137\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006899331212043762, AUC: 0.657114\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012319467067718505, AUC: 0.8096279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012445133924484254, AUC: 0.847517\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001337042450904846, AUC: 0.847077\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006820880770683289, AUC: 0.710675\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012821065187454224, AUC: 0.816725\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012086466550827026, AUC: 0.8384739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012879958152770996, AUC: 0.850204\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006903654634952545, AUC: 0.593124\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001118379533290863, AUC: 0.8279394999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014810474514961242, AUC: 0.8571609999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013770051002502442, AUC: 0.854878\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0007069114446640014, AUC: 0.242749\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014144913554191589, AUC: 0.8204989999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001513637661933899, AUC: 0.8455180000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001306992769241333, AUC: 0.8610800000000001\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006921878755092621, AUC: 0.5970345\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013711398243904114, AUC: 0.7955110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015437301397323608, AUC: 0.8171569999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012172499895095825, AUC: 0.8463309999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006925551295280457, AUC: 0.5933379999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014085525274276733, AUC: 0.78868\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013914011716842651, AUC: 0.826568\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012870569229125977, AUC: 0.854255\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006961119771003723, AUC: 0.38871350000000005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008103238046169282, AUC: 0.858504\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000931158035993576, AUC: 0.8774884999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013895972967147827, AUC: 0.8711614999999999\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006905738711357116, AUC: 0.644471\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012654099464416504, AUC: 0.7992060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011641114950180053, AUC: 0.843349\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015687112212181091, AUC: 0.84473\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Avg. loss: 0.0006835061609745026, AUC: 0.7417645\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013023046255111693, AUC: 0.825409\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014262330532073975, AUC: 0.8375239999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012669787406921387, AUC: 0.852042\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000697836846113205, AUC: 0.37008850000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011396661400794982, AUC: 0.770827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013007211685180664, AUC: 0.811997\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012499591708183288, AUC: 0.815539\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006927756369113922, AUC: 0.518756\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011860894560813903, AUC: 0.7321660000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001197644829750061, AUC: 0.792614\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012735504508018494, AUC: 0.824929\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690514475107193, AUC: 0.6386790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001202582597732544, AUC: 0.767078\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001092694878578186, AUC: 0.817475\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011756229400634766, AUC: 0.840884\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006964456140995025, AUC: 0.36736450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001126945436000824, AUC: 0.7768120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001062393069267273, AUC: 0.8200779999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012183101773262023, AUC: 0.8248995\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006926549971103668, AUC: 0.532581\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011874104738235473, AUC: 0.76892\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012728224992752076, AUC: 0.8023020000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012980905175209046, AUC: 0.826338\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007008664906024933, AUC: 0.3223985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012494014501571655, AUC: 0.74944\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012630877494812013, AUC: 0.7997829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013315749168395995, AUC: 0.8174549999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006991316080093384, AUC: 0.33853449999999996\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011583274602890014, AUC: 0.7944675000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012185652256011964, AUC: 0.824557\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001334098756313324, AUC: 0.8564489999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006847937405109406, AUC: 0.705627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011758599281311035, AUC: 0.7457389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012690225839614869, AUC: 0.793452\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013103622794151306, AUC: 0.813781\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006925979852676392, AUC: 0.57975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011308208107948304, AUC: 0.750285\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012988250255584718, AUC: 0.7969920000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011933872103691101, AUC: 0.824703\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000695866584777832, AUC: 0.42814450000000004\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001182684600353241, AUC: 0.750788\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001265819251537323, AUC: 0.814612\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001408158242702484, AUC: 0.8340939999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006949114203453064, AUC: 0.45558\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014354689121246337, AUC: 0.5758650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013175629377365113, AUC: 0.675001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001132315695285797, AUC: 0.706466\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000690006971359253, AUC: 0.6346470000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013463624715805054, AUC: 0.507289\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012996394634246825, AUC: 0.6847089999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001164216935634613, AUC: 0.7226600000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000693820834159851, AUC: 0.471494\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013598218560218812, AUC: 0.5849859999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014249856472015382, AUC: 0.669624\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001213870644569397, AUC: 0.704044\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000694388210773468, AUC: 0.4512545\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015957908034324646, AUC: 0.6283139999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015082186460494995, AUC: 0.693443\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012417213916778565, AUC: 0.7187049999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006957093477249146, AUC: 0.38944199999999995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000748483657836914, AUC: 0.582685\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010371949672698974, AUC: 0.700732\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010211527645587921, AUC: 0.7242780000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006927612721920013, AUC: 0.5246435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011021042466163636, AUC: 0.674104\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010914167761802674, AUC: 0.7199840000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001138194501399994, AUC: 0.738764\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006960340440273284, AUC: 0.6067405\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001342705488204956, AUC: 0.590044\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013636059165000915, AUC: 0.6818770000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012205939888954164, AUC: 0.710601\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006931335330009461, AUC: 0.5170950000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013164325952529908, AUC: 0.626445\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001208702564239502, AUC: 0.710852\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011207187175750733, AUC: 0.734961\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006902595460414886, AUC: 0.606362\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001275633454322815, AUC: 0.6548795\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012172811031341553, AUC: 0.7008749999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011490079164505004, AUC: 0.7142949999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006903997659683228, AUC: 0.6133195\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011215248703956603, AUC: 0.6702289999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011986317038536073, AUC: 0.7079549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011585350036621093, AUC: 0.7373209999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006897677779197693, AUC: 0.6272169999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013875076174736023, AUC: 0.811742\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012797714471817017, AUC: 0.850464\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015719639658927918, AUC: 0.8589289999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006917991638183594, AUC: 0.5485805\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013168479800224305, AUC: 0.816217\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014584704637527466, AUC: 0.8348270000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014950295686721802, AUC: 0.848802\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006945172548294068, AUC: 0.4559815\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012278074026107787, AUC: 0.828437\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010732338428497315, AUC: 0.845\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011716055870056152, AUC: 0.846575\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006956190168857574, AUC: 0.4005905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011252890825271607, AUC: 0.8047855\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012148600220680237, AUC: 0.841369\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001485383629798889, AUC: 0.8348699999999999\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006944859623908996, AUC: 0.46946450000000006\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014134332537651061, AUC: 0.806386\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011645344495773316, AUC: 0.8577410000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015893993973731995, AUC: 0.8401670000000001\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000698531299829483, AUC: 0.3484005\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012929802536964417, AUC: 0.8213739999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014258021712303163, AUC: 0.832266\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017997503876686096, AUC: 0.828209\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006885204911231995, AUC: 0.683658\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001426667034626007, AUC: 0.8046515\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.001412131667137146, AUC: 0.8346359999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011856539249420167, AUC: 0.837064\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.000691462904214859, AUC: 0.6056925\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013597025275230408, AUC: 0.824577\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014291183352470398, AUC: 0.856176\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015407857894897462, AUC: 0.843618\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0007023155391216278, AUC: 0.270157\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012542378306388854, AUC: 0.8123429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018739871978759766, AUC: 0.817189\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0015053725242614746, AUC: 0.834164\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Avg. loss: 0.0006912327408790588, AUC: 0.64205\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011190905570983886, AUC: 0.82108\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0013176281452178956, AUC: 0.862773\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014106485247612, AUC: 0.85267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# capped smote using triplet loss w/ positive dist squared \n",
    "\n",
    "momentum=0\n",
    "learning_rates = [5e-3, 1e-3, 1e-2]\n",
    "\n",
    "loss_fn_args = {}\n",
    "\n",
    "\n",
    "start_epoch = 2\n",
    "\n",
    "loss_caps = [1, 5, 10]\n",
    "cap_calc = loss_fns.TripletLoss\n",
    "\n",
    "cap_aucs = []\n",
    "\n",
    "\n",
    "for loss_cap in loss_caps:\n",
    "    learning_rate_aucs = []\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            print(loss_cap)\n",
    "            model_aucs = []\n",
    "            network = models.ConvNetWithEmbeddings(2)\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                loss_fn_args['loss_cap'] = None\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, network, optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            for epoch in range(start_epoch, n_epochs + 1):\n",
    "                loss_fn_args['loss_cap'] = loss_cap\n",
    "                _, _ = train.train_triplet_capped_loss(epoch, train_loader_tripletloss_smote, network, optimizer, verbose=False, cap_calc=cap_calc, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, network, embeddings=True)\n",
    "                    model_aucs.append(auc)\n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    \n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"triplet_loss_capped_smote_pos_squared\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], cap, norm]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f81e35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1276f1a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.011970190048217774, AUC: 0.471541\n",
      "\n",
      "Train loss: Avg. loss: 1149.4229728552946\n",
      "\n",
      "Test set: Avg. loss: 955.8448486328125\n",
      "Train loss: Avg. loss: 822.1455412457703\n",
      "\n",
      "Test set: Avg. loss: 921.1502685546875\n",
      "Train loss: Avg. loss: 705.6538779872238\n",
      "\n",
      "Test set: Avg. loss: 940.68798828125\n",
      "Train loss: Avg. loss: 630.0720074890526\n",
      "\n",
      "Test set: Avg. loss: 930.7140808105469\n",
      "Train loss: Avg. loss: 588.9198482051777\n",
      "\n",
      "Test set: Avg. loss: 901.2696838378906\n",
      "Train loss: Avg. loss: 562.3851232832404\n",
      "\n",
      "Test set: Avg. loss: 890.3906555175781\n",
      "Train loss: Avg. loss: 532.2037812251194\n",
      "\n",
      "Test set: Avg. loss: 880.769287109375\n",
      "Train loss: Avg. loss: 511.3853182458574\n",
      "\n",
      "Test set: Avg. loss: 920.7016906738281\n",
      "Train loss: Avg. loss: 471.581619748644\n",
      "\n",
      "Test set: Avg. loss: 855.3821411132812\n",
      "Train loss: Avg. loss: 462.1116079342593\n",
      "\n",
      "Test set: Avg. loss: 846.2129516601562\n",
      "Train loss: Avg. loss: 434.8682642651212\n",
      "\n",
      "Test set: Avg. loss: 944.2405700683594\n",
      "Train loss: Avg. loss: 429.6245647843476\n",
      "\n",
      "Test set: Avg. loss: 964.2573852539062\n",
      "Train loss: Avg. loss: 410.71695772401847\n",
      "\n",
      "Test set: Avg. loss: 932.6674499511719\n",
      "Train loss: Avg. loss: 399.32670748765304\n",
      "\n",
      "Test set: Avg. loss: 931.6121826171875\n",
      "Train loss: Avg. loss: 376.88346853073995\n",
      "\n",
      "Test set: Avg. loss: 882.996826171875\n",
      "Train loss: Avg. loss: 356.74724107487185\n",
      "\n",
      "Test set: Avg. loss: 875.4489440917969\n",
      "Train loss: Avg. loss: 364.9263662350406\n",
      "\n",
      "Test set: Avg. loss: 920.4262084960938\n",
      "Train loss: Avg. loss: 342.5292890026311\n",
      "\n",
      "Test set: Avg. loss: 942.4323120117188\n",
      "Train loss: Avg. loss: 342.06359474522293\n",
      "\n",
      "Test set: Avg. loss: 948.2861328125\n",
      "Train loss: Avg. loss: 341.24466719900727\n",
      "\n",
      "Test set: Avg. loss: 913.5072631835938\n",
      "\n",
      "Test set: Avg. loss: 0.0006430117189884186, AUC: 0.8400940000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006327671706676483, AUC: 0.8484389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006205790638923645, AUC: 0.8570110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006164615750312805, AUC: 0.8579939999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006097684502601624, AUC: 0.86059\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0014243626594543457, AUC: 0.47868449999999996\n",
      "\n",
      "Train loss: Avg. loss: 778.8356488464744\n",
      "\n",
      "Test set: Avg. loss: 813.9452209472656\n",
      "Train loss: Avg. loss: 598.2735352728777\n",
      "\n",
      "Test set: Avg. loss: 917.18994140625\n",
      "Train loss: Avg. loss: 538.1481257153165\n",
      "\n",
      "Test set: Avg. loss: 881.706787109375\n",
      "Train loss: Avg. loss: 497.78240850169186\n",
      "\n",
      "Test set: Avg. loss: 941.0055541992188\n",
      "Train loss: Avg. loss: 460.05006943234974\n",
      "\n",
      "Test set: Avg. loss: 895.4686279296875\n",
      "Train loss: Avg. loss: 430.99582579788887\n",
      "\n",
      "Test set: Avg. loss: 857.6063232421875\n",
      "Train loss: Avg. loss: 428.8692037983305\n",
      "\n",
      "Test set: Avg. loss: 910.7769165039062\n",
      "Train loss: Avg. loss: 391.657039083493\n",
      "\n",
      "Test set: Avg. loss: 891.2506103515625\n",
      "Train loss: Avg. loss: 374.3253704484101\n",
      "\n",
      "Test set: Avg. loss: 932.5437622070312\n",
      "Train loss: Avg. loss: 359.2720556562873\n",
      "\n",
      "Test set: Avg. loss: 887.3350219726562\n",
      "Train loss: Avg. loss: 342.0217832334482\n",
      "\n",
      "Test set: Avg. loss: 957.511962890625\n",
      "Train loss: Avg. loss: 318.62396774777943\n",
      "\n",
      "Test set: Avg. loss: 884.8118591308594\n",
      "Train loss: Avg. loss: 299.49098642920234\n",
      "\n",
      "Test set: Avg. loss: 927.1674194335938\n",
      "Train loss: Avg. loss: 300.59717239088314\n",
      "\n",
      "Test set: Avg. loss: 860.4481201171875\n",
      "Train loss: Avg. loss: 284.9995609939478\n",
      "\n",
      "Test set: Avg. loss: 900.4115295410156\n",
      "Train loss: Avg. loss: 267.8200101427212\n",
      "\n",
      "Test set: Avg. loss: 878.7414855957031\n",
      "Train loss: Avg. loss: 255.785304221378\n",
      "\n",
      "Test set: Avg. loss: 900.5666198730469\n",
      "Train loss: Avg. loss: 261.69834593754666\n",
      "\n",
      "Test set: Avg. loss: 904.3453063964844\n",
      "Train loss: Avg. loss: 249.48885996630239\n",
      "\n",
      "Test set: Avg. loss: 909.7150268554688\n",
      "Train loss: Avg. loss: 238.9507792770483\n",
      "\n",
      "Test set: Avg. loss: 907.417236328125\n",
      "\n",
      "Test set: Avg. loss: 0.0006327728927135467, AUC: 0.849134\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006234646141529083, AUC: 0.8482399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006164008378982544, AUC: 0.8480200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000600941389799118, AUC: 0.8528629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000601049780845642, AUC: 0.851388\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017289690971374512, AUC: 0.380344\n",
      "\n",
      "Train loss: Avg. loss: 973.1365212604499\n",
      "\n",
      "Test set: Avg. loss: 928.2741088867188\n",
      "Train loss: Avg. loss: 672.9494918531673\n",
      "\n",
      "Test set: Avg. loss: 863.4281311035156\n",
      "Train loss: Avg. loss: 602.9939938684937\n",
      "\n",
      "Test set: Avg. loss: 785.409423828125\n",
      "Train loss: Avg. loss: 542.831009202702\n",
      "\n",
      "Test set: Avg. loss: 827.9874267578125\n",
      "Train loss: Avg. loss: 493.72089905951435\n",
      "\n",
      "Test set: Avg. loss: 869.2048034667969\n",
      "Train loss: Avg. loss: 460.49894102060114\n",
      "\n",
      "Test set: Avg. loss: 872.5003662109375\n",
      "Train loss: Avg. loss: 424.98007386808945\n",
      "\n",
      "Test set: Avg. loss: 891.0016174316406\n",
      "Train loss: Avg. loss: 387.496914517348\n",
      "\n",
      "Test set: Avg. loss: 902.6998901367188\n",
      "Train loss: Avg. loss: 363.1237156375958\n",
      "\n",
      "Test set: Avg. loss: 837.655517578125\n",
      "Train loss: Avg. loss: 355.7677761976886\n",
      "\n",
      "Test set: Avg. loss: 906.4925537109375\n",
      "Train loss: Avg. loss: 334.70267987099425\n",
      "\n",
      "Test set: Avg. loss: 890.1360473632812\n",
      "Train loss: Avg. loss: 306.5906447878309\n",
      "\n",
      "Test set: Avg. loss: 912.5223693847656\n",
      "Train loss: Avg. loss: 304.686306510002\n",
      "\n",
      "Test set: Avg. loss: 850.1275329589844\n",
      "Train loss: Avg. loss: 274.1461181640625\n",
      "\n",
      "Test set: Avg. loss: 910.6288757324219\n",
      "Train loss: Avg. loss: 265.2972767337872\n",
      "\n",
      "Test set: Avg. loss: 894.1485900878906\n",
      "Train loss: Avg. loss: 251.58453019257564\n",
      "\n",
      "Test set: Avg. loss: 910.1618041992188\n",
      "Train loss: Avg. loss: 253.90615835007588\n",
      "\n",
      "Test set: Avg. loss: 881.0578002929688\n",
      "Train loss: Avg. loss: 242.13471304535108\n",
      "\n",
      "Test set: Avg. loss: 906.6133117675781\n",
      "Train loss: Avg. loss: 222.3480453491211\n",
      "\n",
      "Test set: Avg. loss: 859.0086364746094\n",
      "Train loss: Avg. loss: 230.08833121038546\n",
      "\n",
      "Test set: Avg. loss: 935.0205688476562\n",
      "\n",
      "Test set: Avg. loss: 0.000626446932554245, AUC: 0.8357370000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006145733594894409, AUC: 0.8548279999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006152859330177307, AUC: 0.855791\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006044679582118988, AUC: 0.8652949999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006001460552215577, AUC: 0.8683899999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.003980049014091492, AUC: 0.3634775\n",
      "\n",
      "Train loss: Avg. loss: 1484.8525487814738\n",
      "\n",
      "Test set: Avg. loss: 966.4229431152344\n",
      "Train loss: Avg. loss: 795.4771188140675\n",
      "\n",
      "Test set: Avg. loss: 952.955078125\n",
      "Train loss: Avg. loss: 683.7763007097184\n",
      "\n",
      "Test set: Avg. loss: 878.4426879882812\n",
      "Train loss: Avg. loss: 634.9779942992386\n",
      "\n",
      "Test set: Avg. loss: 922.2305297851562\n",
      "Train loss: Avg. loss: 584.7494969459096\n",
      "\n",
      "Test set: Avg. loss: 930.3320617675781\n",
      "Train loss: Avg. loss: 558.4230552721934\n",
      "\n",
      "Test set: Avg. loss: 892.1585693359375\n",
      "Train loss: Avg. loss: 538.1876780516023\n",
      "\n",
      "Test set: Avg. loss: 890.9949645996094\n",
      "Train loss: Avg. loss: 524.809955378247\n",
      "\n",
      "Test set: Avg. loss: 839.3385314941406\n",
      "Train loss: Avg. loss: 487.8229576159435\n",
      "\n",
      "Test set: Avg. loss: 922.5910949707031\n",
      "Train loss: Avg. loss: 488.70114991011894\n",
      "\n",
      "Test set: Avg. loss: 925.7367858886719\n",
      "Train loss: Avg. loss: 467.39412635754627\n",
      "\n",
      "Test set: Avg. loss: 897.149658203125\n",
      "Train loss: Avg. loss: 460.8653460460104\n",
      "\n",
      "Test set: Avg. loss: 922.908935546875\n",
      "Train loss: Avg. loss: 446.36778861853725\n",
      "\n",
      "Test set: Avg. loss: 859.0227355957031\n",
      "Train loss: Avg. loss: 449.7819831993929\n",
      "\n",
      "Test set: Avg. loss: 877.6337890625\n",
      "Train loss: Avg. loss: 430.9165673711497\n",
      "\n",
      "Test set: Avg. loss: 908.852294921875\n",
      "Train loss: Avg. loss: 434.5744339280827\n",
      "\n",
      "Test set: Avg. loss: 902.9852905273438\n",
      "Train loss: Avg. loss: 408.9096528071507\n",
      "\n",
      "Test set: Avg. loss: 889.0010070800781\n",
      "Train loss: Avg. loss: 411.74437601854845\n",
      "\n",
      "Test set: Avg. loss: 879.3017272949219\n",
      "Train loss: Avg. loss: 394.67920884053416\n",
      "\n",
      "Test set: Avg. loss: 935.7184753417969\n",
      "Train loss: Avg. loss: 397.339884375311\n",
      "\n",
      "Test set: Avg. loss: 889.031982421875\n",
      "\n",
      "Test set: Avg. loss: 0.0005964651107788086, AUC: 0.8866399999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005869301557540894, AUC: 0.891293\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005755687057971955, AUC: 0.8923419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005693828761577606, AUC: 0.892653\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005652413666248321, AUC: 0.89299\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012209163308143616, AUC: 0.335669\n",
      "\n",
      "Train loss: Avg. loss: 895.3509573966834\n",
      "\n",
      "Test set: Avg. loss: 909.5339965820312\n",
      "Train loss: Avg. loss: 602.4495688274408\n",
      "\n",
      "Test set: Avg. loss: 847.3341369628906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: Avg. loss: 540.0754262353205\n",
      "\n",
      "Test set: Avg. loss: 840.2581176757812\n",
      "Train loss: Avg. loss: 484.78332577845094\n",
      "\n",
      "Test set: Avg. loss: 896.7619934082031\n",
      "Train loss: Avg. loss: 450.80659718118653\n",
      "\n",
      "Test set: Avg. loss: 936.3514404296875\n",
      "Train loss: Avg. loss: 420.15379129397644\n",
      "\n",
      "Test set: Avg. loss: 868.5693969726562\n",
      "Train loss: Avg. loss: 406.2026587808208\n",
      "\n",
      "Test set: Avg. loss: 927.0424194335938\n",
      "Train loss: Avg. loss: 394.4376327611838\n",
      "\n",
      "Test set: Avg. loss: 899.2440490722656\n",
      "Train loss: Avg. loss: 363.00736941197874\n",
      "\n",
      "Test set: Avg. loss: 873.066162109375\n",
      "Train loss: Avg. loss: 362.63898890792944\n",
      "\n",
      "Test set: Avg. loss: 863.5249633789062\n",
      "Train loss: Avg. loss: 338.4038825551416\n",
      "\n",
      "Test set: Avg. loss: 844.7805786132812\n",
      "Train loss: Avg. loss: 333.99996131848377\n",
      "\n",
      "Test set: Avg. loss: 900.2032775878906\n",
      "Train loss: Avg. loss: 318.2685418098595\n",
      "\n",
      "Test set: Avg. loss: 901.7999877929688\n",
      "Train loss: Avg. loss: 307.0509993437749\n",
      "\n",
      "Test set: Avg. loss: 906.2280578613281\n",
      "Train loss: Avg. loss: 303.446942469117\n",
      "\n",
      "Test set: Avg. loss: 879.0897521972656\n",
      "Train loss: Avg. loss: 285.17419521064517\n",
      "\n",
      "Test set: Avg. loss: 967.2149658203125\n",
      "Train loss: Avg. loss: 289.770729210726\n",
      "\n",
      "Test set: Avg. loss: 936.0834350585938\n",
      "Train loss: Avg. loss: 281.2266302898431\n",
      "\n",
      "Test set: Avg. loss: 908.5448913574219\n",
      "Train loss: Avg. loss: 279.29151615215716\n",
      "\n",
      "Test set: Avg. loss: 900.5394287109375\n",
      "Train loss: Avg. loss: 271.8057339905174\n",
      "\n",
      "Test set: Avg. loss: 926.5850524902344\n",
      "\n",
      "Test set: Avg. loss: 0.0006006447672843934, AUC: 0.8804265000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005895957350730896, AUC: 0.890793\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005831368863582611, AUC: 0.8917269999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005844570398330688, AUC: 0.8912954999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005731068551540375, AUC: 0.892145\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.008536664009094239, AUC: 0.2953945\n",
      "\n",
      "Train loss: Avg. loss: 1025.446262238132\n",
      "\n",
      "Test set: Avg. loss: 877.5339660644531\n",
      "Train loss: Avg. loss: 643.9232064994277\n",
      "\n",
      "Test set: Avg. loss: 846.046875\n",
      "Train loss: Avg. loss: 563.108371613132\n",
      "\n",
      "Test set: Avg. loss: 858.9489135742188\n",
      "Train loss: Avg. loss: 510.800487153849\n",
      "\n",
      "Test set: Avg. loss: 908.5508422851562\n",
      "Train loss: Avg. loss: 460.9350337131768\n",
      "\n",
      "Test set: Avg. loss: 938.7705688476562\n",
      "Train loss: Avg. loss: 437.7188441768573\n",
      "\n",
      "Test set: Avg. loss: 940.8458251953125\n",
      "Train loss: Avg. loss: 407.32432313178\n",
      "\n",
      "Test set: Avg. loss: 902.5734252929688\n",
      "Train loss: Avg. loss: 387.04129854433097\n",
      "\n",
      "Test set: Avg. loss: 892.7881469726562\n",
      "Train loss: Avg. loss: 363.8063095967481\n",
      "\n",
      "Test set: Avg. loss: 871.7922058105469\n",
      "Train loss: Avg. loss: 341.66576137664214\n",
      "\n",
      "Test set: Avg. loss: 815.4048767089844\n",
      "Train loss: Avg. loss: 333.03843251611016\n",
      "\n",
      "Test set: Avg. loss: 898.2102661132812\n",
      "Train loss: Avg. loss: 307.1596948903078\n",
      "\n",
      "Test set: Avg. loss: 853.1839904785156\n",
      "Train loss: Avg. loss: 307.9381727473751\n",
      "\n",
      "Test set: Avg. loss: 941.6612548828125\n",
      "Train loss: Avg. loss: 296.6100199511097\n",
      "\n",
      "Test set: Avg. loss: 848.8335876464844\n",
      "Train loss: Avg. loss: 290.658831553854\n",
      "\n",
      "Test set: Avg. loss: 890.3936157226562\n",
      "Train loss: Avg. loss: 272.7788127729088\n",
      "\n",
      "Test set: Avg. loss: 895.54052734375\n",
      "Train loss: Avg. loss: 267.08738421786364\n",
      "\n",
      "Test set: Avg. loss: 956.4490966796875\n",
      "Train loss: Avg. loss: 246.08397856791308\n",
      "\n",
      "Test set: Avg. loss: 851.5364990234375\n",
      "Train loss: Avg. loss: 243.94993061624515\n",
      "\n",
      "Test set: Avg. loss: 922.8983764648438\n",
      "Train loss: Avg. loss: 238.3142114141185\n",
      "\n",
      "Test set: Avg. loss: 853.0683898925781\n",
      "\n",
      "Test set: Avg. loss: 0.0005978721678256988, AUC: 0.881778\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005894590020179748, AUC: 0.8880345000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005838269293308258, AUC: 0.8877550000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005821603238582611, AUC: 0.8874495\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005735565721988678, AUC: 0.8873424999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0018167497515678406, AUC: 0.5175215\n",
      "\n",
      "Train loss: Avg. loss: 933.2342191076582\n",
      "\n",
      "Test set: Avg. loss: 886.9049377441406\n",
      "Train loss: Avg. loss: 647.6783247054763\n",
      "\n",
      "Test set: Avg. loss: 870.5634460449219\n",
      "Train loss: Avg. loss: 582.5612520837481\n",
      "\n",
      "Test set: Avg. loss: 893.5596008300781\n",
      "Train loss: Avg. loss: 532.6993833894182\n",
      "\n",
      "Test set: Avg. loss: 909.9246826171875\n",
      "Train loss: Avg. loss: 511.3187290847681\n",
      "\n",
      "Test set: Avg. loss: 912.0228271484375\n",
      "Train loss: Avg. loss: 479.7590613881494\n",
      "\n",
      "Test set: Avg. loss: 910.4073486328125\n",
      "Train loss: Avg. loss: 440.64306465683467\n",
      "\n",
      "Test set: Avg. loss: 905.8450622558594\n",
      "Train loss: Avg. loss: 417.8047625304787\n",
      "\n",
      "Test set: Avg. loss: 893.8045654296875\n",
      "Train loss: Avg. loss: 400.9696705812102\n",
      "\n",
      "Test set: Avg. loss: 920.8846740722656\n",
      "Train loss: Avg. loss: 400.7930506809502\n",
      "\n",
      "Test set: Avg. loss: 875.4591674804688\n",
      "Train loss: Avg. loss: 385.5520270280777\n",
      "\n",
      "Test set: Avg. loss: 894.08837890625\n",
      "Train loss: Avg. loss: 373.80442955843205\n",
      "\n",
      "Test set: Avg. loss: 884.2942199707031\n",
      "Train loss: Avg. loss: 370.8101614204941\n",
      "\n",
      "Test set: Avg. loss: 879.0103759765625\n",
      "Train loss: Avg. loss: 345.4709408511022\n",
      "\n",
      "Test set: Avg. loss: 930.5303649902344\n",
      "Train loss: Avg. loss: 350.589965625933\n",
      "\n",
      "Test set: Avg. loss: 936.9560546875\n",
      "Train loss: Avg. loss: 323.972586176198\n",
      "\n",
      "Test set: Avg. loss: 983.1318969726562\n",
      "Train loss: Avg. loss: 338.6718556592419\n",
      "\n",
      "Test set: Avg. loss: 909.621826171875\n",
      "Train loss: Avg. loss: 326.56484567435683\n",
      "\n",
      "Test set: Avg. loss: 901.9941711425781\n",
      "Train loss: Avg. loss: 316.10082667648413\n",
      "\n",
      "Test set: Avg. loss: 984.4230651855469\n",
      "Train loss: Avg. loss: 309.70682044545555\n",
      "\n",
      "Test set: Avg. loss: 915.3056335449219\n",
      "\n",
      "Test set: Avg. loss: 0.0005970433652400971, AUC: 0.8607585000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005885826349258423, AUC: 0.8714125000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005832213461399078, AUC: 0.8746665\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005781299471855164, AUC: 0.8765974999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005772077739238739, AUC: 0.8773905\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008650731444358826, AUC: 0.501306\n",
      "\n",
      "Train loss: Avg. loss: 958.0591131319666\n",
      "\n",
      "Test set: Avg. loss: 918.26953125\n",
      "Train loss: Avg. loss: 708.6816649224348\n",
      "\n",
      "Test set: Avg. loss: 878.777587890625\n",
      "Train loss: Avg. loss: 602.0753307949965\n",
      "\n",
      "Test set: Avg. loss: 919.8451232910156\n",
      "Train loss: Avg. loss: 557.4956449277842\n",
      "\n",
      "Test set: Avg. loss: 884.6618957519531\n",
      "Train loss: Avg. loss: 500.6115166730942\n",
      "\n",
      "Test set: Avg. loss: 867.8126220703125\n",
      "Train loss: Avg. loss: 460.9664541839794\n",
      "\n",
      "Test set: Avg. loss: 932.6734619140625\n",
      "Train loss: Avg. loss: 426.4776395566904\n",
      "\n",
      "Test set: Avg. loss: 900.1567077636719\n",
      "Train loss: Avg. loss: 398.63470089663366\n",
      "\n",
      "Test set: Avg. loss: 919.0745239257812\n",
      "Train loss: Avg. loss: 376.82488790135477\n",
      "\n",
      "Test set: Avg. loss: 888.4896545410156\n",
      "Train loss: Avg. loss: 364.3821788229001\n",
      "\n",
      "Test set: Avg. loss: 894.9230651855469\n",
      "Train loss: Avg. loss: 351.44334158927774\n",
      "\n",
      "Test set: Avg. loss: 917.7711486816406\n",
      "Train loss: Avg. loss: 314.91261456726465\n",
      "\n",
      "Test set: Avg. loss: 969.7252502441406\n",
      "Train loss: Avg. loss: 305.2046491294909\n",
      "\n",
      "Test set: Avg. loss: 889.7532043457031\n",
      "Train loss: Avg. loss: 297.88483239435084\n",
      "\n",
      "Test set: Avg. loss: 833.1579284667969\n",
      "Train loss: Avg. loss: 294.7515523145153\n",
      "\n",
      "Test set: Avg. loss: 885.2728881835938\n",
      "Train loss: Avg. loss: 276.63490480070664\n",
      "\n",
      "Test set: Avg. loss: 872.3020324707031\n",
      "Train loss: Avg. loss: 279.6370361716884\n",
      "\n",
      "Test set: Avg. loss: 981.9070739746094\n",
      "Train loss: Avg. loss: 268.512108626639\n",
      "\n",
      "Test set: Avg. loss: 892.7408752441406\n",
      "Train loss: Avg. loss: 248.3931366258366\n",
      "\n",
      "Test set: Avg. loss: 862.9714660644531\n",
      "Train loss: Avg. loss: 239.1527669141247\n",
      "\n",
      "Test set: Avg. loss: 864.5671691894531\n",
      "\n",
      "Test set: Avg. loss: 0.0006378053426742554, AUC: 0.8292929999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006277632713317872, AUC: 0.8351\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006240120828151703, AUC: 0.8344740000000002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000607389748096466, AUC: 0.8404429999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000615617722272873, AUC: 0.8378519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0022265459299087523, AUC: 0.3349745\n",
      "\n",
      "Train loss: Avg. loss: 856.9047521117386\n",
      "\n",
      "Test set: Avg. loss: 878.3485412597656\n",
      "Train loss: Avg. loss: 706.8185514219248\n",
      "\n",
      "Test set: Avg. loss: 853.82470703125\n",
      "Train loss: Avg. loss: 637.1937815672273\n",
      "\n",
      "Test set: Avg. loss: 878.8326416015625\n",
      "Train loss: Avg. loss: 541.244027301764\n",
      "\n",
      "Test set: Avg. loss: 840.5386962890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: Avg. loss: 499.6870381543591\n",
      "\n",
      "Test set: Avg. loss: 867.663330078125\n",
      "Train loss: Avg. loss: 461.7127741916924\n",
      "\n",
      "Test set: Avg. loss: 811.6494445800781\n",
      "Train loss: Avg. loss: 434.3250710068235\n",
      "\n",
      "Test set: Avg. loss: 945.4025573730469\n",
      "Train loss: Avg. loss: 411.8556913145029\n",
      "\n",
      "Test set: Avg. loss: 930.64794921875\n",
      "Train loss: Avg. loss: 380.7037494440747\n",
      "\n",
      "Test set: Avg. loss: 931.2692565917969\n",
      "Train loss: Avg. loss: 370.9395670313744\n",
      "\n",
      "Test set: Avg. loss: 960.3101806640625\n",
      "Train loss: Avg. loss: 346.6971704762453\n",
      "\n",
      "Test set: Avg. loss: 856.8403625488281\n",
      "Train loss: Avg. loss: 333.8442057226874\n",
      "\n",
      "Test set: Avg. loss: 913.5760192871094\n",
      "Train loss: Avg. loss: 314.7012680928419\n",
      "\n",
      "Test set: Avg. loss: 832.1317443847656\n",
      "Train loss: Avg. loss: 303.52034871289686\n",
      "\n",
      "Test set: Avg. loss: 929.9712829589844\n",
      "Train loss: Avg. loss: 285.1557879599796\n",
      "\n",
      "Test set: Avg. loss: 890.0650024414062\n",
      "Train loss: Avg. loss: 274.6236451750348\n",
      "\n",
      "Test set: Avg. loss: 887.913330078125\n",
      "Train loss: Avg. loss: 274.9542120672335\n",
      "\n",
      "Test set: Avg. loss: 938.7119445800781\n",
      "Train loss: Avg. loss: 257.44857263261343\n",
      "\n",
      "Test set: Avg. loss: 959.8792419433594\n",
      "Train loss: Avg. loss: 250.1183166989855\n",
      "\n",
      "Test set: Avg. loss: 920.0390625\n",
      "Train loss: Avg. loss: 247.08739996260138\n",
      "\n",
      "Test set: Avg. loss: 903.0464172363281\n",
      "\n",
      "Test set: Avg. loss: 0.0006180092990398407, AUC: 0.842027\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000603181540966034, AUC: 0.8639389999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005980743467807769, AUC: 0.8690650000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005932260751724244, AUC: 0.8732110000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005806367695331574, AUC: 0.8784890000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010750616192817689, AUC: 0.41741100000000003\n",
      "\n",
      "Train loss: Avg. loss: 972.900798433146\n",
      "\n",
      "Test set: Avg. loss: 937.8157958984375\n",
      "Train loss: Avg. loss: 726.3888899687748\n",
      "\n",
      "Test set: Avg. loss: 888.915771484375\n",
      "Train loss: Avg. loss: 615.8173791192899\n",
      "\n",
      "Test set: Avg. loss: 871.9834899902344\n",
      "Train loss: Avg. loss: 541.1233749875597\n",
      "\n",
      "Test set: Avg. loss: 864.3016357421875\n",
      "Train loss: Avg. loss: 495.7417192398363\n",
      "\n",
      "Test set: Avg. loss: 869.6619873046875\n",
      "Train loss: Avg. loss: 455.6842775770054\n",
      "\n",
      "Test set: Avg. loss: 928.4888610839844\n",
      "Train loss: Avg. loss: 418.3151474484972\n",
      "\n",
      "Test set: Avg. loss: 912.7711791992188\n",
      "Train loss: Avg. loss: 387.8382315666053\n",
      "\n",
      "Test set: Avg. loss: 925.544189453125\n",
      "Train loss: Avg. loss: 369.09198863035556\n",
      "\n",
      "Test set: Avg. loss: 918.3468627929688\n",
      "Train loss: Avg. loss: 361.7445258851264\n",
      "\n",
      "Test set: Avg. loss: 896.2483215332031\n",
      "Train loss: Avg. loss: 337.67153143427174\n",
      "\n",
      "Test set: Avg. loss: 965.5973815917969\n",
      "Train loss: Avg. loss: 316.1325566966063\n",
      "\n",
      "Test set: Avg. loss: 877.5892028808594\n",
      "Train loss: Avg. loss: 303.9557817787122\n",
      "\n",
      "Test set: Avg. loss: 901.6127624511719\n",
      "Train loss: Avg. loss: 293.75481614033885\n",
      "\n",
      "Test set: Avg. loss: 861.8774719238281\n",
      "Train loss: Avg. loss: 280.2648957853864\n",
      "\n",
      "Test set: Avg. loss: 932.0419616699219\n",
      "Train loss: Avg. loss: 267.3033719396895\n",
      "\n",
      "Test set: Avg. loss: 919.3879699707031\n",
      "Train loss: Avg. loss: 254.6460493537271\n",
      "\n",
      "Test set: Avg. loss: 890.7640380859375\n",
      "Train loss: Avg. loss: 253.03336256476723\n",
      "\n",
      "Test set: Avg. loss: 913.2732849121094\n",
      "Train loss: Avg. loss: 248.85663517265561\n",
      "\n",
      "Test set: Avg. loss: 921.2451171875\n",
      "Train loss: Avg. loss: 234.48010419128806\n",
      "\n",
      "Test set: Avg. loss: 945.8767395019531\n",
      "\n",
      "Test set: Avg. loss: 0.0006396496891975402, AUC: 0.797492\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006177774965763092, AUC: 0.848985\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006089461445808411, AUC: 0.8592910000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006029734611511231, AUC: 0.861669\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000606326401233673, AUC: 0.862108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# triplet loss first few epochs + capped smote (cosine distance)\n",
    "\n",
    "# 2 class triplet loss with capped SMOTE \n",
    "\n",
    "\n",
    "momentum=0\n",
    "learning_rates = [(3e-6, 5e-4)]\n",
    "\n",
    "\n",
    "cap_aucs = []\n",
    "loss_fn_args = {}\n",
    "loss_caps = [1]\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "loss_fn_args['distance'] = 'cosine'\n",
    "start_epoch = 20\n",
    "\n",
    "triplet_loss_fn_args = {}\n",
    "triplet_loss_fn_args['margin'] = 1000 \n",
    "\n",
    "n_epochs = 71\n",
    "\n",
    "for cap in loss_caps:\n",
    "    learning_rate_aucs = []\n",
    "    for learning_rate in learning_rates:\n",
    "        aucs = []\n",
    "        for i in range(10): \n",
    "            best_embed_network = None \n",
    "            best_loss = 100000000\n",
    "            model_aucs = []\n",
    "            embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "            linear_probe = models.ConvNetLinearProbe(2)\n",
    "            complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "            embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "            linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "            _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "            model_aucs.append(auc)\n",
    "            for epoch in range(start_epoch):\n",
    "                _, train_losses = train.train_triplet_loss_smote(epoch, train_loader_tripletloss_smote, embed_network, embed_optimizer, verbose=False, loss_fn_args=triplet_loss_fn_args)\n",
    "                print(\"Train loss: Avg. loss: \" + str(np.mean(np.array(train_losses))))\n",
    "                test_losses = metric_utils.triplet_loss(test_loader_tripletloss, embed_network)\n",
    "                if (test_losses[0] < best_loss and test_losses != 0):\n",
    "                    best_embed_network = copy.deepcopy(embed_network)\n",
    "                    best_loss = test_losses[0]\n",
    "            for epoch in range(start_epoch, n_epochs+1):\n",
    "                complete_network = models.CompleteConvNet(best_embed_network, linear_probe)\n",
    "                loss_fn_args['loss_cap'] = cap\n",
    "                loss_fn_args['avg_tensors'] = []\n",
    "                for k in range(2):\n",
    "                    avg_tensor = best_embed_network(avg_tensors_list[k])\n",
    "                    loss_fn_args['avg_tensors'].append(avg_tensor)\n",
    "                _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, complete_network, linear_optimizer, verbose=False, loss_fn=loss_fns.CappedBCELossAvgDistance, loss_fn_args=loss_fn_args)\n",
    "                if (epoch + 1) % 10 == 0: \n",
    "                    _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "                    model_aucs.append(auc) \n",
    "            aucs.append(model_aucs)\n",
    "        learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "    learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "    auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "    auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "    cap_aucs.append([auc_mean, auc_variance])\n",
    "\n",
    "\n",
    "\n",
    "for c in range(len(cap_aucs)):\n",
    "    auc_mean = cap_aucs[c][0]\n",
    "    auc_variance = cap_aucs[c][1]\n",
    "    cap = loss_caps[c]\n",
    "    for i in range(len(learning_rates)): \n",
    "        row = [\"cosine_distance_capped_smote_with_smote_triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "                auc_mean[i][0], auc_variance[i][0], \n",
    "                auc_mean[i][1], auc_variance[i][1],\n",
    "                auc_mean[i][2], auc_variance[i][2],\n",
    "                auc_mean[i][3], auc_variance[i][3], # try saving more \n",
    "               cap, norm, \"start_epoch=\" + str(start_epoch) + \", get_best_model\"]\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0064e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "197ea5c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006948792040348052, AUC: 0.5014595000000001\n",
      "\n",
      "Train loss: 0.2345415927186797\n",
      "Train loss: 0.1294703808011888\n",
      "Train loss: 0.04265807248369048\n",
      "Train loss: 0.04238216786444941\n",
      "Train loss: 0.031144190438186066\n",
      "\n",
      "Test set: Avg. loss: 0.0005516567230224609, AUC: 0.8167754999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000635780543088913, AUC: 0.8704635000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008938502073287963, AUC: 0.8610120000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006958925127983093, AUC: 0.41396750000000004\n",
      "\n",
      "Train loss: 0.23915491224844246\n",
      "Train loss: 0.16468658552894108\n",
      "Train loss: 0.07288317438922351\n",
      "Train loss: 0.04761100418959992\n",
      "Train loss: 0.04834819971760617\n",
      "\n",
      "Test set: Avg. loss: 0.0005578775405883789, AUC: 0.8069790000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005317577719688416, AUC: 0.882526\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005789307355880737, AUC: 0.890787\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007073509097099305, AUC: 0.24827900000000003\n",
      "\n",
      "Train loss: 0.05548391915574859\n",
      "Train loss: 0.05076181737682487\n",
      "Train loss: 0.026790391040753713\n",
      "Train loss: 0.04403432336034654\n",
      "Train loss: 0.03734620236143281\n",
      "\n",
      "Test set: Avg. loss: 0.0005565661787986755, AUC: 0.8305435\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006685854196548462, AUC: 0.864727\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000819800466299057, AUC: 0.883482\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006946022808551789, AUC: 0.4465255\n",
      "\n",
      "Train loss: 0.2108210023445419\n",
      "Train loss: 0.12267575611042071\n",
      "Train loss: 0.06467939705788335\n",
      "Train loss: 0.04588776675960685\n",
      "Train loss: 0.050633625893653195\n",
      "\n",
      "Test set: Avg. loss: 0.0005582447946071625, AUC: 0.8064635\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006907750070095062, AUC: 0.864106\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008076609969139099, AUC: 0.884417\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007046984732151032, AUC: 0.2992165\n",
      "\n",
      "Train loss: 0.06428205212460289\n",
      "Train loss: 0.05348832320563401\n",
      "Train loss: 0.03763766077500355\n",
      "Train loss: 0.026214450975007648\n",
      "Train loss: 0.028648608847509457\n",
      "\n",
      "Test set: Avg. loss: 0.0005787582099437713, AUC: 0.831553\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000708237737417221, AUC: 0.859827\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008424067795276642, AUC: 0.8698835000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007039610743522644, AUC: 0.3674215\n",
      "\n",
      "Train loss: 0.0898788631716861\n",
      "Train loss: 0.04052136895022815\n",
      "Train loss: 0.03145033347455761\n",
      "Train loss: 0.018740395956401583\n",
      "Train loss: 0.012098006809814066\n",
      "\n",
      "Test set: Avg. loss: 0.0005109424591064453, AUC: 0.847133\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006449491083621979, AUC: 0.881591\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005325709879398345, AUC: 0.8827199999999998\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969795823097229, AUC: 0.3573845\n",
      "\n",
      "Train loss: 0.06886200587960738\n",
      "Train loss: 0.015502620346938507\n",
      "Train loss: 0.01465775317783597\n",
      "Train loss: 0.00875907985469963\n",
      "Train loss: 0.008310166340840013\n",
      "\n",
      "Test set: Avg. loss: 0.0005603133738040924, AUC: 0.803252\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006743178963661194, AUC: 0.8504079999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007782246470451355, AUC: 0.8768229999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006882103681564332, AUC: 0.7001930000000001\n",
      "\n",
      "Train loss: 0.10242060468166689\n",
      "Train loss: 0.044728699364239656\n",
      "Train loss: 0.024884712846973276\n",
      "Train loss: 0.03254192614857154\n",
      "Train loss: 0.01699110676970663\n",
      "\n",
      "Test set: Avg. loss: 0.0005637276470661164, AUC: 0.816737\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007200266122817993, AUC: 0.8564519999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009438620209693909, AUC: 0.8537380000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006895764470100403, AUC: 0.680987\n",
      "\n",
      "Train loss: 0.02276312625860866\n",
      "Train loss: 0.029110296617580366\n",
      "Train loss: 0.023664291146435316\n",
      "Train loss: 0.018022574201414857\n",
      "Train loss: 0.005415395845340777\n",
      "\n",
      "Test set: Avg. loss: 0.0005636400878429413, AUC: 0.809014\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007056748867034912, AUC: 0.85894\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009673133492469788, AUC: 0.8548060000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006920580267906189, AUC: 0.5592174999999999\n",
      "\n",
      "Train loss: 0.059465562995476055\n",
      "Train loss: 0.027641125872165342\n",
      "Train loss: 0.01703945443600039\n",
      "Train loss: 0.008914494816261002\n",
      "Train loss: 0.01579270181776602\n",
      "\n",
      "Test set: Avg. loss: 0.0005305800139904023, AUC: 0.8337589999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007109472751617432, AUC: 0.86267\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007067797780036927, AUC: 0.864947\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931989490985871, AUC: 0.5746359999999999\n",
      "\n",
      "Train loss: 0.068475080441825\n",
      "Train loss: 0.03125052361548701\n",
      "Train loss: 0.029076746747463564\n",
      "Train loss: 0.020407660098015507\n",
      "Train loss: 0.009345308134827432\n",
      "\n",
      "Test set: Avg. loss: 0.0005570430755615234, AUC: 0.845983\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008643153905868531, AUC: 0.8695419999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00107396399974823, AUC: 0.8811329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006895874440670013, AUC: 0.669326\n",
      "\n",
      "Train loss: 0.09200361974631684\n",
      "Train loss: 0.06771723605409453\n",
      "Train loss: 0.03587656986864307\n",
      "Train loss: 0.043934637987160984\n",
      "Train loss: 0.030096318148359467\n",
      "\n",
      "Test set: Avg. loss: 0.000576610267162323, AUC: 0.825008\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010546754598617554, AUC: 0.8507649999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009215476214885712, AUC: 0.885155\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006893592774868012, AUC: 0.6518594999999999\n",
      "\n",
      "Train loss: 0.19249790756008292\n",
      "Train loss: 0.09574962643128407\n",
      "Train loss: 0.056115876270245904\n",
      "Train loss: 0.02252537389344807\n",
      "Train loss: 0.018381772162039067\n",
      "\n",
      "Test set: Avg. loss: 0.0005590758919715881, AUC: 0.8460449999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008604009747505188, AUC: 0.8831390000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008280726671218873, AUC: 0.8879609999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007028565406799317, AUC: 0.297151\n",
      "\n",
      "Train loss: 0.13023216135894197\n",
      "Train loss: 0.05095446260669564\n",
      "Train loss: 0.03402220297463333\n",
      "Train loss: 0.03922224648391144\n",
      "Train loss: 0.04058719963967046\n",
      "\n",
      "Test set: Avg. loss: 0.000625956267118454, AUC: 0.8361829999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000985925853252411, AUC: 0.869995\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008865223824977875, AUC: 0.880958\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006922201812267304, AUC: 0.630326\n",
      "\n",
      "Train loss: 0.28093192698080327\n",
      "Train loss: 0.23621123425568205\n",
      "Train loss: 0.19653263574914087\n",
      "Train loss: 0.1047255351573606\n",
      "Train loss: 0.06822770381275611\n",
      "\n",
      "Test set: Avg. loss: 0.0005367255806922913, AUC: 0.8549699999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006181947886943817, AUC: 0.894611\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007875047326087952, AUC: 0.8908209999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000692348837852478, AUC: 0.6462735\n",
      "\n",
      "Train loss: 0.2000963408735734\n",
      "Train loss: 0.05765920802007748\n",
      "Train loss: 0.02051693276513981\n",
      "Train loss: 0.021672979185852825\n",
      "Train loss: 0.00466366885583612\n",
      "\n",
      "Test set: Avg. loss: 0.000577528715133667, AUC: 0.8503719999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007408493161201477, AUC: 0.893535\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0012513612508773804, AUC: 0.8682840000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007002864480018616, AUC: 0.2588235\n",
      "\n",
      "Train loss: 0.01950611645662332\n",
      "Train loss: 0.021222967135755322\n",
      "Train loss: 0.014264856712727607\n",
      "Train loss: 0.014929495280302024\n",
      "Train loss: 0.012512131582332563\n",
      "\n",
      "Test set: Avg. loss: 0.0006310569345951081, AUC: 0.8285570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000633629322052002, AUC: 0.890015\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011676799654960633, AUC: 0.875313\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965773105621338, AUC: 0.4295545\n",
      "\n",
      "Train loss: 0.03886495813538757\n",
      "Train loss: 0.03170137013061137\n",
      "Train loss: 0.025623226467567154\n",
      "Train loss: 0.01797549407693404\n",
      "Train loss: 0.018450113036964512\n",
      "\n",
      "Test set: Avg. loss: 0.0006050005853176117, AUC: 0.8481210000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010432253479957581, AUC: 0.8585975\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009336974322795868, AUC: 0.8692629999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006991313397884369, AUC: 0.4225405\n",
      "\n",
      "Train loss: 0.0866254319118548\n",
      "Train loss: 0.047092921371701395\n",
      "Train loss: 0.037978405439400974\n",
      "Train loss: 0.01995441958874087\n",
      "Train loss: 0.03221281721622129\n",
      "\n",
      "Test set: Avg. loss: 0.0005348641574382781, AUC: 0.8595189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006190233528614044, AUC: 0.879317\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009914110898971557, AUC: 0.881281\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006951979100704194, AUC: 0.3660665\n",
      "\n",
      "Train loss: 0.1624610341047939\n",
      "Train loss: 0.09496597096889833\n",
      "Train loss: 0.034901417508909974\n",
      "Train loss: 0.024344672885122178\n",
      "Train loss: 0.016825703125965746\n",
      "\n",
      "Test set: Avg. loss: 0.0005479520857334137, AUC: 0.853917\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0006467879116535186, AUC: 0.8732800000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010787789225578308, AUC: 0.871984\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.00069139564037323, AUC: 0.627242\n",
      "\n",
      "Train loss: 0.23791108063504665\n",
      "Train loss: 0.1295122564593448\n",
      "Train loss: 0.04239873644671863\n",
      "Train loss: 0.031116662146169927\n",
      "Train loss: 0.02645398393461976\n",
      "\n",
      "Test set: Avg. loss: 0.0006408562958240509, AUC: 0.777537\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005720231831073761, AUC: 0.7908329999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005550744533538818, AUC: 0.8156479999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006923474669456482, AUC: 0.5990475\n",
      "\n",
      "Train loss: 0.07152876597416552\n",
      "Train loss: 0.053868718539612205\n",
      "Train loss: 0.046308331097228615\n",
      "Train loss: 0.04059772551814212\n",
      "Train loss: 0.03315314311015455\n",
      "\n",
      "Test set: Avg. loss: 0.000648346722126007, AUC: 0.8064709999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005927608907222748, AUC: 0.780133\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005653988718986511, AUC: 0.804872\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006931006014347077, AUC: 0.5272245\n",
      "\n",
      "Train loss: 0.22763264556474325\n",
      "Train loss: 0.19008965280991566\n",
      "Train loss: 0.11652533653416211\n",
      "Train loss: 0.06316898329348504\n",
      "Train loss: 0.015016183822969847\n",
      "\n",
      "Test set: Avg. loss: 0.000640290915966034, AUC: 0.80511\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005853354632854461, AUC: 0.8233505\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005264957249164581, AUC: 0.843159\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006948291063308716, AUC: 0.437684\n",
      "\n",
      "Train loss: 0.16830931283250639\n",
      "Train loss: 0.07224250971516476\n",
      "Train loss: 0.03772362123561811\n",
      "Train loss: 0.021490948864176303\n",
      "Train loss: 0.02223817457126666\n",
      "\n",
      "Test set: Avg. loss: 0.0006516968607902527, AUC: 0.7864119999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005697245001792907, AUC: 0.792885\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005335564017295838, AUC: 0.8319970000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926256418228149, AUC: 0.6087290000000001\n",
      "\n",
      "Train loss: 0.1979982619044147\n",
      "Train loss: 0.097652122189727\n",
      "Train loss: 0.04752385540853573\n",
      "Train loss: 0.029117482372477084\n",
      "Train loss: 0.012496623057353346\n",
      "\n",
      "Test set: Avg. loss: 0.0006442910730838776, AUC: 0.747839\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005881699919700622, AUC: 0.774605\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000566236674785614, AUC: 0.8118810000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006950429975986481, AUC: 0.4577475\n",
      "\n",
      "Train loss: 0.3080034527597548\n",
      "Train loss: 0.2262739908846119\n",
      "Train loss: 0.10473707205132593\n",
      "Train loss: 0.09034999940968767\n",
      "Train loss: 0.05079212747042692\n",
      "\n",
      "Test set: Avg. loss: 0.0006519955396652222, AUC: 0.7642184999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005786177814006805, AUC: 0.780702\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005462760627269745, AUC: 0.8219200000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006926006078720093, AUC: 0.5821839999999999\n",
      "\n",
      "Train loss: 0.21515751865845692\n",
      "Train loss: 0.13020990845523303\n",
      "Train loss: 0.042362018476558634\n",
      "Train loss: 0.03310360033300858\n",
      "Train loss: 0.030505354645886\n",
      "\n",
      "Test set: Avg. loss: 0.0006447869837284088, AUC: 0.75187\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005956031978130341, AUC: 0.7715590000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005615503489971161, AUC: 0.8111240000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006792235970497131, AUC: 0.810316\n",
      "\n",
      "Train loss: 0.10501643751240984\n",
      "Train loss: 0.06456325853927226\n",
      "Train loss: 0.020847472963453847\n",
      "Train loss: 0.02660625644876987\n",
      "Train loss: 0.012627238714242284\n",
      "\n",
      "Test set: Avg. loss: 0.0006157079935073853, AUC: 0.8062539999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005554465651512146, AUC: 0.804002\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005333884358406066, AUC: 0.833733\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006965742409229278, AUC: 0.377181\n",
      "\n",
      "Train loss: 0.29595768678037426\n",
      "Train loss: 0.1686072108111804\n",
      "Train loss: 0.12233480169803282\n",
      "Train loss: 0.057010672514951684\n",
      "Train loss: 0.05732471656195725\n",
      "\n",
      "Test set: Avg. loss: 0.0006485117673873901, AUC: 0.7756869999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005770880579948426, AUC: 0.7821570000000001\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.000543806791305542, AUC: 0.8264549999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006969125270843506, AUC: 0.367363\n",
      "\n",
      "Train loss: 0.27801936864852905\n",
      "Train loss: 0.21866004074676126\n",
      "Train loss: 0.15187525749206543\n",
      "Train loss: 0.09686868703818019\n",
      "Train loss: 0.06382920772214479\n",
      "\n",
      "Test set: Avg. loss: 0.0006552173793315887, AUC: 0.75627\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005898939669132233, AUC: 0.7829189999999999\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005500214993953705, AUC: 0.8196399999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# smote w/ triplet loss \n",
    "    \n",
    "momentum=0\n",
    "learning_rates = [(1e-4, 5e-3), (1e-4, 1e-2), (1e-4, 1e-3)]\n",
    "\n",
    "loss_fn_args = {}\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "start_epoch = 5\n",
    "\n",
    "\n",
    "learning_rate_aucs = []\n",
    "for learning_rate in learning_rates:\n",
    "    aucs = []\n",
    "    for i in range(10): \n",
    "        model_aucs = []\n",
    "        embed_network = models.ConvNetOnlyEmbeddings(2)\n",
    "        linear_probe = models.ConvNetLinearProbe(2)\n",
    "        complete_network = models.CompleteConvNet(embed_network, linear_probe)\n",
    "        embed_optimizer = optim.SGD(embed_network.parameters(), lr=learning_rate[0], momentum=momentum)\n",
    "        linear_optimizer = optim.SGD(complete_network.parameters(), lr=learning_rate[1], momentum=momentum)\n",
    "        _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True) \n",
    "        model_aucs.append(auc)\n",
    "        for epoch in range(start_epoch):\n",
    "            _, train_losses = train.train_triplet_loss(epoch, train_loader_tripletloss_ratio, embed_network, embed_optimizer, verbose=False)\n",
    "            print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "        for epoch in range(start_epoch, n_epochs+1):\n",
    "            loss_fn_args['loss_cap'] = None\n",
    "            _, _ = train.train_sigmoid_with_embeddings(epoch, train_loader_smote, complete_network, linear_optimizer, verbose=False, loss_fn=loss_fns.CappedBCELoss, loss_fn_args=loss_fn_args)\n",
    "            if (epoch + 1) % 10 == 0: \n",
    "                _, auc = metric_utils.auc_sigmoid(test_loader_reduced, complete_network, embeddings=True)\n",
    "                model_aucs.append(auc) \n",
    "        aucs.append(model_aucs)\n",
    "    learning_rate_aucs.append(aucs)\n",
    "\n",
    "\n",
    "learning_rate_aucs = np.asarray(learning_rate_aucs)\n",
    "\n",
    "auc_mean = np.mean(learning_rate_aucs, axis=1)\n",
    "auc_variance = np.var(learning_rate_aucs, axis=1)\n",
    "\n",
    "\n",
    "for i in range(len(learning_rates)): \n",
    "    row = [\"smote_with_triplet_loss\", 2, nums, ratio, learning_rates[i],\n",
    "            auc_mean[i][0], auc_variance[i][0], \n",
    "            auc_mean[i][1], auc_variance[i][1],\n",
    "            auc_mean[i][2], auc_variance[i][2],\n",
    "            auc_mean[i][3], auc_variance[i][3], None, norm, \"start_epoch=5\"]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fab37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/convnet_aucs.csv')\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns = col_names) \n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.to_csv('results/convnet_aucs.csv', index=False)\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdef2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
